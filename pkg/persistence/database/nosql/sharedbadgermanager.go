package nosql

import (
	"encoding/json"
	"fmt"
	"reflect"
	"strings"
	"sync"
	"time"

	"github.com/dgraph-io/badger/v3"
	"github.com/digitalwayhk/core/pkg/persistence/entity"
	"github.com/digitalwayhk/core/pkg/persistence/types"
	"github.com/zeromicro/go-zero/core/logx"
)

// SharedBadgerManager å…±äº«çš„ BadgerDB ç®¡ç†å™¨
type SharedBadgerManager struct {
	db       *badger.DB
	config   BadgerDBConfig
	mu       sync.RWMutex
	refs     map[string]int // å¼•ç”¨è®¡æ•°: prefix -> count
	closeCh  chan struct{}
	wg       sync.WaitGroup
	isClosed bool
}

var (
	globalManagers = make(map[string]*SharedBadgerManager) // basePath -> manager
	managerMutex   sync.RWMutex
)

// DefaultSharedConfig å…±äº«æ¨¡å¼é…ç½®ï¼ˆé€‚åˆå¤šä¸ªå°è¡¨å…±äº«ï¼‰
func DefaultSharedConfig(path string) BadgerDBConfig {
	return BadgerDBConfig{
		Path:                 path,
		Mode:                 "shared",
		MemTableSize:         128 << 20, // 128MBï¼ˆæ¯”ç‹¬ç«‹æ¨¡å¼å¤§ï¼‰
		NumCompactors:        8,         // å¢åŠ  compactor
		NumLevelZeroTables:   4,
		NumLevelZeroStall:    8,
		ValueLogFileSize:     512 << 20, // 512MBï¼ˆæ¯”ç‹¬ç«‹æ¨¡å¼å¤§ï¼‰
		ValueThreshold:       1024,
		SyncWrites:           false,
		DetectConflicts:      true,
		GCInterval:           10 * time.Minute,
		GCDiscardRatio:       0.5,
		EnableLogger:         false,
		PeriodicSync:         true,
		PeriodicSyncInterval: 3 * time.Second,
		AutoSync:             true,
		SyncInterval:         10 * time.Second,
		SyncMinInterval:      2 * time.Second,
		SyncMaxInterval:      5 * time.Minute,
		SyncBatchSize:        500,
		AutoCleanup:          true,
		CleanupInterval:      30 * time.Minute,
		KeepDuration:         24 * time.Hour,
		SizeThreshold:        500 * 1024 * 1024, // 500MB è§¦å‘æ¸…ç†
	}
}

// GetSharedManager è·å–æˆ–åˆ›å»ºå…±äº«ç®¡ç†å™¨
func GetSharedManager(basePath string, config ...BadgerDBConfig) (*SharedBadgerManager, error) {
	managerMutex.Lock()
	defer managerMutex.Unlock()

	// å¦‚æœå·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
	if manager, ok := globalManagers[basePath]; ok {
		return manager, nil
	}

	// åˆ›å»ºæ–°çš„ç®¡ç†å™¨
	var cfg BadgerDBConfig
	if len(config) > 0 {
		cfg = config[0]
	} else {
		cfg = DefaultSharedConfig(basePath)
	}

	if err := cfg.Validate(); err != nil {
		return nil, fmt.Errorf("é…ç½®éªŒè¯å¤±è´¥: %w", err)
	}

	// ğŸ”§ å°è¯•æ¸…ç†æ—§é”æ–‡ä»¶
	if cfg.Mode == "fast" || cfg.Mode == "test" {
		diagnosis := diagnoseLockError(basePath)
		logx.Infof("å…±äº«DBæ£€æŸ¥é”: %s", diagnosis)
	}

	// æ„å»º BadgerDB é€‰é¡¹ï¼ˆé’ˆå¯¹å…±äº«åœºæ™¯ä¼˜åŒ–ï¼‰
	opts := badger.DefaultOptions(basePath).
		WithSyncWrites(cfg.SyncWrites).
		WithDetectConflicts(cfg.DetectConflicts).
		WithNumVersionsToKeep(1).
		WithNumCompactors(cfg.NumCompactors). // å…±äº«æ¨¡å¼å¢åŠ  compactor
		WithCompactL0OnClose(true).
		WithNumLevelZeroTables(cfg.NumLevelZeroTables).
		WithNumLevelZeroTablesStall(cfg.NumLevelZeroStall).
		WithValueLogFileSize(cfg.ValueLogFileSize). // å…±äº«æ¨¡å¼å¢å¤§ vlog
		WithMemTableSize(cfg.MemTableSize).         // å…±äº«æ¨¡å¼å¢å¤§å†…å­˜
		WithValueThreshold(cfg.ValueThreshold)

	// é…ç½®æ—¥å¿—
	if cfg.EnableLogger {
		opts = opts.WithLogger(&badgerLogger{})
	} else {
		opts = opts.WithLogger(nil)
	}

	// æ‰“å¼€æ•°æ®åº“ï¼ˆå¸¦é‡è¯•ï¼‰
	var db *badger.DB
	var err error
	maxRetries := 3

	for i := 0; i < maxRetries; i++ {
		db, err = badger.Open(opts)
		if err == nil {
			break
		}

		if isLockError(err) {
			diagnosis := diagnoseLockError(basePath)
			if i < maxRetries-1 {
				logx.Errorf("å…±äº«DBé”å®šï¼Œé‡è¯• (%d/%d): %s", i+1, maxRetries, diagnosis)
				time.Sleep(time.Second * time.Duration(i+1))
				continue
			}
			return nil, fmt.Errorf("æ‰“å¼€å…±äº«DBå¤±è´¥: %s\nåŸå§‹é”™è¯¯: %w", diagnosis, err)
		}

		return nil, fmt.Errorf("æ‰“å¼€å…±äº«DBå¤±è´¥: %w", err)
	}

	manager := &SharedBadgerManager{
		db:      db,
		config:  cfg,
		refs:    make(map[string]int),
		closeCh: make(chan struct{}),
	}

	// å¯åŠ¨å…¨å±€ GC
	manager.wg.Add(1)
	go manager.runGC()

	// å¯åŠ¨å®šæœŸåŒæ­¥
	if cfg.PeriodicSync {
		manager.wg.Add(1)
		go manager.periodicSync()
	}

	globalManagers[basePath] = manager

	logx.Infof("å…±äº«BadgerDBå·²å¯åŠ¨ [path=%s, mode=%s]", basePath, cfg.Mode)
	return manager, nil
}

// AddRef å¢åŠ å¼•ç”¨è®¡æ•°
func (m *SharedBadgerManager) AddRef(prefix string) {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.refs[prefix]++
	logx.Infof("å…±äº«DBæ·»åŠ å¼•ç”¨ [prefix=%s, refs=%d]", prefix, m.refs[prefix])
}

// RemoveRef å‡å°‘å¼•ç”¨è®¡æ•°
func (m *SharedBadgerManager) RemoveRef(prefix string) {
	m.mu.Lock()
	defer m.mu.Unlock()

	if count, ok := m.refs[prefix]; ok {
		m.refs[prefix] = count - 1
		if m.refs[prefix] <= 0 {
			delete(m.refs, prefix)
		}
		logx.Infof("å…±äº«DBç§»é™¤å¼•ç”¨ [prefix=%s, remaining=%d]", prefix, m.refs[prefix])
	}
}

// GetRefCount è·å–æ€»å¼•ç”¨è®¡æ•°
func (m *SharedBadgerManager) GetRefCount() int {
	m.mu.RLock()
	defer m.mu.RUnlock()
	return len(m.refs)
}

// periodicSync å®šæœŸåŒæ­¥
func (m *SharedBadgerManager) periodicSync() {
	defer m.wg.Done()

	ticker := time.NewTicker(m.config.PeriodicSyncInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			if err := m.db.Sync(); err != nil {
				logx.Errorf("å…±äº«DBåŒæ­¥å¤±è´¥: %v", err)
			}
		case <-m.closeCh:
			logx.Info("å…±äº«DB periodicSync é€€å‡º")
			return
		}
	}
}

// runGC åƒåœ¾å›æ”¶
func (m *SharedBadgerManager) runGC() {
	defer m.wg.Done()

	ticker := time.NewTicker(m.config.GCInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			var reclaimed int
			for {
				err := m.db.RunValueLogGC(m.config.GCDiscardRatio)
				if err != nil {
					break
				}
				reclaimed++
			}
			if reclaimed > 0 {
				logx.Infof("å…±äº«DB GCå®Œæˆï¼Œå›æ”¶ %d ä¸ªæ–‡ä»¶", reclaimed)
			}
		case <-m.closeCh:
			logx.Info("å…±äº«DB runGC é€€å‡º")
			return
		}
	}
}

// Close å…³é—­å…±äº«ç®¡ç†å™¨
func (m *SharedBadgerManager) Close() error {
	m.mu.Lock()
	if m.isClosed {
		m.mu.Unlock()
		return nil
	}
	m.isClosed = true
	m.mu.Unlock()

	close(m.closeCh)
	m.wg.Wait()

	if err := m.db.Sync(); err != nil {
		logx.Errorf("å…±äº«DBå…³é—­å‰syncå¤±è´¥: %v", err)
	}

	if err := m.db.Close(); err != nil {
		return fmt.Errorf("å…³é—­å…±äº«DBå¤±è´¥: %w", err)
	}

	logx.Info("å…±äº«BadgerDBå·²å…³é—­")
	return nil
}

// PrefixedBadgerDB å¸¦å‰ç¼€çš„å…±äº« BadgerDB
type PrefixedBadgerDB[T types.IModel] struct {
	manager *SharedBadgerManager
	prefix  string // "user:", "order:", "product:"

	syncDB         bool
	syncList       *entity.ModelList[T]
	syncLock       sync.RWMutex
	syncMutex      sync.Mutex
	syncInProgress bool
	closeCh        chan struct{}
	wg             sync.WaitGroup
	syncOnce       sync.Once
	isAutoClean    bool

	// å¾…åŒæ­¥è®¡æ•°ç¼“å­˜
	pendingCountCache int
	pendingCountMutex sync.RWMutex
	lastCountUpdate   time.Time
}

// NewSharedBadgerDB åˆ›å»ºå…±äº« BadgerDB å®ä¾‹
func NewSharedBadgerDB[T types.IModel](basePath string, config ...BadgerDBConfig) (*PrefixedBadgerDB[T], error) {
	prefix := reflect.TypeOf((*T)(nil)).Elem().Name() + ":"
	manager, err := GetSharedManager(basePath, config...)
	if err != nil {
		return nil, err
	}

	manager.AddRef(prefix)

	db := &PrefixedBadgerDB[T]{
		manager: manager,
		prefix:  prefix,
		closeCh: make(chan struct{}),
	}

	logx.Infof("å…±äº«BadgerDBå®ä¾‹å·²åˆ›å»º [prefix=%s]", prefix)
	return db, nil
}

// generateKey ç”Ÿæˆå¸¦å‰ç¼€çš„ key
func (p *PrefixedBadgerDB[T]) generateKey(item *T) string {
	if item == nil {
		return ""
	}
	if rowCode, ok := any(item).(types.IRowCode); ok {
		return p.prefix + rowCode.GetHash()
	}
	return ""
}

// SetSyncDB è®¾ç½®åŒæ­¥æ•°æ®åº“
func (p *PrefixedBadgerDB[T]) SetSyncDB(list *entity.ModelList[T]) {
	p.syncLock.Lock()
	defer p.syncLock.Unlock()

	if list != nil {
		if p.syncDB {
			return
		}
		p.syncDB = true
	} else {
		if !p.syncDB {
			return
		}
		p.syncDB = false
	}

	p.syncList = list

	if list != nil && p.syncDB {
		p.syncOnce.Do(func() {
			p.wg.Add(1)
			go p.syncToOtherDB()
			logx.Infof("å…±äº«DBè‡ªåŠ¨åŒæ­¥å·²å¯åŠ¨ [prefix=%s]", p.prefix)
		})
	}
}

// Set å†™å…¥æ•°æ®
func (p *PrefixedBadgerDB[T]) Set(item *T, ttl time.Duration, fn ...func(wrapper *SyncQueueItem[T])) error {
	key := p.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}

	p.syncLock.RLock()
	needSync := p.syncDB
	p.syncLock.RUnlock()

	data, err := p.setItem(key, needSync, item, fn...)
	if err != nil {
		return err
	}

	err = p.manager.db.Update(func(txn *badger.Txn) error {
		entry := badger.NewEntry([]byte(key), data)
		if ttl > 0 {
			entry = entry.WithTTL(ttl)
		}
		return txn.SetEntry(entry)
	})

	if err == nil && needSync {
		p.incrementPendingCount(1)
	}
	return err
}
func (p *PrefixedBadgerDB[T]) BatchInsert(items []*T) error {
	if len(items) == 0 {
		return nil
	}

	p.syncLock.RLock()
	needSync := p.syncDB
	p.syncLock.RUnlock()

	err := p.manager.db.Update(func(txn *badger.Txn) error {
		for _, item := range items {
			key := p.generateKey(item)
			if key == "" {
				return badger.ErrEmptyKey
			}

			data, err := p.setItem(key, needSync, item)
			if err != nil {
				return err
			}

			entry := badger.NewEntry([]byte(key), data)
			if err := txn.SetEntry(entry); err != nil {
				return err
			}
		}
		return nil
	})

	if err == nil && needSync {
		p.incrementPendingCount(len(items))
	}
	return err
}

// setItem å†…éƒ¨æ–¹æ³•ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
func (p *PrefixedBadgerDB[T]) setItem(key string, needSync bool, item *T, fn ...func(wrapper *SyncQueueItem[T])) ([]byte, error) {
	if item == nil {
		return nil, fmt.Errorf("item ä¸èƒ½ä¸ºç©º")
	}

	existingWrapper, err := p.getWrapper(key)
	var wrapper *SyncQueueItem[T]

	if err == nil && existingWrapper != nil {
		if existingWrapper.IsDeleted {
			return nil, fmt.Errorf("æ— æ³•æ›´æ–°å·²åˆ é™¤çš„é¡¹ï¼Œkey=%s", key)
		}
		wrapper = existingWrapper
		wrapper.Op = OpUpdate
		wrapper.Item = item
		wrapper.UpdatedAt = time.Now()
		wrapper.IsSynced = !needSync
	} else {
		now := time.Now()
		wrapper = &SyncQueueItem[T]{
			Key:       key,
			Item:      item,
			Op:        OpInsert,
			CreatedAt: now,
			UpdatedAt: now,
			IsSynced:  !needSync,
			IsDeleted: false,
		}
	}

	data, err := json.Marshal(wrapper)
	if err != nil {
		return nil, fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
	}

	if len(fn) > 0 {
		fn[0](wrapper)
	}
	return data, nil
}

// Get è·å–æ•°æ®
func (p *PrefixedBadgerDB[T]) Get(key string) (*T, error) {
	fullKey := p.prefix + key
	wrapper, err := p.getWrapper(fullKey)
	if err != nil {
		return nil, err
	}

	if wrapper.IsDeleted {
		return nil, badger.ErrKeyNotFound
	}

	return wrapper.Item, nil
}

// getWrapper å†…éƒ¨æ–¹æ³•
func (p *PrefixedBadgerDB[T]) getWrapper(key string) (*SyncQueueItem[T], error) {
	var wrapper = new(SyncQueueItem[T])

	err := p.manager.db.View(func(txn *badger.Txn) error {
		item, err := txn.Get([]byte(key))
		if err != nil {
			return err
		}

		return item.Value(func(val []byte) error {
			return json.Unmarshal(val, wrapper)
		})
	})

	if err != nil {
		return nil, err
	}

	if wrapper.Item != nil {
		if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
			hook.NewModel()
		}
	}

	return wrapper, nil
}

// Delete åˆ é™¤æ•°æ®
func (p *PrefixedBadgerDB[T]) Delete(key string) error {
	fullKey := p.prefix + key

	p.syncLock.RLock()
	needSync := p.syncDB
	p.syncLock.RUnlock()

	return p.delete(fullKey, needSync)
}
func (p *PrefixedBadgerDB[T]) DeleteByItem(item *T) error {
	key := p.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}

	p.syncLock.RLock()
	needSync := p.syncDB
	p.syncLock.RUnlock()

	return p.delete(key, needSync)
}
func (p *PrefixedBadgerDB[T]) DeleteByItemWithSync(item *T, needSync bool) error {
	key := p.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}

	return p.delete(key, needSync)
}

// delete å†…éƒ¨æ–¹æ³•
func (p *PrefixedBadgerDB[T]) delete(key string, needSync bool) error {
	if !needSync {
		return p.manager.db.Update(func(txn *badger.Txn) error {
			return txn.Delete([]byte(key))
		})
	}

	if !p.syncDB {
		return fmt.Errorf("æœªå¯ç”¨åŒæ­¥æ•°æ®åº“åŠŸèƒ½ï¼Œæ— æ³•æ‰§è¡Œè½¯åˆ é™¤")
	}

	return p.manager.db.Update(func(txn *badger.Txn) error {
		item, err := txn.Get([]byte(key))
		if err != nil {
			if err == badger.ErrKeyNotFound {
				return nil
			}
			return err
		}

		var wrapper SyncQueueItem[T]
		err = item.Value(func(val []byte) error {
			return json.Unmarshal(val, &wrapper)
		})
		if err != nil {
			return err
		}

		if wrapper.IsDeleted {
			return nil
		}

		now := time.Now()
		wrapper.Op = OpDelete
		wrapper.IsDeleted = true
		wrapper.DeletedAt = now
		wrapper.UpdatedAt = now
		wrapper.IsSynced = false

		data, err := json.Marshal(&wrapper)
		if err != nil {
			return fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
		}

		return txn.Set([]byte(key), data)
	})
}

// Scan æ‰«ææ•°æ®ï¼ˆä»…æ‰«æå½“å‰å‰ç¼€ï¼‰
func (p *PrefixedBadgerDB[T]) Scan(prefix string, limit int) ([]*T, error) {
	var results []*T
	prefix = p.prefix + prefix
	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		count := 0
		for it.Seek([]byte(prefix)); it.ValidForPrefix([]byte(prefix)); it.Next() {
			if limit > 0 && count >= limit {
				break
			}

			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				if wrapper.IsDeleted {
					return nil
				}

				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					results = append(results, wrapper.Item)
					count++
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})

	return results, err
}
func (p *PrefixedBadgerDB[T]) ScanAll() ([]*T, error) {
	var results []*T
	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()
		for it.Seek([]byte(p.prefix)); it.ValidForPrefix([]byte(p.prefix)); it.Next() {
			item := it.Item()
			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}
				if wrapper.IsDeleted {
					return nil
				}
				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					results = append(results, wrapper.Item)
				}
				return nil
			})
			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})
	return results, err
}
func (p *PrefixedBadgerDB[T]) ScanPage(prefix string, limit int, lastKey string) (*ScanResult[T], error) {
	prefix = p.prefix + prefix
	if limit <= 0 {
		limit = 1000 // é»˜è®¤æ¯é¡µ 1000 æ¡
	}

	result := &ScanResult[T]{
		Items: make([]*T, 0, limit),
	}

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		// ç¡®å®šèµ·å§‹ä½ç½®
		var startKey []byte
		if lastKey != "" {
			startKey = []byte(lastKey)
		} else {
			startKey = []byte(prefix)
		}

		count := 0
		firstItem := true

		for it.Seek(startKey); it.ValidForPrefix([]byte(prefix)); it.Next() {
			// è·³è¿‡ä¸Šä¸€é¡µçš„æœ€åä¸€æ¡ï¼ˆé¿å…é‡å¤ï¼‰
			if lastKey != "" && firstItem {
				currentKey := string(it.Item().Key())
				if currentKey == lastKey {
					firstItem = false
					continue
				}
			}
			firstItem = false

			// è¾¾åˆ°é™åˆ¶åå†è¯»ä¸€æ¡ï¼Œåˆ¤æ–­æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
			if count >= limit {
				result.HasMore = true
				break
			}

			item := it.Item()
			currentKey := string(item.Key())

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// è¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®
				if wrapper.IsDeleted {
					return nil
				}

				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					result.Items = append(result.Items, wrapper.Item)
					result.LastKey = currentKey
					count++
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}

		return nil
	})

	return result, err
}

// GetPendingSyncCount è·å–å¾…åŒæ­¥æ•°é‡
func (p *PrefixedBadgerDB[T]) GetPendingSyncCount() (int, error) {
	count := 0

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Seek([]byte(p.prefix)); it.ValidForPrefix([]byte(p.prefix)); it.Next() {
			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				if !wrapper.IsSynced {
					count++
				}
				return nil
			})

			if err != nil {
				continue
			}
		}
		return nil
	})

	return count, err
}

// incrementPendingCount æ›´æ–°å¾…åŒæ­¥è®¡æ•°
func (p *PrefixedBadgerDB[T]) incrementPendingCount(delta int) {
	p.pendingCountMutex.Lock()
	p.pendingCountCache += delta
	p.pendingCountMutex.Unlock()
}

// getDataAction è·å–åŒæ­¥æ“ä½œ
func (p *PrefixedBadgerDB[T]) getDataAction(item *T) types.IDataAction {
	if p.syncList != nil {
		model := item
		if model == nil {
			model = new(T)
			if nm, ok := any(model).(types.IModelNewHook); ok {
				nm.NewModel()
			}
		}
		searchItem := p.syncList.GetSearchItem()
		searchItem.Model = model
		action := p.syncList.GetDBAdapter(searchItem)
		return action
	}
	return nil
}

// syncToOtherDB åŒæ­¥åˆ°å…¶ä»–æ•°æ®åº“ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
func (p *PrefixedBadgerDB[T]) syncToOtherDB() {
	defer p.wg.Done()

	config := p.manager.config
	interval := config.SyncInterval
	minInterval := config.SyncMinInterval
	maxInterval := config.SyncMaxInterval

	ticker := time.NewTicker(interval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			p.syncLock.RLock()
			hasDB := p.syncDB
			p.syncLock.RUnlock()

			if !hasDB {
				continue
			}

			pendingCount, err := p.GetPendingSyncCount()
			if err != nil {
				logx.Errorf("è·å–å¾…åŒæ­¥æ•°é‡å¤±è´¥ [prefix=%s]: %v", p.prefix, err)
				continue
			}

			if pendingCount == 0 {
				interval = min(interval*2, maxInterval)
				ticker.Reset(interval)
				continue
			}

			p.syncMutex.Lock()
			if p.syncInProgress {
				p.syncMutex.Unlock()
				interval = min(interval*2, maxInterval)
				ticker.Reset(interval)
				continue
			}
			p.syncInProgress = true
			p.syncMutex.Unlock()

			start := time.Now()

			if err := p.processSyncQueue(); err != nil {
				logx.Errorf("åŒæ­¥å¤±è´¥ [prefix=%s]: %v", p.prefix, err)
			}

			duration := time.Since(start)

			p.syncMutex.Lock()
			p.syncInProgress = false
			p.syncMutex.Unlock()

			if duration < interval/2 {
				interval = max(interval/2, minInterval)
			} else if duration > interval {
				interval = min(duration*2, maxInterval)
			}

			ticker.Reset(interval)
			logx.Infof("åŒæ­¥å®Œæˆ [prefix=%s, å¤„ç†: %d, è€—æ—¶: %v]", p.prefix, pendingCount, duration)

		case <-p.closeCh:
			logx.Infof("syncToOtherDB é€€å‡º [prefix=%s]", p.prefix)
			return
		}
	}
}

// processSyncQueue å¤„ç†åŒæ­¥é˜Ÿåˆ—ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼Œé™å®šå‰ç¼€ï¼‰
func (p *PrefixedBadgerDB[T]) processSyncQueue() error {
	unsyncedItems, err := p.getUnsyncedBatch(p.manager.config.SyncBatchSize)
	if err != nil {
		return fmt.Errorf("è·å–æœªåŒæ­¥æ•°æ®å¤±è´¥: %w", err)
	}

	if len(unsyncedItems) == 0 {
		return nil
	}

	_, err = p.syncBatch(unsyncedItems)
	return err
}

// getUnsyncedBatch è·å–æœªåŒæ­¥æ•°æ®ï¼ˆé™å®šå‰ç¼€ï¼‰
func (p *PrefixedBadgerDB[T]) getUnsyncedBatch(limit int) ([]*SyncQueueItem[T], error) {
	var items []*SyncQueueItem[T]

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		count := 0
		for it.Seek([]byte(p.prefix)); it.ValidForPrefix([]byte(p.prefix)); it.Next() {
			if count >= limit {
				break
			}

			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				if !wrapper.IsSynced {
					if wrapper.Item != nil {
						if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
							hook.NewModel()
						}
					}
					items = append(items, &wrapper)
					count++
				}
				return nil
			})

			if err != nil {
				continue
			}
		}
		return nil
	})

	return items, err
}

// syncBatch æ‰¹é‡åŒæ­¥ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
func (p *PrefixedBadgerDB[T]) syncBatch(items []*SyncQueueItem[T]) ([]string, error) {
	successKeys := make([]string, 0, len(items))

	p.syncLock.RLock()
	defer p.syncLock.RUnlock()

	if !p.syncDB {
		return nil, fmt.Errorf("æœªå¼€å¯ syncDB")
	}

	for _, wrapper := range items {
		var err error
		syncAction := p.getDataAction(wrapper.Item)
		if syncAction == nil {
			logx.Errorf("æœªæ‰¾åˆ°åŒæ­¥æ“ä½œå¯¹è±¡ [%s]", wrapper.Key)
			continue
		}

		setHashCode(wrapper.Item)

		switch wrapper.Op {
		case OpInsert:
			if wrapper.Item != nil {
				err = syncAction.Insert(wrapper.Item)
				if err != nil {
					if strings.Contains(err.Error(), "duplicate key") || strings.Contains(err.Error(), "UNIQUE constraint failed") {
						logx.Infof("æ•°æ®å·²å­˜åœ¨ï¼Œå°è¯•æ›´æ–°æ“ä½œ [%s]", wrapper.Key)
						err = nil
					}
				}
				if err == nil {
					p.updateSyncedItem(wrapper)
				}
			}
		case OpUpdate:
			if wrapper.Item != nil {
				err = syncAction.Update(wrapper.Item)
				if err == nil {
					p.updateSyncedItem(wrapper)
				}
			}
		case OpDelete:
			if wrapper.Item != nil {
				err = syncAction.Delete(wrapper.Item)
				if err == nil {
					if err1 := p.delete(wrapper.Key, false); err1 != nil {
						logx.Errorf("ç‰©ç†åˆ é™¤å¤±è´¥ [%s]: %v", wrapper.Key, err1)
					}
				}
			}
		}

		if err != nil {
			logx.Errorf("åŒæ­¥æ•°æ®å¤±è´¥ [%s, op=%s]: %v", wrapper.Key, wrapper.Op, err)
			continue
		}

		successKeys = append(successKeys, wrapper.Key)
	}

	return successKeys, nil
}

// updateSyncedItem æ›´æ–°ä¸ºå·²åŒæ­¥
func (p *PrefixedBadgerDB[T]) updateSyncedItem(wrapper *SyncQueueItem[T]) error {
	return p.manager.db.Update(func(txn *badger.Txn) error {
		wrapper.IsSynced = true
		wrapper.SyncedAt = time.Now()
		wrapper.Op = OpUpdate

		data, err := json.Marshal(&wrapper)
		if err != nil {
			return err
		}

		return txn.Set([]byte(wrapper.Key), data)
	})
}
func (p *PrefixedBadgerDB[T]) Count() (int, error) {
	count := 0

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = false
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Seek([]byte(p.prefix)); it.ValidForPrefix([]byte(p.prefix)); it.Next() {
			count++
		}
		return nil
	})

	return count, err
}
func (p *PrefixedBadgerDB[T]) CountByPrefix(subPrefix string) (int, error) {
	count := 0
	fullPrefix := p.prefix + subPrefix

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = false
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Seek([]byte(fullPrefix)); it.ValidForPrefix([]byte(fullPrefix)); it.Next() {
			count++
		}
		return nil
	})

	return count, err
}

// Close å…³é—­å®ä¾‹
func (p *PrefixedBadgerDB[T]) Close() error {
	close(p.closeCh)
	p.wg.Wait()

	p.manager.RemoveRef(p.prefix)

	logx.Infof("å…±äº«BadgerDBå®ä¾‹å·²å…³é—­ [prefix=%s]", p.prefix)
	return nil
}

// CloseSharedManager å…³é—­å…¨å±€å…±äº«ç®¡ç†å™¨ï¼ˆåº”ç”¨é€€å‡ºæ—¶è°ƒç”¨ï¼‰
func CloseSharedManager(basePath string) error {
	managerMutex.Lock()
	defer managerMutex.Unlock()

	if manager, ok := globalManagers[basePath]; ok {
		delete(globalManagers, basePath)
		return manager.Close()
	}

	return nil
}

// CloseAllSharedManagers å…³é—­æ‰€æœ‰å…±äº«ç®¡ç†å™¨
func CloseAllSharedManagers() error {
	managerMutex.Lock()
	defer managerMutex.Unlock()

	for basePath, manager := range globalManagers {
		if err := manager.Close(); err != nil {
			logx.Errorf("å…³é—­å…±äº«ç®¡ç†å™¨å¤±è´¥ [path=%s]: %v", basePath, err)
		}
	}

	globalManagers = make(map[string]*SharedBadgerManager)
	return nil
}
