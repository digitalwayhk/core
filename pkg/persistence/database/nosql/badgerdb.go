package nosql

import (
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"strconv"
	"strings"
	"sync"
	"syscall"
	"time"

	"github.com/dgraph-io/badger/v3"
	"github.com/digitalwayhk/core/pkg/persistence/entity"
	"github.com/digitalwayhk/core/pkg/persistence/types"
	"github.com/zeromicro/go-zero/core/logx"
)

type OpType string

const (
	OpInsert OpType = "insert"
	OpUpdate OpType = "update"
	OpDelete OpType = "delete" // ğŸ”§ åˆ é™¤æ“ä½œ
)

// SyncQueueItem åŒæ­¥é˜Ÿåˆ—é¡¹ï¼ˆåŒ…è£…æ•°æ®ï¼‰
type SyncQueueItem[T types.IModel] struct {
	Key       string    `json:"key"`
	Item      *T        `json:"item,omitempty"`
	Op        OpType    `json:"op"`
	CreatedAt time.Time `json:"created_at"`
	UpdatedAt time.Time `json:"updated_at"`
	IsSynced  bool      `json:"is_synced"`
	SyncedAt  time.Time `json:"synced_at,omitempty"`
	IsDeleted bool      `json:"is_deleted"`
	DeletedAt time.Time `json:"deleted_at,omitempty"`
}

// BadgerDB æ³›å‹ KV æ•°æ®åº“
type BadgerDB[T types.IModel] struct {
	db             *badger.DB
	path           string
	config         BadgerDBConfig // ğŸ†• é…ç½®
	syncDB         bool
	syncList       *entity.ModelList[T]
	syncLock       sync.RWMutex
	syncMutex      sync.Mutex
	syncInProgress bool
	closeCh        chan struct{}
	wg             sync.WaitGroup
	syncOnce       sync.Once
	cleanupOnce    sync.Once // ğŸ†• æ¸…ç†å¯åŠ¨æ§åˆ¶
	bufferPool     sync.Pool
	isAutoClean    bool // ğŸ†• æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ¸…ç†

	// ğŸ†• å¾…åŒæ­¥è®¡æ•°ç¼“å­˜
	pendingCountCache int
	pendingCountMutex sync.RWMutex
	lastCountUpdate   time.Time
}

// NewBadgerDB åˆ›å»ºç”Ÿäº§ç¯å¢ƒ BadgerDBï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
func NewBadgerDB[T types.IModel](path string) (*BadgerDB[T], error) {
	config := DefaultProductionConfig(path)
	return NewBadgerDBWithConfig[T](config)
}

// NewBadgerDBFast åˆ›å»ºå¿«é€Ÿæ¨¡å¼ BadgerDBï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
func NewBadgerDBFast[T types.IModel](path string) (*BadgerDB[T], error) {
	config := DefaultFastConfig(path)
	config.PeriodicSync = true
	return NewBadgerDBWithConfig[T](config)
}

// ğŸ†• æ£€æŸ¥å¹¶è¯Šæ–­é”å®šé”™è¯¯
func diagnoseLockError(dbPath string) string {
	lockFile := filepath.Join(dbPath, "LOCK")

	// æ£€æŸ¥é”æ–‡ä»¶æ˜¯å¦å­˜åœ¨
	if _, err := os.Stat(lockFile); os.IsNotExist(err) {
		return "é”æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆå¯èƒ½æ˜¯å…¶ä»–é”™è¯¯ï¼‰"
	}

	// å°è¯•è¯»å–é”æ–‡ä»¶å†…å®¹ï¼ˆBadgerDB ä¼šå†™å…¥è¿›ç¨‹ä¿¡æ¯ï¼‰
	content, err := os.ReadFile(lockFile)
	if err != nil {
		return fmt.Sprintf("æ— æ³•è¯»å–é”æ–‡ä»¶: %v", err)
	}

	// è§£æé”æ–‡ä»¶å†…å®¹
	lines := strings.Split(string(content), "\n")
	if len(lines) > 0 && lines[0] != "" {
		pid, err := strconv.Atoi(strings.TrimSpace(lines[0]))
		if err == nil {
			// æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
			process, err := os.FindProcess(pid)
			if err != nil {
				return fmt.Sprintf("é”å®šè¿›ç¨‹ PID=%d å·²ä¸å­˜åœ¨ï¼ˆå¯èƒ½æ˜¯åƒµå°¸é”ï¼‰", pid)
			}

			// å°è¯•å‘é€ä¿¡å· 0 æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜æ´»
			err = process.Signal(syscall.Signal(0))
			if err != nil {
				return fmt.Sprintf("é”å®šè¿›ç¨‹ PID=%d å·²ä¸å­˜åœ¨ï¼ˆåƒµå°¸é”ï¼‰ï¼Œå»ºè®®æ‰‹åŠ¨åˆ é™¤é”æ–‡ä»¶", pid)
			}

			// ğŸ”§ è·å–è¿›ç¨‹ä¿¡æ¯ï¼ˆmacOS/Linuxï¼‰
			processInfo := getProcessInfo(pid)
			return fmt.Sprintf("æ•°æ®åº“è¢«è¿›ç¨‹é”å®š [PID=%d, %s]", pid, processInfo)
		}
	}

	return fmt.Sprintf("é”æ–‡ä»¶å­˜åœ¨ä½†æ ¼å¼å¼‚å¸¸: %s", string(content))
}

// ğŸ†• è·å–è¿›ç¨‹ä¿¡æ¯
func getProcessInfo(pid int) string {
	// macOS: ä½¿ç”¨ ps å‘½ä»¤
	cmdPath := fmt.Sprintf("/proc/%d/cmdline", pid)
	if content, err := os.ReadFile(cmdPath); err == nil {
		cmd := strings.ReplaceAll(string(content), "\x00", " ")
		return fmt.Sprintf("å‘½ä»¤: %s", cmd)
	}

	// å¤‡ç”¨æ–¹æ¡ˆï¼šåªè¿”å› PID
	return "è¿›ç¨‹æ­£åœ¨è¿è¡Œ"
}

// ğŸ”§ æ”¹è¿›çš„é”å®šé”™è¯¯æ£€æŸ¥
func isLockError(err error) bool {
	if err == nil {
		return false
	}

	errStr := err.Error()

	// BadgerDB çš„å…¸å‹é”å®šé”™è¯¯ä¿¡æ¯
	lockKeywords := []string{
		"Cannot acquire directory lock",
		"resource temporarily unavailable",
		"å¦ä¸€ä¸ªè¿›ç¨‹æ­£åœ¨ä½¿ç”¨",
		"LOCK",
	}

	for _, keyword := range lockKeywords {
		if strings.Contains(errStr, keyword) {
			return true
		}
	}

	// ç³»ç»Ÿçº§é”å®šé”™è¯¯
	return os.IsExist(err) ||
		syscall.EAGAIN.Error() == errStr ||
		os.IsPermission(err)
}

// NewBadgerDBWithConfig ä½¿ç”¨é…ç½®åˆ›å»º BadgerDB
func NewBadgerDBWithConfig[T types.IModel](config BadgerDBConfig) (*BadgerDB[T], error) {
	// éªŒè¯é…ç½®
	if err := config.Validate(); err != nil {
		return nil, fmt.Errorf("é…ç½®éªŒè¯å¤±è´¥: %w", err)
	}

	// ğŸ†• å°è¯•æ¸…ç†æ—§çš„é”æ–‡ä»¶ï¼ˆä»…åœ¨å¼€å‘/æµ‹è¯•ç¯å¢ƒï¼‰
	if config.Mode == "fast" || config.Mode == "test" {
		lockFile := filepath.Join(config.Path, "LOCK")
		if _, err := os.Stat(lockFile); err == nil {
			// ğŸ”§ å…ˆè¯Šæ–­é”å®šæƒ…å†µ
			diagnosis := diagnoseLockError(config.Path)
			logx.Infof("å‘ç°é”æ–‡ä»¶: %s", diagnosis)

			// å°è¯•åˆ é™¤é”æ–‡ä»¶ï¼ˆå¯èƒ½å¤±è´¥ï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼‰
			if err := os.Remove(lockFile); err != nil {
				logx.Errorf("æ— æ³•åˆ é™¤é”æ–‡ä»¶: %v", err)
			} else {
				logx.Info("å·²æ¸…ç†æ—§é”æ–‡ä»¶")
			}
		}
	}

	// æ„å»º BadgerDB é€‰é¡¹
	opts := badger.DefaultOptions(config.Path).
		WithSyncWrites(config.SyncWrites).
		WithDetectConflicts(config.DetectConflicts).
		WithNumVersionsToKeep(1).
		WithNumCompactors(config.NumCompactors).
		WithCompactL0OnClose(true).
		WithNumLevelZeroTables(config.NumLevelZeroTables).
		WithNumLevelZeroTablesStall(config.NumLevelZeroStall).
		WithValueLogFileSize(config.ValueLogFileSize).
		WithMemTableSize(config.MemTableSize).
		WithValueThreshold(config.ValueThreshold)

	// é…ç½®æ—¥å¿—
	if config.EnableLogger {
		opts = opts.WithLogger(&badgerLogger{})
	} else {
		opts = opts.WithLogger(nil)
	}

	// ğŸ†• æ·»åŠ é‡è¯•é€»è¾‘
	var db *badger.DB
	var err error
	maxRetries := 3

	for i := 0; i < maxRetries; i++ {
		db, err = badger.Open(opts)
		if err == nil {
			break
		}

		// æ£€æŸ¥æ˜¯å¦æ˜¯é”å®šé”™è¯¯
		if isLockError(err) {
			// ğŸ”§ è¯¦ç»†è¯Šæ–­
			diagnosis := diagnoseLockError(config.Path)

			if i < maxRetries-1 {
				logx.Errorf("æ•°æ®åº“è¢«é”å®šï¼Œç­‰å¾…é‡è¯•... (%d/%d)\nè¯¦æƒ…: %s", i+1, maxRetries, diagnosis)
				time.Sleep(time.Second * time.Duration(i+1))
				continue
			} else {
				// ğŸ”§ æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼Œè¿”å›è¯¦ç»†é”™è¯¯
				return nil, fmt.Errorf("æ‰“å¼€ BadgerDB å¤±è´¥ï¼ˆå·²é‡è¯• %d æ¬¡ï¼‰: %s\nåŸå§‹é”™è¯¯: %w",
					maxRetries, diagnosis, err)
			}
		}

		return nil, fmt.Errorf("æ‰“å¼€ BadgerDB å¤±è´¥: %w", err)
	}

	b := &BadgerDB[T]{
		db:      db,
		path:    config.Path,
		config:  config,
		closeCh: make(chan struct{}),
		bufferPool: sync.Pool{
			New: func() interface{} {
				return make([]byte, 0, 1024)
			},
		},
	}

	// å¯åŠ¨ GC
	b.wg.Add(1)
	go b.runGC()

	// å¯åŠ¨å®šæœŸç£ç›˜åŒæ­¥ï¼ˆFast æ¨¡å¼ï¼‰
	if config.PeriodicSync {
		b.wg.Add(1)
		go b.periodicSync()
	}

	if config.AutoCleanup {
		b.SetAutoCleanup(true)
	}

	logx.Infof("BadgerDB å·²å¯åŠ¨ [mode=%s, path=%s, autoSync=%v, autoCleanup=%v]",
		config.Mode, config.Path, config.AutoSync, config.AutoCleanup)

	return b, nil
}

// ğŸ†• æ·»åŠ æ‰‹åŠ¨æ£€æŸ¥é”å®šçŠ¶æ€çš„æ–¹æ³•
func CheckDatabaseLock(dbPath string) error {
	lockFile := filepath.Join(dbPath, "LOCK")

	if _, err := os.Stat(lockFile); os.IsNotExist(err) {
		return nil // æ— é”
	}

	diagnosis := diagnoseLockError(dbPath)
	return fmt.Errorf("æ•°æ®åº“å·²è¢«é”å®š: %s", diagnosis)
}

// ğŸ†• å¼ºåˆ¶é‡Šæ”¾é”ï¼ˆå±é™©æ“ä½œï¼Œä»…ç”¨äºæ¢å¤ï¼‰
func ForceUnlock(dbPath string) error {
	lockFile := filepath.Join(dbPath, "LOCK")

	// å…ˆè¯Šæ–­
	diagnosis := diagnoseLockError(dbPath)
	logx.Errorf("å¼ºåˆ¶è§£é”å‰è¯Šæ–­: %s", diagnosis)

	// åˆ é™¤é”æ–‡ä»¶
	if err := os.Remove(lockFile); err != nil {
		return fmt.Errorf("åˆ é™¤é”æ–‡ä»¶å¤±è´¥: %w", err)
	}

	logx.Info("å·²å¼ºåˆ¶åˆ é™¤é”æ–‡ä»¶")
	return nil
}
func (b *BadgerDB[T]) getDataAction(item *T) types.IDataAction {
	if b.syncList != nil {
		model := item
		if model == nil {
			model = new(T)
			if nm, ok := any(model).(types.IModelNewHook); ok {
				nm.NewModel()
			}
		}
		searchItem := b.syncList.GetSearchItem()
		searchItem.Model = model
		action := b.syncList.GetDBAdapter(searchItem)
		return action
	}
	return nil
}
func (b *BadgerDB[T]) SetAutoCleanup(auto bool) {
	b.isAutoClean = auto
	if auto {
		// ğŸ”§ å¯åŠ¨è‡ªåŠ¨æ¸…ç†
		b.cleanupOnce.Do(func() {
			b.wg.Add(1)
			go b.autoCleanup()
			logx.Info("è‡ªåŠ¨æ¸…ç†å·²å¯åŠ¨")
		})
	}
}

// SetSyncDB è®¾ç½®åŒæ­¥æ•°æ®åº“
func (b *BadgerDB[T]) SetSyncDB(list *entity.ModelList[T]) {
	b.syncLock.Lock()
	defer b.syncLock.Unlock()
	if list != nil {
		if b.syncDB {
			return
		}
		b.syncDB = true
	} else {
		if !b.syncDB {
			return
		}
		b.syncDB = false
	}

	b.syncList = list

	if list != nil && b.syncDB {
		// ğŸ”§ å¯åŠ¨è‡ªåŠ¨åŒæ­¥
		b.syncOnce.Do(func() {
			b.wg.Add(1)
			go b.syncToOtherDB()
			logx.Info("è‡ªåŠ¨åŒæ­¥å·²å¯åŠ¨")
		})
	}
}

// generateKey ç”Ÿæˆ key
func (b *BadgerDB[T]) generateKey(item *T) string {
	if item == nil {
		return ""
	}
	if rowCode, ok := any(item).(types.IRowCode); ok {
		return rowCode.GetHash()
	}
	return ""
}

// ğŸ†• å¿«é€Ÿè·å–å¾…åŒæ­¥æ•°é‡ï¼ˆå¸¦ç¼“å­˜ï¼‰
func (b *BadgerDB[T]) GetPendingSyncCountFast() (int, error) {
	b.pendingCountMutex.RLock()

	// ç¼“å­˜æœ‰æ•ˆæœŸ 1 ç§’
	if time.Since(b.lastCountUpdate) < time.Second {
		count := b.pendingCountCache
		b.pendingCountMutex.RUnlock()
		return count, nil
	}

	b.pendingCountMutex.RUnlock()

	// é‡æ–°è®¡æ•°
	count, err := b.GetPendingSyncCount()
	if err != nil {
		return 0, err
	}

	b.pendingCountMutex.Lock()
	b.pendingCountCache = count
	b.lastCountUpdate = time.Now()
	b.pendingCountMutex.Unlock()

	return count, nil
}

// ğŸ†• æ›´æ–°å¾…åŒæ­¥è®¡æ•°ï¼ˆåœ¨å†™å…¥/åˆ é™¤æ—¶è°ƒç”¨ï¼‰
func (b *BadgerDB[T]) incrementPendingCount(delta int) {
	b.pendingCountMutex.Lock()
	b.pendingCountCache += delta
	b.pendingCountMutex.Unlock()
}

// Set å†™å…¥æ•°æ®
func (b *BadgerDB[T]) Set(item *T, ttl time.Duration, fn ...func(wrapper *SyncQueueItem[T])) error {
	key := b.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}

	b.syncLock.RLock()
	needSync := b.syncDB
	b.syncLock.RUnlock()

	data, err := b.setItem(key, needSync, item, fn...)
	err = b.db.Update(func(txn *badger.Txn) error {
		entry := badger.NewEntry([]byte(key), data)
		if ttl > 0 {
			entry = entry.WithTTL(ttl)
		}
		return txn.SetEntry(entry)
	})

	// ğŸ†• æ›´æ–°å¾…åŒæ­¥è®¡æ•°
	if err == nil && needSync {
		b.incrementPendingCount(1)
	}
	return err
}
func (b *BadgerDB[T]) setItem(key string, needSync bool, item *T, fn ...func(wrapper *SyncQueueItem[T])) ([]byte, error) {
	if item == nil {
		return nil, fmt.Errorf("item ä¸èƒ½ä¸ºç©º")
	}
	existingWrapper, err := b.getWrapper(key)
	var wrapper *SyncQueueItem[T]

	if err == nil && existingWrapper != nil {
		if existingWrapper.IsDeleted {
			return nil, fmt.Errorf("æ— æ³•æ›´æ–°å·²åˆ é™¤çš„é¡¹ï¼Œkey=%s", key)
		}
		wrapper = existingWrapper
		wrapper.Op = OpUpdate
		wrapper.Item = item
		wrapper.UpdatedAt = time.Now()
		wrapper.IsSynced = !needSync
	} else {
		now := time.Now()
		wrapper = &SyncQueueItem[T]{
			Key:       key,
			Item:      item,
			Op:        OpInsert,
			CreatedAt: now,
			UpdatedAt: now,
			IsSynced:  !needSync,
			IsDeleted: false,
		}
	}

	data, err := json.Marshal(wrapper)
	if err != nil {
		return nil, fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
	}

	if len(fn) > 0 {
		fn[0](wrapper)
	}
	return data, nil
}
func (b *BadgerDB[T]) batchInsert(items []*T, fn ...func(wrapper *SyncQueueItem[T])) error {
	if len(items) == 0 {
		return nil
	}

	b.syncLock.RLock()
	needSync := b.syncDB
	b.syncLock.RUnlock()

	type serializedItem struct {
		key   string
		value []byte
	}

	serialized := make([]serializedItem, 0, len(items))

	for _, item := range items {
		if item == nil {
			continue
		}
		key := b.generateKey(item)
		if key == "" {
			return badger.ErrEmptyKey
		}
		value, err := b.setItem(key, needSync, item, fn...)
		if err != nil {
			return fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
		}
		serialized = append(serialized, serializedItem{
			key:   key,
			value: value,
		})
	}

	// æ‰¹é‡å†™å…¥
	const maxRetries = 3
	var lastErr error

	for retry := 0; retry < maxRetries; retry++ {
		txn := b.db.NewTransaction(true)
		success := true

		for _, si := range serialized {
			if err := txn.Set([]byte(si.key), si.value); err != nil {
				if err == badger.ErrTxnTooBig {
					if commitErr := txn.Commit(); commitErr != nil {
						lastErr = commitErr
						success = false
						break
					}
					txn = b.db.NewTransaction(true)
					if err := txn.Set([]byte(si.key), si.value); err != nil {
						txn.Discard()
						lastErr = err
						success = false
						break
					}
				} else {
					txn.Discard()
					lastErr = err
					success = false
					break
				}
			}
		}

		if success {
			if err := txn.Commit(); err != nil {
				lastErr = err
				time.Sleep(time.Millisecond * 100 * time.Duration(retry+1))
				continue
			}
			// ğŸ†• æ›´æ–°å¾…åŒæ­¥è®¡æ•°
			if needSync {
				b.incrementPendingCount(1)
			}
			return nil
		}

		txn.Discard()
		time.Sleep(time.Millisecond * 100 * time.Duration(retry+1))
	}

	return lastErr
}

// BatchInsert æ‰¹é‡æ’å…¥
func (b *BadgerDB[T]) BatchInsert(items []*T) error {
	return b.batchInsert(items)
}

// BatchInsertWithBack å¸¦å›è°ƒçš„æ‰¹é‡æ’å…¥
func (b *BadgerDB[T]) BatchInsertWithBack(items []*T, fn ...func(wrapper *SyncQueueItem[T])) error {
	return b.batchInsert(items, fn...)
}
func (b *BadgerDB[T]) DeleteByItem(item *T) error {
	if item == nil {
		return fmt.Errorf("item ä¸èƒ½ä¸ºç©º")
	}
	key := b.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}
	b.syncLock.RLock()
	needSync := b.syncDB
	b.syncLock.RUnlock()
	return b.delete(key, needSync)
}
func (b *BadgerDB[T]) DeleteByItemWithSync(item *T, needSync bool) error {
	if item == nil {
		return fmt.Errorf("item ä¸èƒ½ä¸ºç©º")
	}
	key := b.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}
	return b.delete(key, needSync)
}

// Delete åˆ é™¤æ•°æ®ï¼ˆæ”¯æŒè½¯åˆ é™¤ï¼‰
func (b *BadgerDB[T]) Delete(key string) error {
	b.syncLock.RLock()
	needSync := b.syncDB
	b.syncLock.RUnlock()
	return b.delete(key, needSync)
}
func (b *BadgerDB[T]) delete(key string, needSync bool) error {
	if !needSync {
		// ğŸ”§ ä¸éœ€è¦åŒæ­¥ï¼Œç›´æ¥ç‰©ç†åˆ é™¤
		return b.db.Update(func(txn *badger.Txn) error {
			return txn.Delete([]byte(key))
		})
	}
	if !b.syncDB {
		return fmt.Errorf("æœªå¯ç”¨åŒæ­¥æ•°æ®åº“åŠŸèƒ½ï¼Œæ— æ³•æ‰§è¡Œè½¯åˆ é™¤")
	}
	// ğŸ”§ éœ€è¦åŒæ­¥ï¼Œæ‰§è¡Œè½¯åˆ é™¤
	return b.db.Update(func(txn *badger.Txn) error {
		// è¯»å–ç°æœ‰æ•°æ®
		item, err := txn.Get([]byte(key))
		if err != nil {
			if err == badger.ErrKeyNotFound {
				return nil // æ•°æ®ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›
			}
			return err
		}

		var wrapper SyncQueueItem[T]
		err = item.Value(func(val []byte) error {
			return json.Unmarshal(val, &wrapper)
		})
		if err != nil {
			return err
		}

		// ğŸ”§ å¦‚æœå·²ç»æ˜¯åˆ é™¤çŠ¶æ€ï¼Œç›´æ¥è¿”å›
		if wrapper.IsDeleted {
			return nil
		}

		// ğŸ”§ æ ‡è®°ä¸ºè½¯åˆ é™¤
		now := time.Now()
		wrapper.Op = OpDelete
		wrapper.IsDeleted = true
		wrapper.DeletedAt = now
		wrapper.UpdatedAt = now
		wrapper.IsSynced = false // éœ€è¦åŒæ­¥åˆ é™¤æ“ä½œ

		// å†™å›
		data, err := json.Marshal(&wrapper)
		if err != nil {
			return fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
		}

		return txn.Set([]byte(key), data)
	})
}

// Get è·å–æ•°æ®ï¼ˆè¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®ï¼‰
func (b *BadgerDB[T]) Get(key string) (*T, error) {
	wrapper, err := b.getWrapper(key)
	if err != nil {
		return nil, err
	}

	// ğŸ”§ è¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®
	if wrapper.IsDeleted {
		return nil, badger.ErrKeyNotFound
	}

	return wrapper.Item, nil
}

// getWrapper è·å–åŒ…è£…å¯¹è±¡ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
func (b *BadgerDB[T]) getWrapper(key string) (*SyncQueueItem[T], error) {
	var wrapper = new(SyncQueueItem[T])

	err := b.db.View(func(txn *badger.Txn) error {
		item, err := txn.Get([]byte(key))
		if err != nil {
			return err
		}

		return item.Value(func(val []byte) error {
			return json.Unmarshal(val, wrapper)
		})
	})

	if err != nil {
		return nil, err
	}

	// ğŸ”§ åˆå§‹åŒ– Item
	if wrapper.Item != nil {
		if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
			hook.NewModel()
		}
	}

	return wrapper, nil
}

// Scan æ‰«ææ•°æ®ï¼ˆè¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®ï¼‰
func (b *BadgerDB[T]) Scan(prefix string, limit int) ([]*T, error) {
	var results []*T

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		count := 0
		for it.Seek([]byte(prefix)); it.ValidForPrefix([]byte(prefix)); it.Next() {
			// âœ… ä¿®å¤ï¼šlimit <= 0 è¡¨ç¤ºä¸é™åˆ¶
			if limit > 0 && count >= limit {
				break
			}

			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// è¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®
				if wrapper.IsDeleted {
					return nil
				}

				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					results = append(results, wrapper.Item)
					count++
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})

	return results, err
}

// ScanResult åˆ†é¡µæ‰«æç»“æœ
type ScanResult[T types.IModel] struct {
	Items   []*T   `json:"items"`    // æ•°æ®åˆ—è¡¨
	LastKey string `json:"last_key"` // æœ€åä¸€ä¸ª keyï¼ˆç”¨äºä¸‹æ¬¡åˆ†é¡µï¼‰
	HasMore bool   `json:"has_more"` // æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
}

// ScanPage åˆ†é¡µæ‰«ææ•°æ®ï¼ˆåŸºäºæ¸¸æ ‡ï¼‰
// prefix: key å‰ç¼€
// limit: æ¯é¡µæ•°é‡
// lastKey: ä¸Šä¸€é¡µçš„æœ€åä¸€ä¸ª keyï¼ˆé¦–æ¬¡ä¼ ç©ºå­—ç¬¦ä¸²ï¼‰
func (b *BadgerDB[T]) ScanPage(prefix string, limit int, lastKey string) (*ScanResult[T], error) {
	if limit <= 0 {
		limit = 1000 // é»˜è®¤æ¯é¡µ 1000 æ¡
	}

	result := &ScanResult[T]{
		Items: make([]*T, 0, limit),
	}

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		// ç¡®å®šèµ·å§‹ä½ç½®
		var startKey []byte
		if lastKey != "" {
			startKey = []byte(lastKey)
		} else {
			startKey = []byte(prefix)
		}

		count := 0
		firstItem := true

		for it.Seek(startKey); it.ValidForPrefix([]byte(prefix)); it.Next() {
			// è·³è¿‡ä¸Šä¸€é¡µçš„æœ€åä¸€æ¡ï¼ˆé¿å…é‡å¤ï¼‰
			if lastKey != "" && firstItem {
				currentKey := string(it.Item().Key())
				if currentKey == lastKey {
					firstItem = false
					continue
				}
			}
			firstItem = false

			// è¾¾åˆ°é™åˆ¶åå†è¯»ä¸€æ¡ï¼Œåˆ¤æ–­æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
			if count >= limit {
				result.HasMore = true
				break
			}

			item := it.Item()
			currentKey := string(item.Key())

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// è¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®
				if wrapper.IsDeleted {
					return nil
				}

				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					result.Items = append(result.Items, wrapper.Item)
					result.LastKey = currentKey
					count++
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}

		return nil
	})

	return result, err
}

// GetAll è·å–æ‰€æœ‰æ•°æ®ï¼ˆè¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®ï¼‰
func (b *BadgerDB[T]) GetAll() ([]*T, error) {
	var results []*T

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// ğŸ”§ è¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®
				if wrapper.IsDeleted {
					return nil
				}

				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					results = append(results, wrapper.Item)
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})

	return results, err
}

// GetPendingSyncCount è·å–å¾…åŒæ­¥æ•°é‡
func (b *BadgerDB[T]) GetPendingSyncCount() (int, error) {
	count := 0

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// ğŸ”§ ç»Ÿè®¡æœªåŒæ­¥çš„æ•°æ®ï¼ˆåŒ…æ‹¬åˆ é™¤æ“ä½œï¼‰
				if !wrapper.IsSynced {
					count++
				}
				return nil
			})

			if err != nil {
				continue
			}
		}
		return nil
	})

	return count, err
}

// processSyncQueue å¤„ç†åŒæ­¥é˜Ÿåˆ—
func (b *BadgerDB[T]) processSyncQueue() error {
	// ğŸ”§ ä½¿ç”¨é…ç½®ä¸­çš„æ‰¹æ¬¡å¤§å°
	unsyncedItems, err := b.getUnsyncedBatch(b.config.SyncBatchSize)
	if err != nil {
		return fmt.Errorf("è·å–æœªåŒæ­¥æ•°æ®å¤±è´¥: %w", err)
	}

	if len(unsyncedItems) == 0 {
		return nil
	}

	logx.Infof("å¼€å§‹åŒæ­¥ %d æ¡æ•°æ®åˆ°å…¶ä»–DB", len(unsyncedItems))

	_, err = b.syncBatch(unsyncedItems)
	if err != nil {
		logx.Errorf("æ‰¹é‡åŒæ­¥å¤±è´¥: %v", err)
	}

	// if len(successKeys) > 0 {
	// 	if err := b.handleSyncedItems(successKeys); err != nil {
	// 		logx.Errorf("å¤„ç†å·²åŒæ­¥æ•°æ®å¤±è´¥: %v", err)
	// 	} else {
	// 		logx.Infof("æˆåŠŸåŒæ­¥ %d æ¡æ•°æ®", len(successKeys))
	// 	}
	// }

	return nil
}

// getUnsyncedBatch è·å–æœªåŒæ­¥çš„æ•°æ®
func (b *BadgerDB[T]) getUnsyncedBatch(limit int) ([]*SyncQueueItem[T], error) {
	var items []*SyncQueueItem[T]

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		count := 0
		for it.Rewind(); it.Valid(); it.Next() {
			if count >= limit {
				break
			}

			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// ğŸ”§ åªè¿”å›æœªåŒæ­¥çš„æ•°æ®ï¼ˆåŒ…æ‹¬åˆ é™¤æ“ä½œï¼‰
				if !wrapper.IsSynced {
					if wrapper.Item != nil {
						if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
							hook.NewModel()
						}
					}
					items = append(items, &wrapper)
					count++
				}
				return nil
			})

			if err != nil {
				continue
			}
		}
		return nil
	})

	return items, err
}
func setHashCode(item interface{}) {
	if code, ok := item.(types.IRowCode); ok {
		code.SetHashcode(code.GetHash())
	}
}

// syncBatch æ‰¹é‡åŒæ­¥æ•°æ®
func (b *BadgerDB[T]) syncBatch(items []*SyncQueueItem[T]) ([]string, error) {
	successKeys := make([]string, 0, len(items))

	b.syncLock.RLock()
	defer b.syncLock.RUnlock()

	if !b.syncDB {
		return nil, fmt.Errorf("æœªå¼€å¯ syncDB ")
	}

	defer func() {
		if r := recover(); r != nil {
			logx.Errorf("åŒæ­¥ panic: %v", r)
		}
	}()

	for _, wrapper := range items {
		var err error
		syncAction := b.getDataAction(wrapper.Item)
		if syncAction == nil {
			logx.Errorf("æœªæ‰¾åˆ°åŒæ­¥æ“ä½œå¯¹è±¡ [%s]", wrapper.Key)
			continue
		}
		setHashCode(wrapper.Item)
		switch wrapper.Op {
		case OpInsert:
			if wrapper.Item != nil {
				err = syncAction.Insert(wrapper.Item)
				logx.Infof("åŒæ­¥æ’å…¥æ“ä½œ [%s]", wrapper.Key)
				if err != nil {
					if strings.Contains(err.Error(), "duplicate key") || strings.Contains(err.Error(), "UNIQUE constraint failed") {
						logx.Infof("æ•°æ®å·²å­˜åœ¨ï¼Œå°è¯•æ›´æ–°æ“ä½œ [%s]", wrapper.Key)
						err = nil
					}
				}
				if err == nil {
					if err1 := b.updateSyncedItem(wrapper); err1 != nil {
						logx.Errorf("æ›´æ–°å·²åŒæ­¥æ•°æ®å¤±è´¥ [%s]: %v", wrapper.Key, err1)
					}
				}
			}
		case OpUpdate:
			if wrapper.Item != nil {
				err = syncAction.Update(wrapper.Item)
				logx.Infof("åŒæ­¥æ›´æ–°æ“ä½œ [%s]", wrapper.Key)
				if err == nil {
					if err1 := b.updateSyncedItem(wrapper); err1 != nil {
						logx.Errorf("æ›´æ–°å·²åŒæ­¥æ•°æ®å¤±è´¥ [%s]: %v", wrapper.Key, err1)
					}
				}
			}
		case OpDelete:
			// ğŸ”§ åŒæ­¥åˆ é™¤æ“ä½œ
			if wrapper.Item != nil {
				err = syncAction.Delete(wrapper.Item)
				logx.Infof("åŒæ­¥åˆ é™¤æ“ä½œ [%s]", wrapper.Key)
				if err == nil {
					if err1 := b.delete(wrapper.Key, false); err1 != nil {
						logx.Errorf("ç‰©ç†åˆ é™¤å¤±è´¥ [%s]: %v", wrapper.Key, err1)
					}
				}
			}
		default:
			logx.Errorf("æœªçŸ¥æ“ä½œç±»å‹: %s", wrapper.Op)
			continue
		}

		if err != nil {
			logx.Errorf("åŒæ­¥æ•°æ®å¤±è´¥ [%s, op=%s]: %v", wrapper.Key, wrapper.Op, err)
			continue
		}

		successKeys = append(successKeys, wrapper.Key)
	}
	return successKeys, nil
}
func (b *BadgerDB[T]) updateSyncedItem(wrapper *SyncQueueItem[T]) error {
	return b.db.Update(func(txn *badger.Txn) error {
		// ğŸ”§ æ ‡è®°ä¸ºå·²åŒæ­¥
		wrapper.IsSynced = true
		wrapper.SyncedAt = time.Now()
		wrapper.Op = OpUpdate

		data, err := json.Marshal(&wrapper)
		if err != nil {
			return err
		}

		if err := txn.Set([]byte(wrapper.Key), data); err != nil {
			return err
		}
		return nil
	})
}

// handleSyncedItems å¤„ç†å·²åŒæ­¥çš„æ•°æ®
func (b *BadgerDB[T]) handleSyncedItems(keys []string) error {
	return b.db.Update(func(txn *badger.Txn) error {
		for _, key := range keys {
			item, err := txn.Get([]byte(key))
			if err != nil {
				if err == badger.ErrKeyNotFound {
					continue
				}
				return err
			}

			var wrapper SyncQueueItem[T]
			err = item.Value(func(val []byte) error {
				return json.Unmarshal(val, &wrapper)
			})
			if err != nil {
				return err
			}

			// ğŸ”§ å¦‚æœæ˜¯åˆ é™¤æ“ä½œï¼Œç‰©ç†åˆ é™¤
			if wrapper.Op == OpDelete && wrapper.IsDeleted {
				if err := txn.Delete([]byte(key)); err != nil {
					logx.Errorf("ç‰©ç†åˆ é™¤å¤±è´¥ [%s]: %v", key, err)
				}
				continue
			}

			// ğŸ”§ å¦åˆ™ï¼Œæ ‡è®°ä¸ºå·²åŒæ­¥
			wrapper.IsSynced = true
			wrapper.SyncedAt = time.Now()

			data, err := json.Marshal(&wrapper)
			if err != nil {
				return err
			}

			if err := txn.Set([]byte(key), data); err != nil {
				return err
			}
		}
		return nil
	})
}

// ManualSync æ‰‹åŠ¨è§¦å‘åŒæ­¥
func (b *BadgerDB[T]) ManualSync() error {
	b.syncLock.RLock()
	hasDB := b.syncList != nil
	b.syncLock.RUnlock()

	if !hasDB {
		return fmt.Errorf("æœªå¼€å¯ syncDB ")
	}

	return b.processSyncQueue()
}

// CleanupAfterSync æ¸…ç†å·²åŒæ­¥çš„æ•°æ®
func (b *BadgerDB[T]) CleanupAfterSync(keepDuration time.Duration) error {
	count := 0
	cutoffTime := time.Now().Add(-keepDuration)

	// ç¬¬ä¸€æ­¥ï¼šæ”¶é›†éœ€è¦åˆ é™¤çš„ key
	var keysToDelete []string

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			item := it.Item()
			key := string(item.Key())

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				count++

				// æ¸…ç†å·²åŒæ­¥ä¸”è¶…è¿‡ä¿ç•™æ—¶é—´çš„æ•°æ®
				if wrapper.IsSynced && !wrapper.SyncedAt.IsZero() && wrapper.SyncedAt.Before(cutoffTime) {
					keysToDelete = append(keysToDelete, key)
				}

				return nil
			})

			if err != nil {
				logx.Errorf("è¯»å–æ•°æ®å¤±è´¥: %v", err)
			}
		}
		return nil
	})

	if err != nil {
		return fmt.Errorf("æ”¶é›†å¾…åˆ é™¤æ•°æ®å¤±è´¥: %w", err)
	}

	if len(keysToDelete) == 0 {
		logx.Infof("æ¸…ç†å®Œæˆ: æ£€æŸ¥ %d æ¡ï¼Œæ— éœ€åˆ é™¤", count)
		return nil
	}

	// ç¬¬äºŒæ­¥ï¼šåˆ†æ‰¹åˆ é™¤
	const batchSize = 1000
	deletedCount := 0

	for i := 0; i < len(keysToDelete); i += batchSize {
		end := i + batchSize
		if end > len(keysToDelete) {
			end = len(keysToDelete)
		}

		batch := keysToDelete[i:end]

		err := b.db.Update(func(txn *badger.Txn) error {
			for _, key := range batch {
				if err := txn.Delete([]byte(key)); err != nil {
					return err
				}
				deletedCount++
			}
			return nil
		})

		if err != nil {
			logx.Errorf("æ‰¹é‡åˆ é™¤å¤±è´¥ [batch %d-%d]: %v", i, end, err)
			continue
		}
	}

	logx.Infof("æ¸…ç†å®Œæˆ: æ£€æŸ¥ %d æ¡ï¼Œåˆ é™¤ %d æ¡ï¼Œä¿ç•™ %d æ¡", count, deletedCount, count-deletedCount)
	return nil
}

func (b *BadgerDB[T]) periodicSync() {
	defer b.wg.Done()

	// ğŸ”§ ä½¿ç”¨é…ç½®ä¸­çš„é—´éš”
	ticker := time.NewTicker(b.config.PeriodicSyncInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			if err := b.db.Sync(); err != nil {
				logx.Errorf("BadgerDB sync å¤±è´¥: %v", err)
			}
		case <-b.closeCh:
			logx.Info("periodicSync é€€å‡º")
			return
		}
	}
}

// runGC åƒåœ¾å›æ”¶
func (b *BadgerDB[T]) runGC() {
	defer b.wg.Done()

	// ğŸ”§ ä½¿ç”¨é…ç½®ä¸­çš„ GC é—´éš”
	ticker := time.NewTicker(b.config.GCInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			var reclaimed int
			for {
				err := b.db.RunValueLogGC(b.config.GCDiscardRatio)
				if err != nil {
					break
				}
				reclaimed++
			}
			if reclaimed > 0 {
				logx.Infof("GC å®Œæˆï¼Œå›æ”¶ %d ä¸ªæ–‡ä»¶", reclaimed)
			}
		case <-b.closeCh:
			logx.Info("runGC é€€å‡º")
			return
		}
	}
}

// Close å…³é—­æ•°æ®åº“
func (b *BadgerDB[T]) Close() error {
	close(b.closeCh)
	b.wg.Wait()

	if err := b.db.Sync(); err != nil {
		logx.Errorf("å…³é—­å‰ sync å¤±è´¥: %v", err)
	}

	if err := b.db.Close(); err != nil {
		return fmt.Errorf("å…³é—­ BadgerDB å¤±è´¥: %w", err)
	}

	logx.Info("BadgerDB å·²å…³é—­")
	return nil
}

// badgerLogger æ—¥å¿—é€‚é…å™¨
type badgerLogger struct{}

func (l *badgerLogger) Errorf(f string, v ...interface{})   { logx.Errorf(f, v...) }
func (l *badgerLogger) Warningf(f string, v ...interface{}) { logx.Infof(f, v...) }
func (l *badgerLogger) Infof(f string, v ...interface{})    { logx.Infof(f, v...) }
func (l *badgerLogger) Debugf(f string, v ...interface{})   {}

// syncToOtherDB ä½¿ç”¨å¿«é€Ÿè®¡æ•°
func (b *BadgerDB[T]) syncToOtherDB() {
	defer b.wg.Done()

	interval := b.config.SyncInterval
	minInterval := b.config.SyncMinInterval
	maxInterval := b.config.SyncMaxInterval

	ticker := time.NewTicker(interval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			b.syncLock.RLock()
			hasDB := b.syncDB
			b.syncLock.RUnlock()

			if !hasDB {
				continue
			}

			// ğŸ”§ ä½¿ç”¨å¿«é€Ÿç¼“å­˜è®¡æ•°
			pendingCount, err := b.GetPendingSyncCountFast()
			if err != nil {
				logx.Errorf("è·å–å¾…åŒæ­¥æ•°é‡å¤±è´¥: %v", err)
				continue
			}

			if pendingCount == 0 {
				interval = min(interval*2, maxInterval)
				ticker.Reset(interval)
				continue
			}

			b.syncMutex.Lock()
			if b.syncInProgress {
				b.syncMutex.Unlock()
				logx.Info("ä¸Šæ¬¡åŒæ­¥æœªå®Œæˆï¼Œè·³è¿‡æœ¬æ¬¡")
				interval = min(interval*2, maxInterval)
				ticker.Reset(interval)
				continue
			}
			b.syncInProgress = true
			b.syncMutex.Unlock()

			start := time.Now()

			if err := b.processSyncQueue(); err != nil {
				logx.Errorf("åŒæ­¥åˆ°å…¶ä»–DBå¤±è´¥: %v", err)
			}

			duration := time.Since(start)

			b.syncMutex.Lock()
			b.syncInProgress = false
			b.syncMutex.Unlock()

			// ğŸ†• åŒæ­¥åé‡ç½®ç¼“å­˜
			b.pendingCountMutex.Lock()
			b.pendingCountCache = 0
			b.lastCountUpdate = time.Time{}
			b.pendingCountMutex.Unlock()

			if duration < interval/2 {
				interval = max(interval/2, minInterval)
			} else if duration > interval {
				interval = min(duration*2, maxInterval)
			}

			ticker.Reset(interval)
			logx.Infof("åŒæ­¥å®Œæˆ [å¤„ç†: %d, è€—æ—¶: %v, ä¸‹æ¬¡é—´éš”: %v]",
				pendingCount, duration, interval)

		case <-b.closeCh:
			b.syncMutex.Lock()
			for b.syncInProgress {
				b.syncMutex.Unlock()
				time.Sleep(100 * time.Millisecond)
				b.syncMutex.Lock()
			}
			b.syncMutex.Unlock()

			logx.Info("syncToOtherDB é€€å‡º")
			return
		}
	}
}

// autoCleanup è‡ªåŠ¨æ¸…ç†ï¼ˆæ–°æ–¹æ³•ï¼Œä½¿ç”¨é…ç½®ï¼‰
func (b *BadgerDB[T]) autoCleanup() {
	defer b.wg.Done()

	ticker := time.NewTicker(b.config.CleanupInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			// ğŸ”§ æ”¹è¿›ï¼šæ”¯æŒå¤šç§æ¸…ç†è§¦å‘æ¡ä»¶
			shouldCleanup := false
			cleanupReason := ""

			// æ¡ä»¶ 1ï¼šæ£€æŸ¥æ–‡ä»¶å¤§å°
			if b.config.SizeThreshold > 0 {
				// å…ˆåˆ·ç›˜
				if err := b.db.Sync(); err != nil {
					logx.Errorf("åŒæ­¥åˆ°ç£ç›˜å¤±è´¥: %v", err)
				}

				lsm, vlog, err := b.GetDBSize()
				if err != nil {
					logx.Errorf("è·å–æ•°æ®åº“å¤§å°å¤±è´¥: %v", err)
				} else {
					totalSize := lsm + vlog
					logx.Infof("æ•°æ®åº“å¤§å°: LSM=%dMB, VLog=%dMB, Total=%dMB",
						lsm/(1024*1024), vlog/(1024*1024), totalSize/(1024*1024))

					if totalSize >= b.config.SizeThreshold {
						shouldCleanup = true
						cleanupReason = fmt.Sprintf("æ–‡ä»¶å¤§å°è¶…è¿‡é˜ˆå€¼ (%dMB)", b.config.SizeThreshold/(1024*1024))
					}
				}
			}

			// ğŸ†• æ¡ä»¶ 2ï¼šæ£€æŸ¥å·²åŒæ­¥çš„æ—§æ•°æ®ï¼ˆé€‚åˆå°æ•°æ®é‡åœºæ™¯ï¼‰
			if !shouldCleanup {
				syncedCount, err := b.countSyncedOldData()
				if err != nil {
					logx.Errorf("ç»Ÿè®¡å·²åŒæ­¥æ—§æ•°æ®å¤±è´¥: %v", err)
				} else if syncedCount > 0 {
					shouldCleanup = true
					cleanupReason = fmt.Sprintf("å‘ç° %d æ¡å·²åŒæ­¥çš„æ—§æ•°æ®", syncedCount)
				}
			}

			// ğŸ†• æ¡ä»¶ 3ï¼šå®šæœŸå¼ºåˆ¶æ¸…ç†ï¼ˆå½“ SizeThreshold=0 æ—¶ï¼‰
			if !shouldCleanup && b.config.SizeThreshold == 0 {
				// å¼ºåˆ¶å®šæœŸæ¸…ç†
				shouldCleanup = true
				cleanupReason = "å®šæœŸæ¸…ç†ï¼ˆSizeThreshold=0ï¼‰"
			}

			if !shouldCleanup {
				continue
			}

			logx.Infof("è§¦å‘æ¸…ç†: %s", cleanupReason)

			// ğŸ”§ æ¢å¤ï¼šå…ˆç¡®ä¿æ•°æ®åŒæ­¥å®Œæˆ
			// if err := b.ManualSync(); err != nil {
			// 	logx.Errorf("åŒæ­¥å¤±è´¥: %v", err)
			// 	continue
			// }

			//time.Sleep(500 * time.Millisecond)

			// æ¸…ç†å·²åŒæ­¥çš„æ•°æ®
			if err := b.CleanupAfterSync(b.config.KeepDuration); err != nil {
				logx.Errorf("æ¸…ç†å¤±è´¥: %v", err)
				continue
			}

			// GC
			var reclaimed int
			for {
				err := b.db.RunValueLogGC(b.config.GCDiscardRatio)
				if err != nil {
					break
				}
				reclaimed++
			}

			// å†æ¬¡åˆ·ç›˜
			if err := b.db.Sync(); err != nil {
				logx.Errorf("æ¸…ç†ååŒæ­¥å¤±è´¥: %v", err)
			}

			// ç»Ÿè®¡æ¸…ç†æ•ˆæœ
			lsmAfter, vlogAfter, _ := b.GetDBSize()
			totalAfter := lsmAfter + vlogAfter

			if reclaimed > 0 {
				logx.Infof("æ¸…ç†å®Œæˆï¼Œå›æ”¶ %d ä¸ªæ–‡ä»¶ï¼Œå½“å‰å¤§å° %dMB", reclaimed, totalAfter/(1024*1024))
			} else {
				logx.Infof("æ¸…ç†å®Œæˆï¼Œå½“å‰å¤§å° %dMB", totalAfter/(1024*1024))
			}

		case <-b.closeCh:
			logx.Info("autoCleanup é€€å‡º")
			return
		}
	}
}

// ğŸ†• ç»Ÿè®¡å·²åŒæ­¥ä¸”è¶…è¿‡ä¿ç•™æœŸé™çš„æ•°æ®æ•°é‡
func (b *BadgerDB[T]) countSyncedOldData() (int, error) {
	count := 0
	cutoffTime := time.Now().Add(-b.config.KeepDuration)

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		opts.PrefetchSize = 10 // åªé¢„å–å°‘é‡æ•°æ®
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return nil // å¿½ç•¥è§£æé”™è¯¯
				}

				// ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥ä¸‰ä¸ªæ¡ä»¶
				if wrapper.IsSynced && !wrapper.SyncedAt.IsZero() && wrapper.SyncedAt.Before(cutoffTime) {
					count++
				}

				return nil
			})

			if err != nil {
				continue
			}

			// æå‰é€€å‡ºï¼ˆåªéœ€è¦çŸ¥é“æœ‰æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ•°æ®ï¼‰
			if count > 0 {
				break
			}
		}
		return nil
	})

	return count, err
}

// min/max è¾…åŠ©å‡½æ•°
func min(a, b time.Duration) time.Duration {
	if a < b {
		return a
	}
	return b
}

func max(a, b time.Duration) time.Duration {
	if a > b {
		return a
	}
	return b
}

// DropAll åˆ é™¤æ‰€æœ‰æ•°æ®ï¼ˆå±é™©æ“ä½œï¼‰
func (b *BadgerDB[T]) DropAll() error {
	return b.db.DropAll()
}

// GetDBSize è·å–æ•°æ®åº“å¤§å°
func (b *BadgerDB[T]) GetDBSize() (int64, int64, error) {
	lsm, vlog := b.db.Size()
	return lsm, vlog, nil
}

// GetStats è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
func (b *BadgerDB[T]) GetStats() string {
	lsm, vlog := b.db.Size()
	return fmt.Sprintf("LSM å¤§å°: %d MB, VLog å¤§å°: %d MB", lsm/(1024*1024), vlog/(1024*1024))
}

// Sync åŒæ­¥åˆ°ç£ç›˜
func (b *BadgerDB[T]) Sync() error {
	return b.db.Sync()
}

// Backup å¤‡ä»½æ•°æ®åº“
func (b *BadgerDB[T]) Backup(backupPath string) error {
	f, err := os.Create(backupPath)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºå¤‡ä»½æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer f.Close()

	_, err = b.db.Backup(f, 0)
	if err != nil {
		return fmt.Errorf("å¤‡ä»½å¤±è´¥: %w", err)
	}

	logx.Infof("å¤‡ä»½æˆåŠŸ: %s", backupPath)
	return nil
}

// SafeInsert å®‰å…¨æ’å…¥ï¼ˆç«‹å³åŒæ­¥åˆ°ç£ç›˜ï¼‰
func (b *BadgerDB[T]) SafeInsert(data *T) error {
	if err := b.Set(data, 0); err != nil {
		return err
	}

	if err := b.Sync(); err != nil {
		logx.Errorf("åŒæ­¥å¤±è´¥: %v", err)
	}

	return nil
}

// GetConfig è·å–å½“å‰é…ç½®
func (b *BadgerDB[T]) GetConfig() BadgerDBConfig {
	return b.config
}

// UpdateConfig æ›´æ–°é…ç½®ï¼ˆéƒ¨åˆ†å‚æ•°ï¼‰
func (b *BadgerDB[T]) UpdateConfig(updateFn func(*BadgerDBConfig)) error {
	b.syncLock.Lock()
	defer b.syncLock.Unlock()

	oldConfig := b.config
	updateFn(&b.config)

	if err := b.config.Validate(); err != nil {
		b.config = oldConfig
		return fmt.Errorf("é…ç½®æ›´æ–°å¤±è´¥: %w", err)
	}

	logx.Infof("é…ç½®å·²æ›´æ–°: %+v", b.config)
	return nil
}

// Count è·å–æ•°æ®åº“ä¸­çš„æ•°æ®æ€»æ•°ï¼ˆä¸åŒ…æ‹¬å·²åˆ é™¤çš„æ•°æ®ï¼‰
func (b *BadgerDB[T]) Count() (int, error) {
	count := 0

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return nil // å¿½ç•¥è§£æé”™è¯¯
				}

				// åªç»Ÿè®¡æœªåˆ é™¤çš„æ•°æ®
				if !wrapper.IsDeleted {
					count++
				}

				return nil
			})

			if err != nil {
				continue
			}
		}
		return nil
	})

	return count, err
}

// CountAll è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ•°æ®æ€»æ•°ï¼ˆåŒ…æ‹¬å·²åˆ é™¤çš„æ•°æ®ï¼‰
func (b *BadgerDB[T]) CountAll() (int, error) {
	count := 0

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = false // ä¸éœ€è¦è¯»å–å€¼
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			count++
		}
		return nil
	})

	return count, err
}

// CountByPrefix ç»Ÿè®¡æŒ‡å®šå‰ç¼€çš„æ•°æ®æ•°é‡ï¼ˆä¸åŒ…æ‹¬å·²åˆ é™¤ï¼‰
func (b *BadgerDB[T]) CountByPrefix(prefix string) (int, error) {
	count := 0

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Seek([]byte(prefix)); it.ValidForPrefix([]byte(prefix)); it.Next() {
			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return nil
				}

				if !wrapper.IsDeleted {
					count++
				}

				return nil
			})

			if err != nil {
				continue
			}
		}
		return nil
	})

	return count, err
}

// GetStatistics è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
func (b *BadgerDB[T]) GetStatistics() (*DBStatistics, error) {
	stats := &DBStatistics{}

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return nil
				}

				stats.TotalCount++

				if wrapper.IsDeleted {
					stats.DeletedCount++
				} else {
					stats.ActiveCount++
				}

				if !wrapper.IsSynced {
					stats.UnsyncedCount++
				} else {
					stats.SyncedCount++
				}

				return nil
			})

			if err != nil {
				continue
			}
		}
		return nil
	})

	if err != nil {
		return nil, err
	}

	// è·å–æ•°æ®åº“å¤§å°
	lsm, vlog, _ := b.GetDBSize()
	stats.LSMSize = lsm
	stats.VLogSize = vlog
	stats.TotalSize = lsm + vlog

	return stats, nil
}

// DBStatistics æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
type DBStatistics struct {
	TotalCount    int   `json:"total_count"`    // æ€»æ•°æ®é‡
	ActiveCount   int   `json:"active_count"`   // æ´»è·ƒæ•°æ®ï¼ˆæœªåˆ é™¤ï¼‰
	DeletedCount  int   `json:"deleted_count"`  // å·²åˆ é™¤æ•°æ®
	SyncedCount   int   `json:"synced_count"`   // å·²åŒæ­¥æ•°æ®
	UnsyncedCount int   `json:"unsynced_count"` // æœªåŒæ­¥æ•°æ®
	LSMSize       int64 `json:"lsm_size"`       // LSM å¤§å°ï¼ˆå­—èŠ‚ï¼‰
	VLogSize      int64 `json:"vlog_size"`      // VLog å¤§å°ï¼ˆå­—èŠ‚ï¼‰
	TotalSize     int64 `json:"total_size"`     // æ€»å¤§å°ï¼ˆå­—èŠ‚ï¼‰
}

// String æ ¼å¼åŒ–è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
func (s *DBStatistics) String() string {
	return fmt.Sprintf(
		"æ€»æ•°: %d, æ´»è·ƒ: %d, å·²åˆ é™¤: %d, å·²åŒæ­¥: %d, æœªåŒæ­¥: %d, LSM: %dMB, VLog: %dMB, æ€»å¤§å°: %dMB",
		s.TotalCount,
		s.ActiveCount,
		s.DeletedCount,
		s.SyncedCount,
		s.UnsyncedCount,
		s.LSMSize/(1024*1024),
		s.VLogSize/(1024*1024),
		s.TotalSize/(1024*1024),
	)
}
