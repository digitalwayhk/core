package nosql

import (
	"encoding/json"
	"fmt"
	"os"
	"sync"
	"time"

	"github.com/dgraph-io/badger/v3"
	"github.com/digitalwayhk/core/pkg/persistence/types"
	"github.com/zeromicro/go-zero/core/logx"
)

type OpType string

const (
	OpInsert OpType = "insert"
	OpUpdate OpType = "update"
	OpDelete OpType = "delete" // ğŸ”§ åˆ é™¤æ“ä½œ
)

// SyncQueueItem åŒæ­¥é˜Ÿåˆ—é¡¹ï¼ˆåŒ…è£…æ•°æ®ï¼‰
type SyncQueueItem[T types.IModel] struct {
	Key       string    `json:"key"`
	Item      *T        `json:"item,omitempty"`
	Op        OpType    `json:"op"`
	CreatedAt time.Time `json:"created_at"`
	UpdatedAt time.Time `json:"updated_at"`
	IsSynced  bool      `json:"is_synced"`
	SyncedAt  time.Time `json:"synced_at,omitempty"`
	IsDeleted bool      `json:"is_deleted"`
	DeletedAt time.Time `json:"deleted_at,omitempty"`
}

// BadgerDB æ³›å‹ KV æ•°æ®åº“
type BadgerDB[T types.IModel] struct {
	db             *badger.DB
	path           string
	config         BadgerDBConfig // ğŸ†• é…ç½®
	syncDB         types.IDataAction
	syncLock       sync.RWMutex
	syncMutex      sync.Mutex
	syncInProgress bool
	closeCh        chan struct{}
	wg             sync.WaitGroup
	syncOnce       sync.Once
	cleanupOnce    sync.Once // ğŸ†• æ¸…ç†å¯åŠ¨æ§åˆ¶
	bufferPool     sync.Pool
}

// NewBadgerDB åˆ›å»ºç”Ÿäº§ç¯å¢ƒ BadgerDBï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
func NewBadgerDB[T types.IModel](path string) (*BadgerDB[T], error) {
	config := DefaultProductionConfig(path)
	return NewBadgerDBWithConfig[T](config)
}

// NewBadgerDBFast åˆ›å»ºå¿«é€Ÿæ¨¡å¼ BadgerDBï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
func NewBadgerDBFast[T types.IModel](path string) (*BadgerDB[T], error) {
	config := DefaultFastConfig(path)
	config.PeriodicSync = true
	return NewBadgerDBWithConfig[T](config)
}

// NewBadgerDBWithConfig ä½¿ç”¨é…ç½®åˆ›å»º BadgerDB
func NewBadgerDBWithConfig[T types.IModel](config BadgerDBConfig) (*BadgerDB[T], error) {
	// éªŒè¯é…ç½®
	if err := config.Validate(); err != nil {
		return nil, fmt.Errorf("é…ç½®éªŒè¯å¤±è´¥: %w", err)
	}

	// æ„å»º BadgerDB é€‰é¡¹
	opts := badger.DefaultOptions(config.Path).
		WithSyncWrites(config.SyncWrites).
		WithDetectConflicts(config.DetectConflicts).
		WithNumVersionsToKeep(1).
		WithNumCompactors(config.NumCompactors).
		WithCompactL0OnClose(true).
		WithNumLevelZeroTables(config.NumLevelZeroTables).
		WithNumLevelZeroTablesStall(config.NumLevelZeroStall).
		WithValueLogFileSize(config.ValueLogFileSize).
		WithMemTableSize(config.MemTableSize).
		WithValueThreshold(config.ValueThreshold)

	// é…ç½®æ—¥å¿—
	if config.EnableLogger {
		opts = opts.WithLogger(&badgerLogger{})
	} else {
		opts = opts.WithLogger(nil)
	}

	// æ‰“å¼€æ•°æ®åº“
	db, err := badger.Open(opts)
	if err != nil {
		return nil, fmt.Errorf("æ‰“å¼€ BadgerDB å¤±è´¥: %w", err)
	}

	b := &BadgerDB[T]{
		db:      db,
		path:    config.Path,
		config:  config,
		closeCh: make(chan struct{}),
		bufferPool: sync.Pool{
			New: func() interface{} {
				return make([]byte, 0, 1024)
			},
		},
	}

	// å¯åŠ¨ GC
	b.wg.Add(1)
	go b.runGC()

	// å¯åŠ¨å®šæœŸç£ç›˜åŒæ­¥ï¼ˆFast æ¨¡å¼ï¼‰
	if config.PeriodicSync {
		b.wg.Add(1)
		go b.periodicSync()
	}

	logx.Infof("BadgerDB å·²å¯åŠ¨ [mode=%s, path=%s, autoSync=%v, autoCleanup=%v]",
		config.Mode, config.Path, config.AutoSync, config.AutoCleanup)

	return b, nil
}

// SetSyncDB è®¾ç½®åŒæ­¥æ•°æ®åº“
func (b *BadgerDB[T]) SetSyncDB(action types.IDataAction) {
	b.syncLock.Lock()
	defer b.syncLock.Unlock()

	if b.syncDB != nil {
		logx.Error("syncDB å·²è®¾ç½®ï¼Œè·³è¿‡")
		return
	}

	b.syncDB = action

	if action != nil {
		// ğŸ”§ å¯åŠ¨è‡ªåŠ¨åŒæ­¥
		if b.config.AutoSync {
			b.syncOnce.Do(func() {
				b.wg.Add(1)
				go b.syncToOtherDB()
				logx.Info("è‡ªåŠ¨åŒæ­¥å·²å¯åŠ¨")
			})
		}

		// ğŸ”§ å¯åŠ¨è‡ªåŠ¨æ¸…ç†
		if b.config.AutoCleanup {
			b.cleanupOnce.Do(func() {
				b.wg.Add(1)
				go b.autoCleanup()
				logx.Info("è‡ªåŠ¨æ¸…ç†å·²å¯åŠ¨")
			})
		}
	}
}

// generateKey ç”Ÿæˆ key
func (b *BadgerDB[T]) generateKey(item *T) string {
	if item == nil {
		return ""
	}
	if rowCode, ok := any(item).(types.IRowCode); ok {
		return rowCode.GetHash()
	}
	return ""
}

// Set å†™å…¥æ•°æ®
func (b *BadgerDB[T]) Set(item *T, ttl time.Duration, fn ...func(wrapper *SyncQueueItem[T])) error {
	if item == nil {
		return fmt.Errorf("item ä¸èƒ½ä¸ºç©º")
	}

	key := b.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}

	b.syncLock.RLock()
	needSync := b.syncDB != nil
	b.syncLock.RUnlock()

	// ğŸ”§ æ£€æŸ¥æ˜¯æ’å…¥è¿˜æ˜¯æ›´æ–°
	op := OpInsert
	existingWrapper, err := b.getWrapper(key)
	if err == nil && existingWrapper != nil && !existingWrapper.IsDeleted {
		op = OpUpdate
	}

	// ğŸ”§ åˆ›å»ºåŒ…è£…å¯¹è±¡
	now := time.Now()
	wrapper := &SyncQueueItem[T]{
		Key:       key,
		Item:      item,
		Op:        op,
		CreatedAt: now,
		UpdatedAt: now,
		IsSynced:  !needSync,
		IsDeleted: false,
	}

	// ğŸ”§ ä¿ç•™åˆ›å»ºæ—¶é—´ï¼ˆå¦‚æœæ˜¯æ›´æ–°ï¼‰
	if op == OpUpdate && existingWrapper != nil {
		wrapper.CreatedAt = existingWrapper.CreatedAt
	}

	// åºåˆ—åŒ–
	data, err := json.Marshal(wrapper)
	if err != nil {
		return fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
	}
	if len(fn) > 0 {
		fn[0](wrapper)
	}
	// å†™å…¥æ•°æ®åº“
	return b.db.Update(func(txn *badger.Txn) error {
		entry := badger.NewEntry([]byte(key), data)
		if ttl > 0 {
			entry = entry.WithTTL(ttl)
		}
		return txn.SetEntry(entry)
	})
}

// BatchInsert æ‰¹é‡æ’å…¥
func (b *BadgerDB[T]) BatchInsert(items []*T) error {
	if len(items) == 0 {
		return nil
	}

	b.syncLock.RLock()
	needSync := b.syncDB != nil
	b.syncLock.RUnlock()

	now := time.Now()

	type serializedItem struct {
		key   string
		value []byte
	}

	serialized := make([]serializedItem, 0, len(items))

	for _, item := range items {
		if item == nil {
			continue
		}

		key := b.generateKey(item)
		if key == "" {
			return badger.ErrEmptyKey
		}

		// ğŸ”§ åˆ›å»ºåŒ…è£…å¯¹è±¡
		wrapper := &SyncQueueItem[T]{
			Key:       key,
			Item:      item,
			Op:        OpInsert,
			CreatedAt: now,
			UpdatedAt: now,
			IsSynced:  !needSync,
			IsDeleted: false,
		}

		value, err := json.Marshal(wrapper)
		if err != nil {
			return fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
		}

		serialized = append(serialized, serializedItem{
			key:   key,
			value: value,
		})
	}

	// æ‰¹é‡å†™å…¥
	const maxRetries = 3
	var lastErr error

	for retry := 0; retry < maxRetries; retry++ {
		txn := b.db.NewTransaction(true)
		success := true

		for _, si := range serialized {
			if err := txn.Set([]byte(si.key), si.value); err != nil {
				if err == badger.ErrTxnTooBig {
					if commitErr := txn.Commit(); commitErr != nil {
						lastErr = commitErr
						success = false
						break
					}
					txn = b.db.NewTransaction(true)
					if err := txn.Set([]byte(si.key), si.value); err != nil {
						txn.Discard()
						lastErr = err
						success = false
						break
					}
				} else {
					txn.Discard()
					lastErr = err
					success = false
					break
				}
			}
		}

		if success {
			if err := txn.Commit(); err != nil {
				lastErr = err
				time.Sleep(time.Millisecond * 100 * time.Duration(retry+1))
				continue
			}
			return nil
		}

		txn.Discard()
		time.Sleep(time.Millisecond * 100 * time.Duration(retry+1))
	}

	return lastErr
}

// Delete åˆ é™¤æ•°æ®ï¼ˆæ”¯æŒè½¯åˆ é™¤ï¼‰
func (b *BadgerDB[T]) Delete(key string) error {
	b.syncLock.RLock()
	needSync := b.syncDB != nil
	b.syncLock.RUnlock()

	if !needSync {
		// ğŸ”§ ä¸éœ€è¦åŒæ­¥ï¼Œç›´æ¥ç‰©ç†åˆ é™¤
		return b.db.Update(func(txn *badger.Txn) error {
			return txn.Delete([]byte(key))
		})
	}

	// ğŸ”§ éœ€è¦åŒæ­¥ï¼Œæ‰§è¡Œè½¯åˆ é™¤
	return b.db.Update(func(txn *badger.Txn) error {
		// è¯»å–ç°æœ‰æ•°æ®
		item, err := txn.Get([]byte(key))
		if err != nil {
			if err == badger.ErrKeyNotFound {
				return nil // æ•°æ®ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›
			}
			return err
		}

		var wrapper SyncQueueItem[T]
		err = item.Value(func(val []byte) error {
			return json.Unmarshal(val, &wrapper)
		})
		if err != nil {
			return err
		}

		// ğŸ”§ å¦‚æœå·²ç»æ˜¯åˆ é™¤çŠ¶æ€ï¼Œç›´æ¥è¿”å›
		if wrapper.IsDeleted {
			return nil
		}

		// ğŸ”§ æ ‡è®°ä¸ºè½¯åˆ é™¤
		now := time.Now()
		wrapper.Op = OpDelete
		wrapper.IsDeleted = true
		wrapper.DeletedAt = now
		wrapper.UpdatedAt = now
		wrapper.IsSynced = false // éœ€è¦åŒæ­¥åˆ é™¤æ“ä½œ

		// å†™å›
		data, err := json.Marshal(&wrapper)
		if err != nil {
			return fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
		}

		return txn.Set([]byte(key), data)
	})
}

// Get è·å–æ•°æ®ï¼ˆè¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®ï¼‰
func (b *BadgerDB[T]) Get(key string) (*T, error) {
	wrapper, err := b.getWrapper(key)
	if err != nil {
		return nil, err
	}

	// ğŸ”§ è¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®
	if wrapper.IsDeleted {
		return nil, badger.ErrKeyNotFound
	}

	return wrapper.Item, nil
}

// getWrapper è·å–åŒ…è£…å¯¹è±¡ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
func (b *BadgerDB[T]) getWrapper(key string) (*SyncQueueItem[T], error) {
	var wrapper = new(SyncQueueItem[T])

	err := b.db.View(func(txn *badger.Txn) error {
		item, err := txn.Get([]byte(key))
		if err != nil {
			return err
		}

		return item.Value(func(val []byte) error {
			return json.Unmarshal(val, wrapper)
		})
	})

	if err != nil {
		return nil, err
	}

	// ğŸ”§ åˆå§‹åŒ– Item
	if wrapper.Item != nil {
		if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
			hook.NewModel()
		}
	}

	return wrapper, nil
}

// Scan æ‰«ææ•°æ®ï¼ˆè¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®ï¼‰
func (b *BadgerDB[T]) Scan(prefix string, limit int) ([]*T, error) {
	var results []*T

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		count := 0
		for it.Seek([]byte(prefix)); it.ValidForPrefix([]byte(prefix)); it.Next() {
			if count >= limit {
				break
			}

			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// ğŸ”§ è¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®
				if wrapper.IsDeleted {
					return nil
				}

				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					results = append(results, wrapper.Item)
					count++
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})

	return results, err
}

// GetAll è·å–æ‰€æœ‰æ•°æ®ï¼ˆè¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®ï¼‰
func (b *BadgerDB[T]) GetAll() ([]*T, error) {
	var results []*T

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// ğŸ”§ è¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®
				if wrapper.IsDeleted {
					return nil
				}

				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					results = append(results, wrapper.Item)
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})

	return results, err
}

// GetPendingSyncCount è·å–å¾…åŒæ­¥æ•°é‡
func (b *BadgerDB[T]) GetPendingSyncCount() (int, error) {
	count := 0

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// ğŸ”§ ç»Ÿè®¡æœªåŒæ­¥çš„æ•°æ®ï¼ˆåŒ…æ‹¬åˆ é™¤æ“ä½œï¼‰
				if !wrapper.IsSynced {
					count++
				}
				return nil
			})

			if err != nil {
				continue
			}
		}
		return nil
	})

	return count, err
}

// processSyncQueue å¤„ç†åŒæ­¥é˜Ÿåˆ—
func (b *BadgerDB[T]) processSyncQueue() error {
	// ğŸ”§ ä½¿ç”¨é…ç½®ä¸­çš„æ‰¹æ¬¡å¤§å°
	unsyncedItems, err := b.getUnsyncedBatch(b.config.SyncBatchSize)
	if err != nil {
		return fmt.Errorf("è·å–æœªåŒæ­¥æ•°æ®å¤±è´¥: %w", err)
	}

	if len(unsyncedItems) == 0 {
		return nil
	}

	logx.Infof("å¼€å§‹åŒæ­¥ %d æ¡æ•°æ®åˆ°å…¶ä»–DB", len(unsyncedItems))

	successKeys, err := b.syncBatch(unsyncedItems)
	if err != nil {
		logx.Errorf("æ‰¹é‡åŒæ­¥å¤±è´¥: %v", err)
	}

	if len(successKeys) > 0 {
		if err := b.handleSyncedItems(successKeys); err != nil {
			logx.Errorf("å¤„ç†å·²åŒæ­¥æ•°æ®å¤±è´¥: %v", err)
		} else {
			logx.Infof("æˆåŠŸåŒæ­¥ %d æ¡æ•°æ®", len(successKeys))
		}
	}

	return nil
}

// getUnsyncedBatch è·å–æœªåŒæ­¥çš„æ•°æ®
func (b *BadgerDB[T]) getUnsyncedBatch(limit int) ([]*SyncQueueItem[T], error) {
	var items []*SyncQueueItem[T]

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		count := 0
		for it.Rewind(); it.Valid(); it.Next() {
			if count >= limit {
				break
			}

			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// ğŸ”§ åªè¿”å›æœªåŒæ­¥çš„æ•°æ®ï¼ˆåŒ…æ‹¬åˆ é™¤æ“ä½œï¼‰
				if !wrapper.IsSynced {
					if wrapper.Item != nil {
						if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
							hook.NewModel()
						}
					}
					items = append(items, &wrapper)
					count++
				}
				return nil
			})

			if err != nil {
				continue
			}
		}
		return nil
	})

	return items, err
}

// syncBatch æ‰¹é‡åŒæ­¥æ•°æ®
func (b *BadgerDB[T]) syncBatch(items []*SyncQueueItem[T]) ([]string, error) {
	successKeys := make([]string, 0, len(items))

	b.syncLock.RLock()
	defer b.syncLock.RUnlock()

	if b.syncDB == nil {
		return nil, fmt.Errorf("syncDB æœªé…ç½®")
	}

	b.syncDB.Transaction()
	defer func() {
		if r := recover(); r != nil {
			logx.Errorf("åŒæ­¥ panic: %v", r)
		}
	}()

	for _, wrapper := range items {
		var err error

		switch wrapper.Op {
		case OpInsert:
			if wrapper.Item != nil {
				err = b.syncDB.Insert(wrapper.Item)
			}
		case OpUpdate:
			if wrapper.Item != nil {
				err = b.syncDB.Update(wrapper.Item)
			}
		case OpDelete:
			// ğŸ”§ åŒæ­¥åˆ é™¤æ“ä½œ
			if wrapper.Item != nil {
				err = b.syncDB.Delete(wrapper.Item)
			}
		default:
			logx.Errorf("æœªçŸ¥æ“ä½œç±»å‹: %s", wrapper.Op)
			continue
		}

		if err != nil {
			logx.Errorf("åŒæ­¥æ•°æ®å¤±è´¥ [%s, op=%s]: %v", wrapper.Key, wrapper.Op, err)
			continue
		}

		successKeys = append(successKeys, wrapper.Key)
	}

	if err := b.syncDB.Commit(); err != nil {
		return nil, fmt.Errorf("æäº¤åŒæ­¥äº‹åŠ¡å¤±è´¥: %w", err)
	}

	return successKeys, nil
}

// handleSyncedItems å¤„ç†å·²åŒæ­¥çš„æ•°æ®
func (b *BadgerDB[T]) handleSyncedItems(keys []string) error {
	return b.db.Update(func(txn *badger.Txn) error {
		for _, key := range keys {
			item, err := txn.Get([]byte(key))
			if err != nil {
				if err == badger.ErrKeyNotFound {
					continue
				}
				return err
			}

			var wrapper SyncQueueItem[T]
			err = item.Value(func(val []byte) error {
				return json.Unmarshal(val, &wrapper)
			})
			if err != nil {
				return err
			}

			// ğŸ”§ å¦‚æœæ˜¯åˆ é™¤æ“ä½œï¼Œç‰©ç†åˆ é™¤
			if wrapper.Op == OpDelete && wrapper.IsDeleted {
				if err := txn.Delete([]byte(key)); err != nil {
					logx.Errorf("ç‰©ç†åˆ é™¤å¤±è´¥ [%s]: %v", key, err)
				}
				continue
			}

			// ğŸ”§ å¦åˆ™ï¼Œæ ‡è®°ä¸ºå·²åŒæ­¥
			wrapper.IsSynced = true
			wrapper.SyncedAt = time.Now()

			data, err := json.Marshal(&wrapper)
			if err != nil {
				return err
			}

			if err := txn.Set([]byte(key), data); err != nil {
				return err
			}
		}
		return nil
	})
}

// ManualSync æ‰‹åŠ¨è§¦å‘åŒæ­¥
func (b *BadgerDB[T]) ManualSync() error {
	b.syncLock.RLock()
	hasDB := b.syncDB != nil
	b.syncLock.RUnlock()

	if !hasDB {
		return fmt.Errorf("syncDB æœªé…ç½®")
	}

	return b.processSyncQueue()
}

// CleanupAfterSync æ¸…ç†å·²åŒæ­¥çš„æ•°æ®
func (b *BadgerDB[T]) CleanupAfterSync(keepDuration time.Duration) error {
	count := 0
	deletedCount := 0
	cutoffTime := time.Now().Add(-keepDuration)

	err := b.db.Update(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			item := it.Item()
			key := item.Key()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				count++

				// ğŸ”§ æ¸…ç†å·²åŒæ­¥ä¸”è¶…è¿‡ä¿ç•™æ—¶é—´çš„æ•°æ®
				if wrapper.IsSynced && !wrapper.SyncedAt.IsZero() && wrapper.SyncedAt.Before(cutoffTime) {
					if err := txn.Delete(key); err != nil {
						return err
					}
					deletedCount++
				}

				return nil
			})

			if err != nil {
				logx.Errorf("æ¸…ç†æ•°æ®å¤±è´¥: %v", err)
			}
		}
		return nil
	})

	if err != nil {
		return fmt.Errorf("æ¸…ç†å¤±è´¥: %w", err)
	}

	logx.Infof("æ¸…ç†å®Œæˆ: æ£€æŸ¥ %d æ¡ï¼Œåˆ é™¤ %d æ¡ï¼Œä¿ç•™ %d æ¡", count, deletedCount, count-deletedCount)
	return nil
}

func (b *BadgerDB[T]) periodicSync() {
	defer b.wg.Done()

	// ğŸ”§ ä½¿ç”¨é…ç½®ä¸­çš„é—´éš”
	ticker := time.NewTicker(b.config.PeriodicSyncInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			if err := b.db.Sync(); err != nil {
				logx.Errorf("BadgerDB sync å¤±è´¥: %v", err)
			}
		case <-b.closeCh:
			logx.Info("periodicSync é€€å‡º")
			return
		}
	}
}

// runGC åƒåœ¾å›æ”¶
func (b *BadgerDB[T]) runGC() {
	defer b.wg.Done()

	// ğŸ”§ ä½¿ç”¨é…ç½®ä¸­çš„ GC é—´éš”
	ticker := time.NewTicker(b.config.GCInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			var reclaimed int
			for {
				err := b.db.RunValueLogGC(b.config.GCDiscardRatio)
				if err != nil {
					break
				}
				reclaimed++
			}
			if reclaimed > 0 {
				logx.Infof("GC å®Œæˆï¼Œå›æ”¶ %d ä¸ªæ–‡ä»¶", reclaimed)
			}
		case <-b.closeCh:
			logx.Info("runGC é€€å‡º")
			return
		}
	}
}

// Close å…³é—­æ•°æ®åº“
func (b *BadgerDB[T]) Close() error {
	close(b.closeCh)
	b.wg.Wait()

	if err := b.db.Sync(); err != nil {
		logx.Errorf("å…³é—­å‰ sync å¤±è´¥: %v", err)
	}

	if err := b.db.Close(); err != nil {
		return fmt.Errorf("å…³é—­ BadgerDB å¤±è´¥: %w", err)
	}

	logx.Info("BadgerDB å·²å…³é—­")
	return nil
}

// badgerLogger æ—¥å¿—é€‚é…å™¨
type badgerLogger struct{}

func (l *badgerLogger) Errorf(f string, v ...interface{})   { logx.Errorf(f, v...) }
func (l *badgerLogger) Warningf(f string, v ...interface{}) { logx.Infof(f, v...) }
func (l *badgerLogger) Infof(f string, v ...interface{})    { logx.Infof(f, v...) }
func (l *badgerLogger) Debugf(f string, v ...interface{})   {}

// syncToOtherDB åŒæ­¥åˆ°å…¶ä»–æ•°æ®åº“
func (b *BadgerDB[T]) syncToOtherDB() {
	defer b.wg.Done()

	// ğŸ”§ ä½¿ç”¨é…ç½®ä¸­çš„é—´éš”å‚æ•°
	interval := b.config.SyncInterval
	minInterval := b.config.SyncMinInterval
	maxInterval := b.config.SyncMaxInterval

	ticker := time.NewTicker(interval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			b.syncLock.RLock()
			hasDB := b.syncDB != nil
			b.syncLock.RUnlock()

			if !hasDB {
				continue
			}

			b.syncMutex.Lock()
			if b.syncInProgress {
				b.syncMutex.Unlock()
				logx.Info("ä¸Šæ¬¡åŒæ­¥æœªå®Œæˆï¼Œè·³è¿‡æœ¬æ¬¡")
				interval = min(interval*2, maxInterval)
				ticker.Reset(interval)
				continue
			}
			b.syncInProgress = true
			b.syncMutex.Unlock()

			start := time.Now()

			if err := b.processSyncQueue(); err != nil {
				logx.Errorf("åŒæ­¥åˆ°å…¶ä»–DBå¤±è´¥: %v", err)
			}

			duration := time.Since(start)

			b.syncMutex.Lock()
			b.syncInProgress = false
			b.syncMutex.Unlock()

			// ğŸ”§ è‡ªé€‚åº”è°ƒæ•´é—´éš”
			if duration < interval/2 {
				interval = max(interval/2, minInterval)
			} else if duration > interval {
				interval = min(duration*2, maxInterval)
			}

			ticker.Reset(interval)
			logx.Infof("åŒæ­¥å®Œæˆï¼Œè€—æ—¶ %vï¼Œä¸‹æ¬¡é—´éš” %v", duration, interval)

		case <-b.closeCh:
			b.syncMutex.Lock()
			for b.syncInProgress {
				b.syncMutex.Unlock()
				time.Sleep(100 * time.Millisecond)
				b.syncMutex.Lock()
			}
			b.syncMutex.Unlock()

			logx.Info("syncToOtherDB é€€å‡º")
			return
		}
	}
}

// autoCleanup è‡ªåŠ¨æ¸…ç†ï¼ˆæ–°æ–¹æ³•ï¼Œä½¿ç”¨é…ç½®ï¼‰
func (b *BadgerDB[T]) autoCleanup() {
	defer b.wg.Done()

	ticker := time.NewTicker(b.config.CleanupInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			// ğŸ”§ æ”¹è¿›ï¼šæ”¯æŒå¤šç§æ¸…ç†è§¦å‘æ¡ä»¶
			shouldCleanup := false
			cleanupReason := ""

			// æ¡ä»¶ 1ï¼šæ£€æŸ¥æ–‡ä»¶å¤§å°
			if b.config.SizeThreshold > 0 {
				// å…ˆåˆ·ç›˜
				if err := b.db.Sync(); err != nil {
					logx.Errorf("åŒæ­¥åˆ°ç£ç›˜å¤±è´¥: %v", err)
				}

				lsm, vlog, err := b.GetDBSize()
				if err != nil {
					logx.Errorf("è·å–æ•°æ®åº“å¤§å°å¤±è´¥: %v", err)
				} else {
					totalSize := lsm + vlog
					logx.Infof("æ•°æ®åº“å¤§å°: LSM=%dMB, VLog=%dMB, Total=%dMB",
						lsm/(1024*1024), vlog/(1024*1024), totalSize/(1024*1024))

					if totalSize >= b.config.SizeThreshold {
						shouldCleanup = true
						cleanupReason = fmt.Sprintf("æ–‡ä»¶å¤§å°è¶…è¿‡é˜ˆå€¼ (%dMB)", b.config.SizeThreshold/(1024*1024))
					}
				}
			}

			// ğŸ†• æ¡ä»¶ 2ï¼šæ£€æŸ¥å·²åŒæ­¥çš„æ—§æ•°æ®ï¼ˆé€‚åˆå°æ•°æ®é‡åœºæ™¯ï¼‰
			if !shouldCleanup {
				syncedCount, err := b.countSyncedOldData()
				if err != nil {
					logx.Errorf("ç»Ÿè®¡å·²åŒæ­¥æ—§æ•°æ®å¤±è´¥: %v", err)
				} else if syncedCount > 0 {
					shouldCleanup = true
					cleanupReason = fmt.Sprintf("å‘ç° %d æ¡å·²åŒæ­¥çš„æ—§æ•°æ®", syncedCount)
				}
			}

			// ğŸ†• æ¡ä»¶ 3ï¼šå®šæœŸå¼ºåˆ¶æ¸…ç†ï¼ˆå½“ SizeThreshold=0 æ—¶ï¼‰
			if !shouldCleanup && b.config.SizeThreshold == 0 {
				// å¼ºåˆ¶å®šæœŸæ¸…ç†
				shouldCleanup = true
				cleanupReason = "å®šæœŸæ¸…ç†ï¼ˆSizeThreshold=0ï¼‰"
			}

			if !shouldCleanup {
				continue
			}

			logx.Infof("è§¦å‘æ¸…ç†: %s", cleanupReason)

			// ğŸ”§ æ¢å¤ï¼šå…ˆç¡®ä¿æ•°æ®åŒæ­¥å®Œæˆ
			// if err := b.ManualSync(); err != nil {
			// 	logx.Errorf("åŒæ­¥å¤±è´¥: %v", err)
			// 	continue
			// }

			//time.Sleep(500 * time.Millisecond)

			// æ¸…ç†å·²åŒæ­¥çš„æ•°æ®
			if err := b.CleanupAfterSync(b.config.KeepDuration); err != nil {
				logx.Errorf("æ¸…ç†å¤±è´¥: %v", err)
				continue
			}

			// GC
			var reclaimed int
			for {
				err := b.db.RunValueLogGC(b.config.GCDiscardRatio)
				if err != nil {
					break
				}
				reclaimed++
			}

			// å†æ¬¡åˆ·ç›˜
			if err := b.db.Sync(); err != nil {
				logx.Errorf("æ¸…ç†ååŒæ­¥å¤±è´¥: %v", err)
			}

			// ç»Ÿè®¡æ¸…ç†æ•ˆæœ
			lsmAfter, vlogAfter, _ := b.GetDBSize()
			totalAfter := lsmAfter + vlogAfter

			if reclaimed > 0 {
				logx.Infof("æ¸…ç†å®Œæˆï¼Œå›æ”¶ %d ä¸ªæ–‡ä»¶ï¼Œå½“å‰å¤§å° %dMB", reclaimed, totalAfter/(1024*1024))
			} else {
				logx.Infof("æ¸…ç†å®Œæˆï¼Œå½“å‰å¤§å° %dMB", totalAfter/(1024*1024))
			}

		case <-b.closeCh:
			logx.Info("autoCleanup é€€å‡º")
			return
		}
	}
}

// ğŸ†• ç»Ÿè®¡å·²åŒæ­¥ä¸”è¶…è¿‡ä¿ç•™æœŸé™çš„æ•°æ®æ•°é‡
func (b *BadgerDB[T]) countSyncedOldData() (int, error) {
	count := 0
	cutoffTime := time.Now().Add(-b.config.KeepDuration)

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		opts.PrefetchSize = 10 // åªé¢„å–å°‘é‡æ•°æ®
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return nil // å¿½ç•¥è§£æé”™è¯¯
				}

				// ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥ä¸‰ä¸ªæ¡ä»¶
				if wrapper.IsSynced && !wrapper.SyncedAt.IsZero() && wrapper.SyncedAt.Before(cutoffTime) {
					count++
				}

				return nil
			})

			if err != nil {
				continue
			}

			// æå‰é€€å‡ºï¼ˆåªéœ€è¦çŸ¥é“æœ‰æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ•°æ®ï¼‰
			if count > 0 {
				break
			}
		}
		return nil
	})

	return count, err
}

// min/max è¾…åŠ©å‡½æ•°
func min(a, b time.Duration) time.Duration {
	if a < b {
		return a
	}
	return b
}

func max(a, b time.Duration) time.Duration {
	if a > b {
		return a
	}
	return b
}

// DropAll åˆ é™¤æ‰€æœ‰æ•°æ®ï¼ˆå±é™©æ“ä½œï¼‰
func (b *BadgerDB[T]) DropAll() error {
	return b.db.DropAll()
}

// GetDBSize è·å–æ•°æ®åº“å¤§å°
func (b *BadgerDB[T]) GetDBSize() (int64, int64, error) {
	lsm, vlog := b.db.Size()
	return lsm, vlog, nil
}

// GetStats è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
func (b *BadgerDB[T]) GetStats() string {
	lsm, vlog := b.db.Size()
	return fmt.Sprintf("LSM å¤§å°: %d MB, VLog å¤§å°: %d MB", lsm/(1024*1024), vlog/(1024*1024))
}

// Sync åŒæ­¥åˆ°ç£ç›˜
func (b *BadgerDB[T]) Sync() error {
	return b.db.Sync()
}

// Backup å¤‡ä»½æ•°æ®åº“
func (b *BadgerDB[T]) Backup(backupPath string) error {
	f, err := os.Create(backupPath)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºå¤‡ä»½æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer f.Close()

	_, err = b.db.Backup(f, 0)
	if err != nil {
		return fmt.Errorf("å¤‡ä»½å¤±è´¥: %w", err)
	}

	logx.Infof("å¤‡ä»½æˆåŠŸ: %s", backupPath)
	return nil
}

// SafeInsert å®‰å…¨æ’å…¥ï¼ˆç«‹å³åŒæ­¥åˆ°ç£ç›˜ï¼‰
func (b *BadgerDB[T]) SafeInsert(data *T) error {
	if err := b.Set(data, 0); err != nil {
		return err
	}

	if err := b.Sync(); err != nil {
		logx.Errorf("åŒæ­¥å¤±è´¥: %v", err)
	}

	return nil
}

// GetConfig è·å–å½“å‰é…ç½®
func (b *BadgerDB[T]) GetConfig() BadgerDBConfig {
	return b.config
}

// UpdateConfig æ›´æ–°é…ç½®ï¼ˆéƒ¨åˆ†å‚æ•°ï¼‰
func (b *BadgerDB[T]) UpdateConfig(updateFn func(*BadgerDBConfig)) error {
	b.syncLock.Lock()
	defer b.syncLock.Unlock()

	oldConfig := b.config
	updateFn(&b.config)

	if err := b.config.Validate(); err != nil {
		b.config = oldConfig
		return fmt.Errorf("é…ç½®æ›´æ–°å¤±è´¥: %w", err)
	}

	logx.Infof("é…ç½®å·²æ›´æ–°: %+v", b.config)
	return nil
}
