package nosql

import (
	"encoding/json"
	"fmt"
	"os"
	"sync"
	"time"

	"github.com/dgraph-io/badger/v3"
	"github.com/digitalwayhk/core/pkg/persistence/types"
	"github.com/zeromicro/go-zero/core/logx"
)

type opType string

const (
	syncQueuePrefix         = "__sync_queue__"
	maxSyncBatchSize        = 1000
	Insert           opType = "insert"
	Update           opType = "update"
	Delete           opType = "delete"
)

// SyncQueueItem åŒæ­¥é˜Ÿåˆ—é¡¹
type SyncQueueItem struct {
	Key       string    `json:"key"`
	Timestamp time.Time `json:"timestamp"`
	Op        opType    `json:"op"`
}

// BadgerDB æ³›å‹ KV æ•°æ®åº“
type BadgerDB[T types.IModel] struct {
	db             *badger.DB
	path           string
	syncDB         types.IDataAction
	syncLock       sync.RWMutex
	syncMutex      sync.Mutex
	syncInProgress bool // ğŸ”§ åŒæ­¥è¿›è¡Œä¸­æ ‡å¿—
	closeCh        chan struct{}
	wg             sync.WaitGroup
	syncOnce       sync.Once
	bufferPool     sync.Pool // ğŸ†• æ·»åŠ ç¼“å†²æ± 
}

// NewBadgerDB åˆ›å»ºç”Ÿäº§ç¯å¢ƒ BadgerDB
func NewBadgerDB[T types.IModel](path string) (*BadgerDB[T], error) {
	opts := badger.DefaultOptions(path).
		WithSyncWrites(true).
		WithDetectConflicts(true).
		WithNumVersionsToKeep(1).
		WithNumCompactors(4).
		WithCompactL0OnClose(true).
		WithNumLevelZeroTables(4).
		WithNumLevelZeroTablesStall(8).
		WithValueLogFileSize(128 << 20).
		WithMemTableSize(64 << 20).
		WithValueThreshold(1024).
		WithLogger(&badgerLogger{})

	db, err := badger.Open(opts)
	if err != nil {
		return nil, fmt.Errorf("æ‰“å¼€ BadgerDB å¤±è´¥: %w", err)
	}

	b := &BadgerDB[T]{
		db:      db,
		path:    path,
		closeCh: make(chan struct{}),
		bufferPool: sync.Pool{
			New: func() interface{} {
				return make([]byte, 0, 1024) // é¢„åˆ†é… 1KB
			},
		},
	}

	b.wg.Add(1)
	go b.runGC()

	return b, nil
}

// NewBadgerDBFast åˆ›å»ºå¿«é€Ÿæ¨¡å¼ BadgerDBï¼ˆç‰ºç‰²æŒä¹…æ€§æ¢å–æ€§èƒ½ï¼‰
func NewBadgerDBFast[T types.IModel](path string) (*BadgerDB[T], error) {
	opts := badger.DefaultOptions(path).
		WithSyncWrites(false).
		WithDetectConflicts(false).
		WithNumVersionsToKeep(1).
		WithNumCompactors(2).
		WithCompactL0OnClose(true).
		WithNumLevelZeroTables(2).
		WithNumLevelZeroTablesStall(4).
		WithValueLogFileSize(64 << 20).
		WithMemTableSize(8 << 20).
		WithLogger(nil)

	db, err := badger.Open(opts)
	if err != nil {
		return nil, fmt.Errorf("æ‰“å¼€ BadgerDB å¤±è´¥: %w", err)
	}

	b := &BadgerDB[T]{
		db:      db,
		path:    path,
		closeCh: make(chan struct{}),
		bufferPool: sync.Pool{
			New: func() interface{} {
				return make([]byte, 0, 1024) // é¢„åˆ†é… 1KB
			},
		},
	}

	b.wg.Add(2)
	go b.periodicSync()
	go b.runGC()

	return b, nil
}

// SetSyncDB è®¾ç½®åŒæ­¥æ•°æ®åº“
func (b *BadgerDB[T]) SetSyncDB(action types.IDataAction) {
	b.syncLock.Lock()
	defer b.syncLock.Unlock()

	if b.syncDB != nil {
		logx.Errorf("syncDB å·²è®¾ç½®ï¼Œè·³è¿‡")
		return
	}

	b.syncDB = action

	if action != nil {
		// ç¡®ä¿åªå¯åŠ¨ä¸€æ¬¡åŒæ­¥ä»»åŠ¡
		b.syncOnce.Do(func() {
			b.wg.Add(1)
			go b.syncToOtherDB()
		})
	}
}

// periodicSync å®šæœŸåŒæ­¥åˆ°ç£ç›˜
func (b *BadgerDB[T]) periodicSync() {
	defer b.wg.Done()
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			if err := b.db.Sync(); err != nil {
				logx.Errorf("BadgerDB sync å¤±è´¥: %v", err)
			}
		case <-b.closeCh:
			logx.Info("periodicSync é€€å‡º")
			return
		}
	}
}

// runGC åƒåœ¾å›æ”¶
func (b *BadgerDB[T]) runGC() {
	defer b.wg.Done()
	ticker := time.NewTicker(5 * time.Minute)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			var reclaimed int
			for {
				err := b.db.RunValueLogGC(0.5)
				if err != nil {
					break
				}
				reclaimed++
			}
			if reclaimed > 0 {
				logx.Infof("GC å®Œæˆï¼Œå›æ”¶ %d ä¸ªæ–‡ä»¶", reclaimed)
			}
		case <-b.closeCh:
			logx.Info("runGC é€€å‡º")
			return
		}
	}
}

// Close å…³é—­æ•°æ®åº“
func (b *BadgerDB[T]) Close() error {
	close(b.closeCh)
	b.wg.Wait()

	if err := b.db.Sync(); err != nil {
		logx.Errorf("å…³é—­å‰ sync å¤±è´¥: %v", err)
	}

	if err := b.db.Close(); err != nil {
		return fmt.Errorf("å…³é—­ BadgerDB å¤±è´¥: %w", err)
	}

	logx.Info("BadgerDB å·²å…³é—­")
	return nil
}

// badgerLogger æ—¥å¿—é€‚é…å™¨
type badgerLogger struct{}

func (l *badgerLogger) Errorf(f string, v ...interface{})   { logx.Errorf(f, v...) }
func (l *badgerLogger) Warningf(f string, v ...interface{}) { logx.Infof(f, v...) }
func (l *badgerLogger) Infof(f string, v ...interface{})    { logx.Infof(f, v...) }
func (l *badgerLogger) Debugf(f string, v ...interface{})   {}

// generateKey ç”Ÿæˆ key
func (b *BadgerDB[T]) generateKey(item *T) string {
	if item == nil {
		return ""
	}
	if rowCode, ok := any(item).(types.IRowCode); ok {
		return rowCode.GetHash()
	}
	return ""
}

// Set å†™å…¥æ•°æ®
func (b *BadgerDB[T]) Set(item *T, ttl time.Duration) error {
	key := b.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}

	data, err := json.Marshal(item)
	if err != nil {
		return fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
	}

	b.syncLock.RLock()
	needSync := b.syncDB != nil
	b.syncLock.RUnlock()

	op := Insert
	if exists, _ := b.Get(key); exists != nil {
		op = Update
	}
	return b.db.Update(func(txn *badger.Txn) error {
		// å†™å…¥æ•°æ®
		entry := badger.NewEntry([]byte(key), data)
		if ttl > 0 {
			entry = entry.WithTTL(ttl)
		}
		if err := txn.SetEntry(entry); err != nil {
			return err
		}

		// æ·»åŠ åŒæ­¥æ ‡è®°
		if needSync {
			queueItem := SyncQueueItem{
				Key:       key,
				Timestamp: time.Now(),
				Op:        op,
			}
			queueData, err := json.Marshal(queueItem)
			if err != nil {
				return err
			}

			syncKey := fmt.Sprintf("%s%s", syncQueuePrefix, key)
			return txn.Set([]byte(syncKey), queueData)
		}

		return nil
	})
}

func (b *BadgerDB[T]) BatchInsert(items []*T) error {
	if len(items) == 0 {
		return nil
	}

	b.syncLock.RLock()
	needSync := b.syncDB != nil
	b.syncLock.RUnlock()

	// ğŸ”§ ä¼˜åŒ– 1: æ‰¹é‡æ—¶é—´æˆ³å¤ç”¨
	batchTimestamp := time.Now()

	// ğŸ”§ ä¼˜åŒ– 2: é¢„åˆ†é…åºåˆ—åŒ–ç»“æœ
	type serializedItem struct {
		key       string
		value     []byte
		syncKey   string
		syncValue []byte
	}

	serialized := make([]serializedItem, 0, len(items))

	// é˜¶æ®µ 1: é¢„åºåˆ—åŒ–ï¼ˆæ— é”ï¼‰
	for _, item := range items {
		key := b.generateKey(item)
		if key == "" {
			return badger.ErrEmptyKey
		}

		value, err := json.Marshal(item)
		if err != nil {
			return fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
		}

		si := serializedItem{
			key:   key,
			value: value,
		}

		if needSync {
			// ğŸ”§ ä¼˜åŒ– 3: ä½¿ç”¨æ‰¹é‡æ—¶é—´æˆ³
			queueItem := SyncQueueItem{
				Key:       key,
				Timestamp: batchTimestamp,
			}
			queueData, _ := json.Marshal(queueItem)
			si.syncKey = fmt.Sprintf("%s%s", syncQueuePrefix, key)
			si.syncValue = queueData
		}

		serialized = append(serialized, si)
	}

	// é˜¶æ®µ 2: æ‰¹é‡å†™å…¥äº‹åŠ¡ï¼ˆå¿«é€Ÿï¼‰
	const maxRetries = 3
	var lastErr error

	for retry := 0; retry < maxRetries; retry++ {
		txn := b.db.NewTransaction(true)
		success := true

		for _, si := range serialized {
			if err := b.setInTxn(txn, si.key, si.value); err != nil {
				if err == badger.ErrTxnTooBig {
					if commitErr := txn.Commit(); commitErr != nil {
						lastErr = commitErr
						success = false
						break
					}
					txn = b.db.NewTransaction(true)
					if err := b.setInTxn(txn, si.key, si.value); err != nil {
						txn.Discard()
						lastErr = err
						success = false
						break
					}
				} else {
					txn.Discard()
					lastErr = err
					success = false
					break
				}
			}

			if needSync && si.syncKey != "" {
				if err := b.setInTxn(txn, si.syncKey, si.syncValue); err != nil {
					if err == badger.ErrTxnTooBig {
						if commitErr := txn.Commit(); commitErr != nil {
							lastErr = commitErr
							success = false
							break
						}
						txn = b.db.NewTransaction(true)
						if err := b.setInTxn(txn, si.syncKey, si.syncValue); err != nil {
							txn.Discard()
							lastErr = err
							success = false
							break
						}
					} else {
						txn.Discard()
						lastErr = err
						success = false
						break
					}
				}
			}
		}

		if success {
			if err := txn.Commit(); err != nil {
				lastErr = err
				time.Sleep(time.Millisecond * 100 * time.Duration(retry+1))
				continue
			}
			return nil
		}

		txn.Discard()
		time.Sleep(time.Millisecond * 100 * time.Duration(retry+1))
	}

	return lastErr
}

// setInTxn åœ¨äº‹åŠ¡ä¸­è®¾ç½®é”®å€¼
func (b *BadgerDB[T]) setInTxn(txn *badger.Txn, key string, value []byte) error {
	return txn.Set([]byte(key), value)
}

// Delete åˆ é™¤æ•°æ®
func (b *BadgerDB[T]) Delete(key string) error {
	return b.db.Update(func(txn *badger.Txn) error {
		if err := txn.Delete([]byte(key)); err != nil && err != badger.ErrKeyNotFound {
			return err
		}

		syncKey := fmt.Sprintf("%s%s", syncQueuePrefix, key)
		txn.Delete([]byte(syncKey))

		return nil
	})
}

// Get è·å–æ•°æ®
func (b *BadgerDB[T]) Get(key string) (*T, error) {
	var result = new(T)
	if hook, ok := any(result).(types.IModelNewHook); ok {
		hook.NewModel()
	}

	err := b.db.View(func(txn *badger.Txn) error {
		item, err := txn.Get([]byte(key))
		if err != nil {
			return err
		}

		return item.Value(func(val []byte) error {
			return json.Unmarshal(val, result)
		})
	})

	return result, err
}

// Scan æ‰«ææ•°æ®
func (b *BadgerDB[T]) Scan(prefix string, limit int) ([]*T, error) {
	var results []*T

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		count := 0
		for it.Seek([]byte(prefix)); it.ValidForPrefix([]byte(prefix)); it.Next() {
			if count >= limit {
				break
			}

			item := it.Item()
			key := string(item.Key())

			if isInternalKey(key) {
				continue
			}

			err := item.Value(func(val []byte) error {
				var data = new(T)
				if hook, ok := any(data).(types.IModelNewHook); ok {
					hook.NewModel()
				}
				if err := json.Unmarshal(val, data); err != nil {
					return err
				}
				results = append(results, data)
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥ [%s]: %v", key, err)
				continue
			}
			count++
		}
		return nil
	})

	return results, err
}

// isInternalKey åˆ¤æ–­æ˜¯å¦ä¸ºå†…éƒ¨é”®
func isInternalKey(key string) bool {
	return len(key) >= len(syncQueuePrefix) && key[:len(syncQueuePrefix)] == syncQueuePrefix
}

// Sync åŒæ­¥åˆ°ç£ç›˜
func (b *BadgerDB[T]) Sync() error {
	return b.db.Sync()
}

// Backup å¤‡ä»½æ•°æ®åº“
func (b *BadgerDB[T]) Backup(backupPath string) error {
	f, err := os.Create(backupPath)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºå¤‡ä»½æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer f.Close()

	_, err = b.db.Backup(f, 0)
	if err != nil {
		return fmt.Errorf("å¤‡ä»½å¤±è´¥: %w", err)
	}

	logx.Infof("å¤‡ä»½æˆåŠŸ: %s", backupPath)
	return nil
}

// SafeInsert å®‰å…¨æ’å…¥ï¼ˆç«‹å³åŒæ­¥åˆ°ç£ç›˜ï¼‰
func (b *BadgerDB[T]) SafeInsert(data *T) error {
	if err := b.Set(data, 0); err != nil {
		return err
	}

	if err := b.Sync(); err != nil {
		logx.Errorf("åŒæ­¥å¤±è´¥: %v", err)
	}

	return nil
}

// syncToOtherDB åŒæ­¥åˆ°å…¶ä»–æ•°æ®åº“
func (b *BadgerDB[T]) syncToOtherDB() {
	defer b.wg.Done()

	// ğŸ”§ åŠ¨æ€é—´éš”ï¼šåˆå§‹ 1 ç§’ï¼Œæ ¹æ®åŒæ­¥è€—æ—¶è°ƒæ•´
	interval := 1 * time.Second
	minInterval := 1 * time.Second
	maxInterval := 10 * time.Second

	ticker := time.NewTicker(interval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			b.syncLock.RLock()
			hasDB := b.syncDB != nil
			b.syncLock.RUnlock()

			if !hasDB {
				continue
			}

			// ğŸ”§ æ£€æŸ¥æ˜¯å¦æ­£åœ¨åŒæ­¥
			b.syncMutex.Lock()
			if b.syncInProgress {
				b.syncMutex.Unlock()
				logx.Errorf("ä¸Šæ¬¡åŒæ­¥æœªå®Œæˆï¼Œè·³è¿‡æœ¬æ¬¡")

				// ğŸ”§ å»¶é•¿é—´éš”æ—¶é—´
				interval = min(interval*2, maxInterval)
				ticker.Reset(interval)
				continue
			}
			b.syncInProgress = true
			b.syncMutex.Unlock()

			// ğŸ”§ è®°å½•åŒæ­¥å¼€å§‹æ—¶é—´
			start := time.Now()

			// æ‰§è¡ŒåŒæ­¥
			if err := b.processSyncQueue(); err != nil {
				logx.Errorf("åŒæ­¥åˆ°å…¶ä»–DBå¤±è´¥: %v", err)
			}

			// ğŸ”§ è®°å½•åŒæ­¥è€—æ—¶å¹¶è°ƒæ•´é—´éš”
			duration := time.Since(start)

			b.syncMutex.Lock()
			b.syncInProgress = false
			b.syncMutex.Unlock()

			// ğŸ”§ æ ¹æ®è€—æ—¶åŠ¨æ€è°ƒæ•´
			if duration < interval/2 {
				// åŒæ­¥å¾ˆå¿«ï¼Œç¼©çŸ­é—´éš”
				interval = max(interval/2, minInterval)
			} else if duration > interval {
				// åŒæ­¥è¾ƒæ…¢ï¼Œå»¶é•¿é—´éš”
				interval = min(duration*2, maxInterval)
			}

			ticker.Reset(interval)
			logx.Infof("åŒæ­¥å®Œæˆï¼Œè€—æ—¶ %vï¼Œä¸‹æ¬¡é—´éš” %v", duration, interval)

		case <-b.closeCh:
			// ğŸ”§ ç­‰å¾…æœ€åä¸€æ¬¡åŒæ­¥å®Œæˆ
			b.syncMutex.Lock()
			for b.syncInProgress {
				b.syncMutex.Unlock()
				time.Sleep(100 * time.Millisecond)
				b.syncMutex.Lock()
			}
			b.syncMutex.Unlock()

			logx.Info("syncToOtherDB é€€å‡º")
			return
		}
	}
}

// min/max è¾…åŠ©å‡½æ•°ï¼ˆGo 1.21+ï¼‰
func min(a, b time.Duration) time.Duration {
	if a < b {
		return a
	}
	return b
}

func max(a, b time.Duration) time.Duration {
	if a > b {
		return a
	}
	return b
}

// processSyncQueue å¤„ç†åŒæ­¥é˜Ÿåˆ—
func (b *BadgerDB[T]) processSyncQueue() error {
	// è·å–é˜Ÿåˆ—ï¼ˆä¸æŒé”ï¼‰
	syncItems, err := b.getSyncQueueBatch(maxSyncBatchSize)
	if err != nil {
		return fmt.Errorf("è·å–åŒæ­¥é˜Ÿåˆ—å¤±è´¥: %w", err)
	}

	if len(syncItems) == 0 {
		return nil
	}

	logx.Infof("å¼€å§‹åŒæ­¥ %d æ¡æ•°æ®åˆ°å…¶ä»–DB", len(syncItems))

	// åŒæ­¥æ•°æ®ï¼ˆä¸æŒé”ï¼‰
	successKeys, err := b.syncBatch(syncItems)
	if err != nil {
		logx.Errorf("æ‰¹é‡åŒæ­¥å¤±è´¥: %v", err)
	}

	// åˆ é™¤æ ‡è®°
	if len(successKeys) > 0 {
		if err := b.removeSyncMarks(successKeys); err != nil {
			logx.Errorf("åˆ é™¤åŒæ­¥æ ‡è®°å¤±è´¥: %v", err)
		} else {
			logx.Infof("æˆåŠŸåŒæ­¥å¹¶æ¸…ç† %d æ¡æ•°æ®æ ‡è®°", len(successKeys))
		}
	}

	return nil
}

// getSyncQueueBatch åˆ†æ‰¹è·å–åŒæ­¥é˜Ÿåˆ—
func (b *BadgerDB[T]) getSyncQueueBatch(limit int) ([]SyncQueueItem, error) {
	var items []SyncQueueItem

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		prefix := []byte(syncQueuePrefix)
		count := 0

		for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() {
			if count >= limit {
				break
			}

			item := it.Item()

			err := item.Value(func(val []byte) error {
				var queueItem SyncQueueItem
				if err := json.Unmarshal(val, &queueItem); err != nil {
					return err
				}
				items = append(items, queueItem)
				count++
				return nil
			})

			if err != nil {
				logx.Errorf("è§£æåŒæ­¥é˜Ÿåˆ—é¡¹å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})

	return items, err
}

// syncBatch æ‰¹é‡åŒæ­¥æ•°æ®
func (b *BadgerDB[T]) syncBatch(items []SyncQueueItem) ([]string, error) {
	successKeys := make([]string, 0, len(items))

	b.syncLock.RLock()
	b.syncDB.Transaction()
	b.syncLock.RUnlock()

	defer func() {
		if r := recover(); r != nil {
			logx.Errorf("åŒæ­¥ panic: %v", r)
		}
	}()

	for _, queueItem := range items {
		data, err := b.Get(queueItem.Key)
		if err == badger.ErrKeyNotFound {
			// æ•°æ®å·²åˆ é™¤ï¼Œæ ‡è®°ä¸ºæˆåŠŸ
			successKeys = append(successKeys, queueItem.Key)
			continue
		}

		if err != nil {
			logx.Errorf("è¯»å–æ•°æ®å¤±è´¥ [%s]: %v", queueItem.Key, err)
			continue
		}
		if queueItem.Op == Insert {
			if err := b.syncDB.Insert(data); err != nil {
				logx.Errorf("åŒæ­¥æ•°æ®å¤±è´¥ [%s]: %v", queueItem.Key, err)
				continue
			}
		}
		if queueItem.Op == Update {
			if err := b.syncDB.Update(data); err != nil {
				logx.Errorf("åŒæ­¥æ•°æ®å¤±è´¥ [%s]: %v", queueItem.Key, err)
				continue
			}
		}
		successKeys = append(successKeys, queueItem.Key)
	}

	if err := b.syncDB.Commit(); err != nil {
		return nil, fmt.Errorf("æäº¤åŒæ­¥äº‹åŠ¡å¤±è´¥: %w", err)
	}

	return successKeys, nil
}

// removeSyncMarks åˆ é™¤åŒæ­¥æ ‡è®°
func (b *BadgerDB[T]) removeSyncMarks(keys []string) error {
	return b.db.Update(func(txn *badger.Txn) error {
		for _, key := range keys {
			syncKey := fmt.Sprintf("%s%s", syncQueuePrefix, key)
			if err := txn.Delete([]byte(syncKey)); err != nil && err != badger.ErrKeyNotFound {
				logx.Errorf("åˆ é™¤åŒæ­¥æ ‡è®°å¤±è´¥ [%s]: %v", key, err)
			}
		}
		return nil
	})
}

// GetPendingSyncCount è·å–å¾…åŒæ­¥æ•°é‡
func (b *BadgerDB[T]) GetPendingSyncCount() (int, error) {
	count := 0

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = false
		it := txn.NewIterator(opts)
		defer it.Close()

		prefix := []byte(syncQueuePrefix)
		for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() {
			count++
		}
		return nil
	})

	return count, err
}

// ManualSync æ‰‹åŠ¨è§¦å‘åŒæ­¥
func (b *BadgerDB[T]) ManualSync() error {
	b.syncLock.RLock()
	hasDB := b.syncDB != nil
	b.syncLock.RUnlock()

	if !hasDB {
		return fmt.Errorf("syncDB æœªé…ç½®")
	}

	return b.processSyncQueue()
}

// GetAll è·å–æ‰€æœ‰æ•°æ®
func (b *BadgerDB[T]) GetAll() ([]*T, error) {
	var results []*T

	err := b.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			item := it.Item()
			key := string(item.Key())

			if isInternalKey(key) {
				continue
			}

			err := item.Value(func(val []byte) error {
				var data = new(T)
				if hook, ok := any(data).(types.IModelNewHook); ok {
					hook.NewModel()
				}
				if err := json.Unmarshal(val, data); err != nil {
					return err
				}
				results = append(results, data)
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥ [%s]: %v", key, err)
				continue
			}
		}
		return nil
	})

	return results, err
}

// GetStats è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
func (b *BadgerDB[T]) GetStats() string {
	lsm, vlog := b.db.Size()
	return fmt.Sprintf("LSM å¤§å°: %d MB, VLog å¤§å°: %d MB", lsm/(1024*1024), vlog/(1024*1024))
}
