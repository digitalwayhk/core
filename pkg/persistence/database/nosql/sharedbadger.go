package nosql

import (
	"context"
	"fmt"
	"reflect"
	"strings"
	"sync"
	"time"

	"github.com/digitalwayhk/core/pkg/json"

	"github.com/dgraph-io/badger/v3"
	"github.com/digitalwayhk/core/pkg/persistence/entity"
	"github.com/digitalwayhk/core/pkg/persistence/types"
	"github.com/zeromicro/go-zero/core/logx"
)

// ScanResult åˆ†é¡µæ‰«æç»“æœ
type ScanResult[T types.IModel] struct {
	Items   []*T   `json:"items"`    // æ•°æ®åˆ—è¡¨
	LastKey string `json:"last_key"` // æœ€åä¸€ä¸ª keyï¼ˆç”¨äºä¸‹æ¬¡åˆ†é¡µï¼‰
	HasMore bool   `json:"has_more"` // æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
}
type OpType string

const (
	OpInsert OpType = "insert"
	OpUpdate OpType = "update"
	OpDelete OpType = "delete" // ğŸ”§ åˆ é™¤æ“ä½œ
)

// SyncQueueItem åŒæ­¥é˜Ÿåˆ—é¡¹ï¼ˆåŒ…è£…æ•°æ®ï¼‰
type SyncQueueItem[T types.IModel] struct {
	Key       string    `json:"key"`
	Item      *T        `json:"item,omitempty"`
	Op        OpType    `json:"op"`
	CreatedAt time.Time `json:"created_at"`
	UpdatedAt time.Time `json:"updated_at"`
	IsSynced  bool      `json:"is_synced"`
	SyncedAt  time.Time `json:"synced_at,omitempty"`
	IsDeleted bool      `json:"is_deleted"`
	DeletedAt time.Time `json:"deleted_at,omitempty"`
}

// PrefixedBadgerDB å¸¦å‰ç¼€çš„å…±äº« BadgerDB
type PrefixedBadgerDB[T types.IModel] struct {
	manager *SharedBadgerManager
	prefix  string // "user:", "order:", "product:"

	syncDB         bool
	syncList       *entity.ModelList[T]
	syncLock       sync.RWMutex
	syncMutex      sync.Mutex
	syncInProgress bool
	closeCh        chan struct{}
	wg             sync.WaitGroup
	syncOnce       sync.Once
	isAutoClean    bool

	// å¾…åŒæ­¥è®¡æ•°ç¼“å­˜
	pendingCountCache int
	pendingCountMutex sync.RWMutex
	lastCountUpdate   time.Time

	// âœ… å…³é—­çŠ¶æ€æ§åˆ¶
	closeOnce sync.Once
	closed    bool
	closeMu   sync.RWMutex
}

// NewSharedBadgerDB åˆ›å»ºå…±äº« BadgerDB å®ä¾‹
func NewSharedBadgerDB[T types.IModel](basePath string, config ...BadgerDBConfig) (*PrefixedBadgerDB[T], error) {
	prefix := reflect.TypeOf((*T)(nil)).Elem().Name() + ":"
	manager, err := GetSharedManager(basePath, config...)
	if err != nil {
		return nil, err
	}

	manager.AddRef(prefix)

	db := &PrefixedBadgerDB[T]{
		manager: manager,
		prefix:  prefix,
		closeCh: make(chan struct{}),
	}

	logx.Infof("å…±äº«BadgerDBå®ä¾‹å·²åˆ›å»º [prefix=%s]", prefix)
	return db, nil
}

// generateKey ç”Ÿæˆå¸¦å‰ç¼€çš„ key
func (p *PrefixedBadgerDB[T]) generateKey(item *T) string {
	if item == nil {
		return ""
	}
	if rowCode, ok := any(item).(types.IRowCode); ok {
		return p.prefix + rowCode.GetHash()
	}
	return ""
}

// SetSyncDB è®¾ç½®åŒæ­¥æ•°æ®åº“
func (p *PrefixedBadgerDB[T]) SetSyncDB(list *entity.ModelList[T]) {
	p.syncLock.Lock()
	defer p.syncLock.Unlock()

	if list != nil {
		if p.syncDB {
			return
		}
		p.syncDB = true
	} else {
		if !p.syncDB {
			return
		}
		p.syncDB = false
	}

	p.syncList = list

	if list != nil && p.syncDB {
		p.syncOnce.Do(func() {
			p.wg.Add(1)
			go p.syncToOtherDB()
			logx.Infof("å…±äº«DBè‡ªåŠ¨åŒæ­¥å·²å¯åŠ¨ [prefix=%s]", p.prefix)
		})
	}
}

// Set å†™å…¥æ•°æ®
func (p *PrefixedBadgerDB[T]) Set(item *T, ttl time.Duration, fn ...func(wrapper *SyncQueueItem[T])) error {
	if p.IsClosed() {
		return fmt.Errorf("æ•°æ®åº“å®ä¾‹å·²å…³é—­ [prefix=%s]", p.prefix)
	}
	key := p.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}

	p.syncLock.RLock()
	needSync := p.syncDB
	p.syncLock.RUnlock()

	data, err := p.setItem(key, needSync, item, fn...)
	if err != nil {
		return err
	}

	err = p.manager.db.Update(func(txn *badger.Txn) error {
		entry := badger.NewEntry([]byte(key), data)
		if ttl > 0 {
			entry = entry.WithTTL(ttl)
		}
		return txn.SetEntry(entry)
	})

	if err == nil && needSync {
		p.incrementPendingCount(1)
	}
	return err
}
func (p *PrefixedBadgerDB[T]) BatchInsert(items []*T) error {
	if p.IsClosed() {
		return fmt.Errorf("æ•°æ®åº“å®ä¾‹å·²å…³é—­ [prefix=%s]", p.prefix)
	}
	if len(items) == 0 {
		return nil
	}

	p.syncLock.RLock()
	needSync := p.syncDB
	p.syncLock.RUnlock()

	err := p.manager.db.Update(func(txn *badger.Txn) error {
		for _, item := range items {
			key := p.generateKey(item)
			if key == "" {
				return badger.ErrEmptyKey
			}

			data, err := p.setItem(key, needSync, item)
			if err != nil {
				return err
			}

			entry := badger.NewEntry([]byte(key), data)
			if err := txn.SetEntry(entry); err != nil {
				return err
			}
		}
		return nil
	})

	if err == nil && needSync {
		p.incrementPendingCount(len(items))
	}
	return err
}

// setItem å†…éƒ¨æ–¹æ³•ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
func (p *PrefixedBadgerDB[T]) setItem(key string, needSync bool, item *T, fn ...func(wrapper *SyncQueueItem[T])) ([]byte, error) {
	if item == nil {
		return nil, fmt.Errorf("item ä¸èƒ½ä¸ºç©º")
	}

	existingWrapper, err := p.getWrapper(key)
	var wrapper *SyncQueueItem[T]

	if err == nil && existingWrapper != nil {
		if existingWrapper.IsDeleted {
			return nil, fmt.Errorf("æ— æ³•æ›´æ–°å·²åˆ é™¤çš„é¡¹ï¼Œkey=%s", key)
		}
		wrapper = existingWrapper
		wrapper.Op = OpUpdate
		wrapper.Item = item
		wrapper.UpdatedAt = time.Now()
		wrapper.IsSynced = !needSync
		if irde, ok := any(item).(types.IRowDate); ok {
			irde.SetUpdatedAt(wrapper.UpdatedAt)
		}
	} else {
		now := time.Now()
		wrapper = &SyncQueueItem[T]{
			Key:       key,
			Item:      item,
			Op:        OpInsert,
			CreatedAt: now,
			UpdatedAt: now,
			IsSynced:  !needSync,
			IsDeleted: false,
		}
		if irde, ok := any(item).(types.IRowDate); ok {
			irde.SetCreatedAt(now)
			irde.SetUpdatedAt(now)
		}
	}

	data, err := json.Marshal(wrapper)
	if err != nil {
		return nil, fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
	}

	if len(fn) > 0 {
		fn[0](wrapper)
	}
	return data, nil
}

// Get è·å–æ•°æ®
func (p *PrefixedBadgerDB[T]) Get(key string) (*T, error) {
	if p.IsClosed() {
		return nil, fmt.Errorf("æ•°æ®åº“å®ä¾‹å·²å…³é—­ [prefix=%s]", p.prefix)
	}
	fullKey := p.prefix + key
	wrapper, err := p.getWrapper(fullKey)
	if err != nil {
		return nil, err
	}

	if wrapper.IsDeleted {
		return nil, badger.ErrKeyNotFound
	}

	return wrapper.Item, nil
}

// getWrapper å†…éƒ¨æ–¹æ³•
func (p *PrefixedBadgerDB[T]) getWrapper(key string) (*SyncQueueItem[T], error) {
	var wrapper = new(SyncQueueItem[T])

	err := p.manager.db.View(func(txn *badger.Txn) error {
		item, err := txn.Get([]byte(key))
		if err != nil {
			return err
		}

		return item.Value(func(val []byte) error {
			return json.Unmarshal(val, wrapper)
		})
	})

	if err != nil {
		return nil, err
	}

	if wrapper.Item != nil {
		if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
			hook.NewModel()
		}
	}

	return wrapper, nil
}

// Delete åˆ é™¤æ•°æ®
func (p *PrefixedBadgerDB[T]) Delete(key string) error {
	if p.IsClosed() {
		return fmt.Errorf("æ•°æ®åº“å®ä¾‹å·²å…³é—­ [prefix=%s]", p.prefix)
	}
	fullKey := p.prefix + key

	p.syncLock.RLock()
	needSync := p.syncDB
	p.syncLock.RUnlock()

	return p.delete(fullKey, needSync)
}
func (p *PrefixedBadgerDB[T]) DeleteByItem(item *T) error {
	key := p.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}

	p.syncLock.RLock()
	needSync := p.syncDB
	p.syncLock.RUnlock()

	return p.delete(key, needSync)
}
func (p *PrefixedBadgerDB[T]) DeleteByItemWithSync(item *T, needSync bool) error {
	key := p.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}

	return p.delete(key, needSync)
}
func (p *PrefixedBadgerDB[T]) batchDelete(keys []string) error {
	if len(keys) == 0 {
		return nil
	}
	return p.manager.db.Update(func(txn *badger.Txn) error {
		for _, key := range keys {
			if err := txn.Delete([]byte(key)); err != nil {
				return err
			}
		}
		return nil
	})
}

// delete å†…éƒ¨æ–¹æ³•
func (p *PrefixedBadgerDB[T]) delete(key string, needSync bool) error {
	if !needSync {
		return p.manager.db.Update(func(txn *badger.Txn) error {
			return txn.Delete([]byte(key))
		})
	}

	if !p.syncDB {
		return fmt.Errorf("æœªå¯ç”¨åŒæ­¥æ•°æ®åº“åŠŸèƒ½ï¼Œæ— æ³•æ‰§è¡Œè½¯åˆ é™¤")
	}

	return p.manager.db.Update(func(txn *badger.Txn) error {
		item, err := txn.Get([]byte(key))
		if err != nil {
			if err == badger.ErrKeyNotFound {
				return nil
			}
			return err
		}

		var wrapper SyncQueueItem[T]
		err = item.Value(func(val []byte) error {
			return json.Unmarshal(val, &wrapper)
		})
		if err != nil {
			return err
		}

		if wrapper.IsDeleted {
			return nil
		}

		now := time.Now()
		wrapper.Op = OpDelete
		wrapper.IsDeleted = true
		wrapper.DeletedAt = now
		wrapper.UpdatedAt = now
		wrapper.IsSynced = false

		data, err := json.Marshal(&wrapper)
		if err != nil {
			return fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
		}

		return txn.Set([]byte(key), data)
	})
}

// Scan æ‰«ææ•°æ®ï¼ˆä»…æ‰«æå½“å‰å‰ç¼€ï¼‰
func (p *PrefixedBadgerDB[T]) Scan(prefix string, limit int) ([]*T, error) {
	if p.IsClosed() {
		return nil, fmt.Errorf("æ•°æ®åº“å®ä¾‹å·²å…³é—­ [prefix=%s]", p.prefix)
	}
	var results []*T
	prefix = p.prefix + prefix
	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 1000 // å¢åŠ é¢„å–å¤§å°
		opts.PrefetchValues = true
		opts.Reverse = false
		opts.AllVersions = false // åªè¯»å–æœ€æ–°ç‰ˆæœ¬
		it := txn.NewIterator(opts)
		defer it.Close()

		count := 0
		for it.Seek([]byte(prefix)); it.ValidForPrefix([]byte(prefix)); it.Next() {
			if limit > 0 && count >= limit {
				break
			}

			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				if wrapper.IsDeleted {
					return nil
				}

				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					results = append(results, wrapper.Item)
					count++
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})

	return results, err
}
func (p *PrefixedBadgerDB[T]) ScanAll() ([]*T, error) {
	var results []*T
	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()
		for it.Seek([]byte(p.prefix)); it.ValidForPrefix([]byte(p.prefix)); it.Next() {
			item := it.Item()
			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}
				if wrapper.IsDeleted {
					return nil
				}
				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					results = append(results, wrapper.Item)
				}
				return nil
			})
			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})
	return results, err
}
func (p *PrefixedBadgerDB[T]) ScanPage(prefix string, limit int, lastKey string) (*ScanResult[T], error) {
	prefix = p.prefix + prefix
	if limit <= 0 {
		limit = 1000 // é»˜è®¤æ¯é¡µ 1000 æ¡
	}

	result := &ScanResult[T]{
		Items: make([]*T, 0, limit),
	}

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		// ç¡®å®šèµ·å§‹ä½ç½®
		var startKey []byte
		if lastKey != "" {
			startKey = []byte(lastKey)
		} else {
			startKey = []byte(prefix)
		}

		count := 0
		firstItem := true

		for it.Seek(startKey); it.ValidForPrefix([]byte(prefix)); it.Next() {
			// è·³è¿‡ä¸Šä¸€é¡µçš„æœ€åä¸€æ¡ï¼ˆé¿å…é‡å¤ï¼‰
			if lastKey != "" && firstItem {
				currentKey := string(it.Item().Key())
				if currentKey == lastKey {
					firstItem = false
					continue
				}
			}
			firstItem = false

			// è¾¾åˆ°é™åˆ¶åå†è¯»ä¸€æ¡ï¼Œåˆ¤æ–­æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
			if count >= limit {
				result.HasMore = true
				break
			}

			item := it.Item()
			currentKey := string(item.Key())

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// è¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®
				if wrapper.IsDeleted {
					return nil
				}

				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					result.Items = append(result.Items, wrapper.Item)
					result.LastKey = currentKey
					count++
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}

		return nil
	})

	return result, err
}

// incrementPendingCount æ›´æ–°å¾…åŒæ­¥è®¡æ•°
func (p *PrefixedBadgerDB[T]) incrementPendingCount(delta int) {
	p.pendingCountMutex.Lock()
	p.pendingCountCache += delta
	p.pendingCountMutex.Unlock()
}

// getDataAction è·å–åŒæ­¥æ“ä½œ
func (p *PrefixedBadgerDB[T]) getDataAction(item *T) types.IDataAction {
	if p.syncList != nil {
		model := item
		if model == nil {
			model = new(T)
			if nm, ok := any(model).(types.IModelNewHook); ok {
				nm.NewModel()
			}
		}
		searchItem := p.syncList.GetSearchItem()
		searchItem.Model = model
		action := p.syncList.GetDBAdapter(searchItem)
		return action
	}
	return nil
}

// syncToOtherDB åŒæ­¥åˆ°å…¶ä»–æ•°æ®åº“ï¼ˆä¿®å¤æ­»å¾ªç¯é—®é¢˜ + ç«‹å³åŒæ­¥ï¼‰
func (p *PrefixedBadgerDB[T]) syncToOtherDB() {
	defer p.wg.Done()

	config := p.manager.config
	interval := config.SyncInterval
	minInterval := config.SyncMinInterval
	maxInterval := config.SyncMaxInterval

	// ğŸ†• å¯åŠ¨åç«‹å³æ£€æŸ¥ä¸€æ¬¡
	p.syncLock.RLock()
	hasDB := p.syncDB
	p.syncLock.RUnlock()

	if hasDB {
		pendingCount, err := p.GetPendingSyncCount()
		if err == nil && pendingCount > 0 {
			logx.Infof("å¯åŠ¨æ—¶å‘ç°å¾…åŒæ­¥æ•°æ® [prefix=%s, count=%d], ç«‹å³æ‰§è¡ŒåŒæ­¥", p.prefix, pendingCount)

			p.syncMutex.Lock()
			p.syncInProgress = true
			p.syncMutex.Unlock()

			start := time.Now()
			if err := p.processSyncQueue(); err != nil {
				logx.Errorf("åˆå§‹åŒæ­¥å¤±è´¥ [prefix=%s]: %v", p.prefix, err)
			}
			duration := time.Since(start)

			remainingCount, _ := p.GetPendingSyncCount()

			p.syncMutex.Lock()
			p.syncInProgress = false
			p.syncMutex.Unlock()

			logx.Infof("åˆå§‹åŒæ­¥å®Œæˆ [prefix=%s, å¤„ç†: %d, å‰©ä½™: %d, è€—æ—¶: %v]",
				p.prefix, pendingCount, remainingCount, duration)

			// ğŸ†• å¦‚æœè¿˜æœ‰å‰©ä½™æ•°æ®,ç¼©çŸ­é—´éš”
			if remainingCount > 0 {
				interval = minInterval
			}
		}
	}

	ticker := time.NewTicker(interval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			p.syncLock.RLock()
			hasDB := p.syncDB
			p.syncLock.RUnlock()

			if !hasDB {
				continue
			}

			pendingCount, err := p.GetPendingSyncCount()
			if err != nil {
				logx.Errorf("è·å–å¾…åŒæ­¥æ•°é‡å¤±è´¥ [prefix=%s]: %v", p.prefix, err)
				interval = min(interval*2, maxInterval)
				ticker.Reset(interval)
				continue
			}

			if pendingCount == 0 {
				interval = min(interval*2, maxInterval)
				ticker.Reset(interval)
				continue
			}

			p.syncMutex.Lock()
			if p.syncInProgress {
				p.syncMutex.Unlock()
				logx.Infof("ä¸Šæ¬¡åŒæ­¥æœªå®Œæˆï¼Œè·³è¿‡æœ¬æ¬¡ [prefix=%s]", p.prefix)
				interval = min(interval*2, maxInterval)
				ticker.Reset(interval)
				continue
			}
			p.syncInProgress = true
			p.syncMutex.Unlock()

			start := time.Now()

			if err := p.processSyncQueue(); err != nil {
				logx.Errorf("åŒæ­¥å¤±è´¥ [prefix=%s]: %v", p.prefix, err)
			}

			duration := time.Since(start)

			remainingCount, _ := p.GetPendingSyncCount()

			p.syncMutex.Lock()
			p.syncInProgress = false
			p.syncMutex.Unlock()

			// ğŸ”§ åŠ¨æ€è°ƒæ•´é—´éš”
			if remainingCount > 0 {
				interval = minInterval
			} else if duration < interval/2 {
				interval = max(interval/2, minInterval)
			} else if duration > interval {
				interval = min(duration*2, maxInterval)
			}

			ticker.Reset(interval)
			logx.Infof("åŒæ­¥å®Œæˆ [prefix=%s, å¤„ç†: %d, å‰©ä½™: %d, è€—æ—¶: %v, ä¸‹æ¬¡é—´éš”: %v]",
				p.prefix, pendingCount, remainingCount, duration, interval)

		case <-p.closeCh:
			// âœ… ä¼˜åŒ–ï¼šæ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…æ— é™ç­‰å¾…
			logx.Infof("æ”¶åˆ°å…³é—­ä¿¡å· [prefix=%s]", p.prefix)

			p.syncMutex.Lock()
			if p.syncInProgress {
				p.syncMutex.Unlock()

				// ç­‰å¾…åŒæ­¥å®Œæˆï¼Œæœ€å¤šç­‰å¾… 10 ç§’
				logx.Infof("ç­‰å¾…å½“å‰åŒæ­¥æ“ä½œå®Œæˆ [prefix=%s]", p.prefix)
				timeout := time.After(10 * time.Second)
				ticker := time.NewTicker(100 * time.Millisecond)
				defer ticker.Stop()

				for {
					select {
					case <-timeout:
						logx.Errorf("ç­‰å¾…åŒæ­¥å®Œæˆè¶…æ—¶ï¼ˆ10ç§’ï¼‰ï¼Œå¼ºåˆ¶é€€å‡º [prefix=%s]", p.prefix)
						return
					case <-ticker.C:
						p.syncMutex.Lock()
						if !p.syncInProgress {
							p.syncMutex.Unlock()
							logx.Infof("åŒæ­¥æ“ä½œå·²å®Œæˆ [prefix=%s]", p.prefix)
							logx.Infof("syncToOtherDB é€€å‡º [prefix=%s]", p.prefix)
							return
						}
						p.syncMutex.Unlock()
					}
				}
			} else {
				p.syncMutex.Unlock()
			}

			logx.Infof("syncToOtherDB é€€å‡º [prefix=%s]", p.prefix)
			return
		}
	}
}

// ğŸ”§ ä¿®å¤ processSyncQueue: è¿”å›åŒæ­¥æˆåŠŸçš„æ•°é‡
func (p *PrefixedBadgerDB[T]) processSyncQueue() error {
	unsyncedItems, err := p.getUnsyncedBatch(p.manager.config.SyncBatchSize)
	if err != nil {
		return fmt.Errorf("è·å–æœªåŒæ­¥æ•°æ®å¤±è´¥: %w", err)
	}

	if len(unsyncedItems) == 0 {
		return nil
	}

	logx.Infof("å¼€å§‹åŒæ­¥ [prefix=%s, æ•°é‡: %d]", p.prefix, len(unsyncedItems))

	_, err = p.syncBatch(unsyncedItems)
	if err != nil {
		logx.Errorf("æ‰¹é‡åŒæ­¥å¤±è´¥ [prefix=%s]: %v", p.prefix, err)
		return err
	}
	successCount := 0
	for _, item := range unsyncedItems {
		if item.IsSynced {
			successCount++
		}
	}
	logx.Infof("åŒæ­¥æˆåŠŸ [prefix=%s, æˆåŠŸ: %d/%d]", p.prefix, successCount, len(unsyncedItems))
	return nil
}

// ğŸ”§ ä¿®å¤ GetPendingSyncCount: æ·»åŠ è¶…æ—¶ä¿æŠ¤
func (p *PrefixedBadgerDB[T]) GetPendingSyncCount() (int, error) {
	count := 0

	// ğŸ†• æ·»åŠ è¶…æ—¶ä¿æŠ¤
	done := make(chan error, 1)

	go func() {
		err := p.manager.db.View(func(txn *badger.Txn) error {
			opts := badger.DefaultIteratorOptions
			opts.PrefetchValues = true
			opts.PrefetchSize = 100 // ğŸ”§ é™åˆ¶é¢„å–å¤§å°
			it := txn.NewIterator(opts)
			defer it.Close()

			for it.Seek([]byte(p.prefix)); it.ValidForPrefix([]byte(p.prefix)); it.Next() {
				item := it.Item()

				err := item.Value(func(val []byte) error {
					var wrapper SyncQueueItem[T]
					if err := json.Unmarshal(val, &wrapper); err != nil {
						return err
					}

					if !wrapper.IsSynced {
						count++
					}
					return nil
				})

				if err != nil {
					continue
				}
			}
			return nil
		})
		done <- err
	}()

	// ğŸ†• ç­‰å¾…å®Œæˆæˆ–è¶…æ—¶
	select {
	case err := <-done:
		return count, err
	case <-time.After(5 * time.Second):
		return 0, fmt.Errorf("è·å–å¾…åŒæ­¥æ•°é‡è¶…æ—¶ [prefix=%s]", p.prefix)
	}
}

// getUnsyncedBatch è·å–æœªåŒæ­¥æ•°æ®ï¼ˆé™å®šå‰ç¼€ï¼‰
func (p *PrefixedBadgerDB[T]) getUnsyncedBatch(limit int) ([]*SyncQueueItem[T], error) {
	var items []*SyncQueueItem[T]

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		count := 0
		for it.Seek([]byte(p.prefix)); it.ValidForPrefix([]byte(p.prefix)); it.Next() {
			if count >= limit {
				break
			}

			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				if !wrapper.IsSynced {
					if wrapper.Item != nil {
						if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
							hook.NewModel()
						}
					}
					items = append(items, &wrapper)
					count++
				}
				return nil
			})

			if err != nil {
				continue
			}
		}
		return nil
	})

	return items, err
}

// ğŸ”§ å®Œæ•´çš„æ‰¹é‡åŒæ­¥å®ç°ï¼ˆåŒ…å«é”™è¯¯å¤„ç†ï¼‰
func (p *PrefixedBadgerDB[T]) syncBatch(items []*SyncQueueItem[T]) ([]string, error) {
	if len(items) == 0 {
		return nil, nil
	}

	p.syncLock.RLock()
	if !p.syncDB {
		p.syncLock.RUnlock()
		return nil, fmt.Errorf("æœªå¼€å¯ syncDB")
	}
	p.syncLock.RUnlock()

	// æŒ‰æ“ä½œç±»å‹åˆ†ç»„
	var (
		insertItems []*SyncQueueItem[T]
		updateItems []*SyncQueueItem[T]
		deleteItems []*SyncQueueItem[T]
	)

	for _, wrapper := range items {
		setHashCode(wrapper.Item)

		switch wrapper.Op {
		case OpInsert:
			insertItems = append(insertItems, wrapper)
		case OpUpdate:
			updateItems = append(updateItems, wrapper)
		case OpDelete:
			deleteItems = append(deleteItems, wrapper)
		}
	}

	successKeys := make([]string, 0, len(items))

	// æ‰¹é‡æ’å…¥ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
	if len(insertItems) > 0 {
		keys := p.batchInsertWithErrorHandling(insertItems)
		successKeys = append(successKeys, keys...)
		p.onSyncAfter(insertItems)
	}

	// æ‰¹é‡æ›´æ–°ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
	if len(updateItems) > 0 {
		keys := p.batchUpdateWithErrorHandling(updateItems)
		successKeys = append(successKeys, keys...)
		p.onSyncAfter(updateItems)
	}

	// æ‰¹é‡åˆ é™¤ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
	if len(deleteItems) > 0 {
		keys := p.batchDeleteWithErrorHandling(deleteItems)
		successKeys = append(successKeys, keys...)
		p.onSyncAfter(deleteItems)
	}

	// æ‰¹é‡æ›´æ–°åŒæ­¥çŠ¶æ€
	if len(successKeys) > 0 {
		p.batchUpdateSyncedStatus(successKeys)
	}
	return successKeys, nil
}
func (p *PrefixedBadgerDB[T]) onSyncAfter(items []*SyncQueueItem[T]) {
	if len(items) == 0 {
		return
	}
	if iosa, ok := any(items[0].Item).(IOnSyncAfter[T]); ok {
		//å¦‚æœå¼‚å¸¸ä¸å½±å“ä¸»æµç¨‹ï¼Œå¯ä»¥å¿½ç•¥é”™è¯¯
		// defer func() {
		// 	if err := recover(); err != nil {
		// 		logx.Errorf("æ‰§è¡Œ OnSyncAfter å‘ç”Ÿææ…Œ: %v", err)
		// 	}
		// }()
		err := iosa.OnSyncAfter(items)
		if err != nil {
			logx.Errorf("æ‰§è¡Œ OnSyncAfter å¤±è´¥: %v", err)
		}
	}
}
func setHashCode(item any) {
	if item == nil {
		return
	}
	if rowCode, ok := item.(types.IRowCode); ok {
		hash := rowCode.GetHash()
		if hash == "" {
			logx.Errorf("IRowCode GetHash è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œå¯èƒ½å¯¼è‡´ key ç”Ÿæˆå¤±è´¥")
		}
		rowCode.SetHashcode(hash)
	}
}

// ğŸ†• æ‰¹é‡æ’å…¥ï¼ˆä½¿ç”¨äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) batchInsertWithErrorHandling(items []*SyncQueueItem[T]) []string {
	if len(items) == 0 {
		return nil
	}

	syncAction := p.getDataAction(items[0].Item)
	if syncAction == nil {
		logx.Errorf("æœªæ‰¾åˆ°åŒæ­¥æ“ä½œå¯¹è±¡")
		return nil
	}

	successKeys := make([]string, 0, len(items))
	physicalDeleteKeys := make([]string, 0, len(items))

	// ğŸ”§ å¼€å¯äº‹åŠ¡ï¼ˆæ‰¹é‡æ“ä½œï¼‰
	if err := syncAction.Transaction(); err != nil {
		logx.Errorf("å¼€å¯äº‹åŠ¡å¤±è´¥: %vï¼Œé™çº§ä¸ºé€æ¡æ’å…¥", err)
		return p.insertItemsOneByOne(items)
	}

	// åœ¨äº‹åŠ¡ä¸­é€æ¡æ’å…¥
	hasError := false
	for _, wrapper := range items {
		if wrapper.Item == nil {
			continue
		}

		err := syncAction.Insert(wrapper.Item)
		if err != nil {
			// ğŸ”§ å¤„ç†ä¸»é”®å†²çª - å°è¯•æ›´æ–°
			if strings.Contains(err.Error(), "duplicate key") ||
				strings.Contains(err.Error(), "UNIQUE constraint failed") {
				logx.Infof("æ’å…¥å†²çªï¼Œå°è¯•æ›´æ–° [%s]", wrapper.Key)

				err = syncAction.Update(wrapper.Item)
				if err == nil {
					// ğŸ”§ ä¿®å¤ï¼šæ›´æ–°æˆåŠŸåä¹Ÿè¦æ£€æŸ¥æ˜¯å¦ç‰©ç†åˆ é™¤
					if syncAfterDelete, ok := any(wrapper.Item).(ISyncAfterDelete[T]); ok {
						if needDelete := syncAfterDelete.IsSyncAfterDelete(); needDelete {
							physicalDeleteKeys = append(physicalDeleteKeys, wrapper.Key)
							continue // ä¸åŠ å…¥ successKeys
						}
					}
					successKeys = append(successKeys, wrapper.Key)
					continue
				}

				logx.Errorf("æ›´æ–°å¤±è´¥ [%s]: %v", wrapper.Key, err)
				hasError = true
				continue
			}

			// å…¶ä»–é”™è¯¯
			logx.Errorf("æ’å…¥å¤±è´¥ [%s]: %v", wrapper.Key, err)
			hasError = true
			continue
		}
		wrapper.IsSynced = true
		// ğŸ”§ æ’å…¥æˆåŠŸï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ç‰©ç†åˆ é™¤
		if syncAfterDelete, ok := any(wrapper.Item).(ISyncAfterDelete[T]); ok {
			if needDelete := syncAfterDelete.IsSyncAfterDelete(); needDelete {
				logx.Infof("ISyncAfterDelete è¿”å› trueï¼Œå°†ç‰©ç†åˆ é™¤ [%s]", wrapper.Key)
				physicalDeleteKeys = append(physicalDeleteKeys, wrapper.Key)
				continue // ğŸ”§ ä¸åŠ å…¥ successKeys
			}
		}
		// æ’å…¥æˆåŠŸä¸”ä¸éœ€è¦ç‰©ç†åˆ é™¤
		successKeys = append(successKeys, wrapper.Key)

	}

	// ğŸ”§ æäº¤äº‹åŠ¡
	if err := syncAction.Commit(); err != nil {
		logx.Errorf("æäº¤äº‹åŠ¡å¤±è´¥: %vï¼Œå›æ»šå¹¶é™çº§ä¸ºé€æ¡å¤„ç†", err)

		if rollbackErr := syncAction.Rollback(); rollbackErr != nil {
			logx.Errorf("å›æ»šå¤±è´¥: %v", rollbackErr)
		}
		successKeys = p.insertItemsOneByOne(items)
	}

	// ğŸ†• æ‰¹é‡ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜
	if len(physicalDeleteKeys) > 0 {
		if err := p.batchDelete(physicalDeleteKeys); err != nil {
			logx.Errorf("æ‰¹é‡ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥: %v", err)
		}
	}

	if hasError {
		logx.Errorf("æ‰¹é‡æ’å…¥éƒ¨åˆ†å¤±è´¥ï¼ŒæˆåŠŸ: %d, ç‰©ç†åˆ é™¤: %d, æ€»æ•°: %d",
			len(successKeys), len(physicalDeleteKeys), len(items))
	}

	// ğŸ”§ å‡å°‘å¾…åŒæ­¥è®¡æ•°
	count := len(successKeys) + len(physicalDeleteKeys)
	p.incrementPendingCount(-count)

	return successKeys // ğŸ”§ åªè¿”å›éœ€è¦æ›´æ–°åŒæ­¥çŠ¶æ€çš„ keys
}

// ğŸ†• å¸¦è¶…æ—¶çš„æ‰¹é‡æ›´æ–°
func (p *PrefixedBadgerDB[T]) batchUpdateWithErrorHandling(items []*SyncQueueItem[T]) []string {
	if len(items) == 0 {
		return nil
	}

	syncAction := p.getDataAction(items[0].Item)
	if syncAction == nil {
		logx.Errorf("æœªæ‰¾åˆ°åŒæ­¥æ“ä½œå¯¹è±¡")
		return nil
	}

	successKeys := make([]string, 0, len(items))
	physicalDeleteKeys := make([]string, 0, len(items))

	// ğŸ†• æ·»åŠ è¶…æ—¶ä¿æŠ¤
	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	// ğŸ”§ å¼€å¯äº‹åŠ¡
	if err := syncAction.Transaction(); err != nil {
		logx.Errorf("å¼€å¯äº‹åŠ¡å¤±è´¥: %vï¼Œé™çº§ä¸ºé€æ¡æ›´æ–°", err)
		return p.updateItemsOneByOne(items)
	}

	// ğŸ†• ç¡®ä¿äº‹åŠ¡ä¸€å®šè¢«æäº¤æˆ–å›æ»š
	committed := false
	defer func() {
		if !committed {
			logx.Errorf("æ£€æµ‹åˆ°æœªæäº¤çš„äº‹åŠ¡ï¼Œè‡ªåŠ¨å›æ»š")
			syncAction.Rollback()
		}
	}()

	// ğŸ†• ä½¿ç”¨é€šé“æ¥æ”¶ç»“æœ
	done := make(chan struct{})
	hasError := false

	go func() {
		// åœ¨äº‹åŠ¡ä¸­é€æ¡æ›´æ–°
		for _, wrapper := range items {
			select {
			case <-ctx.Done():
				logx.Errorf("æ›´æ–°è¶…æ—¶ï¼Œåœæ­¢æ‰¹é‡æ›´æ–°")
				hasError = true
				return
			default:
			}

			if wrapper.Item == nil {
				continue
			}

			err := syncAction.Update(wrapper.Item)

			if err != nil {
				// ğŸ”§ æ£€æŸ¥äº‹åŠ¡é”™è¯¯
				if strings.Contains(err.Error(), "transaction has already been committed") ||
					strings.Contains(err.Error(), "transaction has already been rolled back") {
					logx.Errorf("äº‹åŠ¡å·²å¤±æ•ˆï¼Œåœæ­¢æ‰¹é‡æ›´æ–° [%s]", wrapper.Key)
					hasError = true
					return
				}

				// ğŸ”§ æ£€æŸ¥è¿æ¥é”™è¯¯
				if strings.Contains(err.Error(), "Rows are closed") ||
					strings.Contains(err.Error(), "context canceled") {
					logx.Errorf("è¿æ¥å·²å…³é—­ï¼Œåœæ­¢æ‰¹é‡æ›´æ–° [%s]", wrapper.Key)
					hasError = true
					return
				}

				// ğŸ”§ å¤„ç†è®°å½•ä¸å­˜åœ¨
				if strings.Contains(err.Error(), "record not found") ||
					strings.Contains(err.Error(), "no rows") {
					logx.Infof("è®°å½•ä¸å­˜åœ¨ï¼Œå°è¯•æ’å…¥ [%s]", wrapper.Key)

					err = syncAction.Insert(wrapper.Item)
					if err == nil {
						if syncAfterDelete, ok := any(wrapper.Item).(ISyncAfterDelete[T]); ok {
							if needDelete := syncAfterDelete.IsSyncAfterDelete(); needDelete {
								physicalDeleteKeys = append(physicalDeleteKeys, wrapper.Key)
								continue
							}
						}
						successKeys = append(successKeys, wrapper.Key)
						continue
					}

					// æ’å…¥ä¹Ÿå¤±è´¥
					if strings.Contains(err.Error(), "duplicate key") ||
						strings.Contains(err.Error(), "UNIQUE constraint failed") {
						logx.Errorf("æ’å…¥å†²çªï¼Œé‡è¯•æ›´æ–° [%s]", wrapper.Key)
						err = syncAction.Update(wrapper.Item)
						if err == nil {
							if syncAfterDelete, ok := any(wrapper.Item).(ISyncAfterDelete[T]); ok {
								if needDelete := syncAfterDelete.IsSyncAfterDelete(); needDelete {
									physicalDeleteKeys = append(physicalDeleteKeys, wrapper.Key)
									continue
								}
							}
							successKeys = append(successKeys, wrapper.Key)
							continue
						}
					}

					logx.Errorf("æ’å…¥å¤±è´¥ [%s]: %v", wrapper.Key, err)
					hasError = true
					continue
				}

				logx.Errorf("æ›´æ–°å¤±è´¥ [%s]: %v", wrapper.Key, err)
				hasError = true
				continue
			}

			wrapper.IsSynced = true

			// æ›´æ–°æˆåŠŸï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ç‰©ç†åˆ é™¤
			if syncAfterDelete, ok := any(wrapper.Item).(ISyncAfterDelete[T]); ok {
				if needDelete := syncAfterDelete.IsSyncAfterDelete(); needDelete {
					logx.Infof("ISyncAfterDelete è¿”å› trueï¼Œå°†ç‰©ç†åˆ é™¤ [%s]", wrapper.Key)
					physicalDeleteKeys = append(physicalDeleteKeys, wrapper.Key)
					continue
				}
			}

			successKeys = append(successKeys, wrapper.Key)
		}
		close(done)
	}()

	// ğŸ†• ç­‰å¾…å®Œæˆæˆ–è¶…æ—¶
	select {
	case <-done:
		// æ­£å¸¸å®Œæˆ
	case <-ctx.Done():
		logx.Errorf("æ‰¹é‡æ›´æ–°è¶…æ—¶ [æ•°é‡: %d]", len(items))
		hasError = true
	}

	// ğŸ”§ æäº¤æˆ–å›æ»šäº‹åŠ¡
	if hasError {
		logx.Errorf("æ‰¹é‡æ›´æ–°é‡åˆ°é”™è¯¯ï¼Œå›æ»šäº‹åŠ¡")
		if rollbackErr := syncAction.Rollback(); rollbackErr != nil {
			logx.Errorf("å›æ»šå¤±è´¥: %v", rollbackErr)
		}
		committed = true
		return p.updateItemsOneByOne(items)
	}

	if err := syncAction.Commit(); err != nil {
		logx.Errorf("æäº¤äº‹åŠ¡å¤±è´¥: %v", err)
		committed = true

		if rollbackErr := syncAction.Rollback(); rollbackErr != nil {
			logx.Errorf("å›æ»šå¤±è´¥: %v", rollbackErr)
		}

		return p.updateItemsOneByOne(items)
	}

	committed = true

	// ğŸ†• æ‰¹é‡ç‰©ç†åˆ é™¤
	if len(physicalDeleteKeys) > 0 {
		if err := p.batchDelete(physicalDeleteKeys); err != nil {
			logx.Errorf("æ‰¹é‡ç‰©ç†åˆ é™¤å¤±è´¥: %v", err)
		}
	}

	if hasError {
		logx.Errorf("æ‰¹é‡æ›´æ–°éƒ¨åˆ†å¤±è´¥ï¼ŒæˆåŠŸ: %d, ç‰©ç†åˆ é™¤: %d, æ€»æ•°: %d",
			len(successKeys), len(physicalDeleteKeys), len(items))
	}

	count := len(successKeys) + len(physicalDeleteKeys)
	p.incrementPendingCount(-count)

	return successKeys
}

// ğŸ†• æ‰¹é‡åˆ é™¤ï¼ˆä½¿ç”¨äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) batchDeleteWithErrorHandling(items []*SyncQueueItem[T]) []string {
	if len(items) == 0 {
		return nil
	}

	syncAction := p.getDataAction(items[0].Item)
	if syncAction == nil {
		logx.Errorf("æœªæ‰¾åˆ°åŒæ­¥æ“ä½œå¯¹è±¡")
		return nil
	}

	successKeys := make([]string, 0, len(items))

	// ğŸ”§ å¼€å¯äº‹åŠ¡ï¼ˆæ‰¹é‡æ“ä½œï¼‰
	if err := syncAction.Transaction(); err != nil {
		logx.Errorf("å¼€å¯äº‹åŠ¡å¤±è´¥: %vï¼Œé™çº§ä¸ºé€æ¡åˆ é™¤", err)
		newSyncAction := p.getDataAction(items[0].Item)
		return p.deleteItemsOneByOne(items, newSyncAction)
	}

	// åœ¨äº‹åŠ¡ä¸­é€æ¡åˆ é™¤
	hasError := false
	for _, wrapper := range items {
		if wrapper.Item == nil {
			continue
		}

		err := syncAction.Delete(wrapper.Item)
		if err == nil {
			// åˆ é™¤æˆåŠŸ
			successKeys = append(successKeys, wrapper.Key)
			wrapper.IsSynced = true
		}
		if err != nil {
			// ğŸ”§ å¤„ç†è®°å½•ä¸å­˜åœ¨ - è§†ä¸ºæˆåŠŸ
			if strings.Contains(err.Error(), "record not found") ||
				strings.Contains(err.Error(), "no rows") {
				logx.Infof("åˆ é™¤ç›®æ ‡ä¸å­˜åœ¨ï¼Œè·³è¿‡ [%s]", wrapper.Key)
				successKeys = append(successKeys, wrapper.Key)
				continue
			}
			logx.Errorf("åˆ é™¤å¤±è´¥ [%s]: %v", wrapper.Key, err)
			hasError = true
			continue
		}

	}

	// ğŸ”§ æäº¤äº‹åŠ¡
	if err := syncAction.Commit(); err != nil {
		logx.Errorf("æäº¤äº‹åŠ¡å¤±è´¥: %vï¼Œå›æ»šå¹¶é™çº§ä¸ºé€æ¡å¤„ç†", err)

		if rollbackErr := syncAction.Rollback(); rollbackErr != nil {
			logx.Errorf("å›æ»šå¤±è´¥: %v", rollbackErr)
		}

		// ğŸ†• é‡æ–°è·å–æ–°çš„ syncAction
		newSyncAction := p.getDataAction(items[0].Item)
		if newSyncAction == nil {
			logx.Errorf("é‡æ–°è·å– syncAction å¤±è´¥")
			return nil
		}

		successKeys = p.deleteItemsOneByOne(items, newSyncAction)
	}

	// ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜
	for _, key := range successKeys {
		if err := p.delete(key, false); err != nil {
			logx.Errorf("ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥ [%s]: %v", key, err)
		}
	}

	if hasError {
		logx.Errorf("æ‰¹é‡åˆ é™¤éƒ¨åˆ†å¤±è´¥ï¼ŒæˆåŠŸ: %d/%d", len(successKeys), len(items))
	}
	count := len(successKeys)
	p.incrementPendingCount(-count)
	return successKeys
}

// ğŸ†• é€æ¡æ’å…¥ï¼ˆæ— äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) insertItemsOneByOne(items []*SyncQueueItem[T]) []string {
	successKeys := make([]string, 0, len(items))

	for _, wrapper := range items {
		if wrapper.Item == nil {
			continue
		}
		wrapper.IsSynced = false
		syncAction := p.getDataAction(wrapper.Item)
		err := syncAction.Insert(wrapper.Item)
		if err != nil {
			// ğŸ”§ å¤„ç†ä¸»é”®å†²çª - å°è¯•æ›´æ–°
			if strings.Contains(err.Error(), "duplicate key") ||
				strings.Contains(err.Error(), "UNIQUE constraint failed") {
				logx.Infof("æ’å…¥å†²çªï¼Œå°è¯•æ›´æ–° [%s]", wrapper.Key)

				err = syncAction.Update(wrapper.Item)
				if err == nil {
					successKeys = append(successKeys, wrapper.Key)
					continue
				}

				logx.Errorf("æ›´æ–°å¤±è´¥ [%s]: %v", wrapper.Key, err)
				continue
			}

			logx.Errorf("æ’å…¥å¤±è´¥ [%s]: %v", wrapper.Key, err)
			continue
		}
		wrapper.IsSynced = true
		if syncAfterDelete, ok := any(wrapper.Item).(ISyncAfterDelete[T]); ok {
			if needDelete := syncAfterDelete.IsSyncAfterDelete(); needDelete {
				err := p.delete(wrapper.Key, false)
				if err != nil {
					logx.Errorf("ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥ [%s]: %v", wrapper.Key, err)
				}
				continue
			}
		}
		successKeys = append(successKeys, wrapper.Key)

	}

	return successKeys
}

// ğŸ†• é€æ¡æ›´æ–°ï¼ˆæ— äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) updateItemsOneByOne(items []*SyncQueueItem[T]) []string {
	successKeys := make([]string, 0, len(items))

	for _, wrapper := range items {
		if wrapper.Item == nil {
			continue
		}
		wrapper.IsSynced = false
		syncAction := p.getDataAction(wrapper.Item)
		err := syncAction.Update(wrapper.Item)

		if err != nil {
			// ğŸ”§ å¤„ç†è®°å½•ä¸å­˜åœ¨ - å°è¯•æ’å…¥
			if strings.Contains(err.Error(), "record not found") ||
				strings.Contains(err.Error(), "no rows") {
				logx.Infof("è®°å½•ä¸å­˜åœ¨ï¼Œå°è¯•æ’å…¥ [%s]", wrapper.Key)

				err = syncAction.Insert(wrapper.Item)
				if err == nil {
					successKeys = append(successKeys, wrapper.Key)
					continue
				}

				// æ’å…¥ä¹Ÿå¤±è´¥ï¼ˆå¯èƒ½æ˜¯ä¸»é”®å†²çªï¼Œå†å°è¯•æ›´æ–°ï¼‰
				if strings.Contains(err.Error(), "duplicate key") ||
					strings.Contains(err.Error(), "UNIQUE constraint failed") {
					logx.Errorf("æ’å…¥å†²çªï¼Œé‡è¯•æ›´æ–° [%s]", wrapper.Key)
					err = syncAction.Update(wrapper.Item)
					if err == nil {
						successKeys = append(successKeys, wrapper.Key)
						continue
					}
				}

				logx.Errorf("æ’å…¥å¤±è´¥ [%s]: %v", wrapper.Key, err)
				continue
			}

			logx.Errorf("æ›´æ–°å¤±è´¥ [%s]: %v", wrapper.Key, err)
			continue
		}
		wrapper.IsSynced = true
		if syncAfterDelete, ok := any(wrapper.Item).(ISyncAfterDelete[T]); ok {
			if needDelete := syncAfterDelete.IsSyncAfterDelete(); needDelete {
				err := p.delete(wrapper.Key, false)
				if err != nil {
					logx.Errorf("ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥ [%s]: %v", wrapper.Key, err)
				}
				continue
			}
		}
		successKeys = append(successKeys, wrapper.Key)

	}

	return successKeys
}

// ğŸ†• é€æ¡åˆ é™¤ï¼ˆæ— äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) deleteItemsOneByOne(items []*SyncQueueItem[T], syncAction types.IDataAction) []string {
	successKeys := make([]string, 0, len(items))

	for _, wrapper := range items {
		if wrapper.Item == nil {
			continue
		}
		wrapper.IsSynced = false
		err := syncAction.Delete(wrapper.Item)

		if err != nil {
			// ğŸ”§ å¤„ç†è®°å½•ä¸å­˜åœ¨ - è§†ä¸ºæˆåŠŸ
			if strings.Contains(err.Error(), "record not found") ||
				strings.Contains(err.Error(), "no rows") {
				logx.Infof("åˆ é™¤ç›®æ ‡ä¸å­˜åœ¨ï¼Œè·³è¿‡ [%s]", wrapper.Key)
				successKeys = append(successKeys, wrapper.Key)

				// ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜
				if err1 := p.delete(wrapper.Key, false); err1 != nil {
					logx.Errorf("ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥ [%s]: %v", wrapper.Key, err1)
				}
				continue
			}

			// ğŸ”§ å¤„ç† WHERE æ¡ä»¶ç¼ºå¤± - è¿™æ˜¯ç¼–ç¨‹é”™è¯¯
			if strings.Contains(err.Error(), "WHERE conditions required") {
				logx.Errorf("åˆ é™¤æ¡ä»¶ç¼ºå¤± [%s]ï¼Œéœ€è¦æ£€æŸ¥ Delete å®ç°: %v", wrapper.Key, err)
				// ä¸åŠ å…¥æˆåŠŸåˆ—è¡¨ï¼Œç­‰å¾…é‡è¯•
				continue
			}

			logx.Errorf("åˆ é™¤å¤±è´¥ [%s]: %v", wrapper.Key, err)
			continue
		}

		// åˆ é™¤æˆåŠŸ
		successKeys = append(successKeys, wrapper.Key)
		wrapper.IsSynced = true
		// ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜
		if err1 := p.delete(wrapper.Key, false); err1 != nil {
			logx.Errorf("ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥ [%s]: %v", wrapper.Key, err1)
		}
	}

	return successKeys
}

// ğŸ†• æ‰¹é‡æ›´æ–°åŒæ­¥çŠ¶æ€ï¼ˆä¸€æ¬¡ BadgerDB äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) batchUpdateSyncedStatus(keys []string) error {
	if len(keys) == 0 {
		return nil
	}

	now := time.Now()

	return p.manager.db.Update(func(txn *badger.Txn) error {
		for _, key := range keys {
			item, err := txn.Get([]byte(key))
			if err != nil {
				if err == badger.ErrKeyNotFound {
					continue
				}
				logx.Errorf("è·å–keyå¤±è´¥ [%s]: %v", key, err)
				continue
			}

			var wrapper SyncQueueItem[T]
			err = item.Value(func(val []byte) error {
				return json.Unmarshal(val, &wrapper)
			})
			if err != nil {
				logx.Errorf("ååºåˆ—åŒ–å¤±è´¥ [%s]: %v", key, err)
				continue
			}

			// æ›´æ–°åŒæ­¥çŠ¶æ€
			wrapper.IsSynced = true
			wrapper.SyncedAt = now

			data, err := json.Marshal(&wrapper)
			if err != nil {
				logx.Errorf("åºåˆ—åŒ–å¤±è´¥ [%s]: %v", key, err)
				continue
			}

			if err := txn.Set([]byte(key), data); err != nil {
				logx.Errorf("æ›´æ–°åŒæ­¥çŠ¶æ€å¤±è´¥ [%s]: %v", key, err)
			}
		}
		return nil
	})
}

func (p *PrefixedBadgerDB[T]) Count() (int, error) {
	count := 0

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = false
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Seek([]byte(p.prefix)); it.ValidForPrefix([]byte(p.prefix)); it.Next() {
			count++
		}
		return nil
	})

	return count, err
}
func (p *PrefixedBadgerDB[T]) CountByPrefix(subPrefix string) (int, error) {
	count := 0
	fullPrefix := p.prefix + subPrefix

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true // ğŸ”§ éœ€è¦è¯»å–å€¼æ¥åˆ¤æ–­æ˜¯å¦åˆ é™¤
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Seek([]byte(fullPrefix)); it.ValidForPrefix([]byte(fullPrefix)); it.Next() {
			item := it.Item()

			// ğŸ†• è§£ææ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦å·²åˆ é™¤
			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// åªç»Ÿè®¡æœªåˆ é™¤çš„æ•°æ®
				if !wrapper.IsDeleted {
					count++
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})

	return count, err
}

// Close å…³é—­å®ä¾‹
func (p *PrefixedBadgerDB[T]) Close() error {
	return p.CloseWithTimeout(30*time.Second, 10*time.Second)
}

// CloseWithTimeout å¸¦è¶…æ—¶çš„å…³é—­å®ä¾‹
func (p *PrefixedBadgerDB[T]) CloseWithTimeout(waitTimeout, syncTimeout time.Duration) error {
	// âœ… ä½¿ç”¨ sync.Once ç¡®ä¿åªå…³é—­ä¸€æ¬¡
	p.closeOnce.Do(func() {
		// æ ‡è®°ä¸ºå·²å…³é—­
		p.closeMu.Lock()
		p.closed = true
		p.closeMu.Unlock()

		// å…³é—­ channelï¼Œé€šçŸ¥ goroutine é€€å‡º
		close(p.closeCh)

		// âœ… ç­‰å¾… goroutine é€€å‡ºï¼ˆå¸¦è¶…æ—¶ï¼‰
		done := make(chan struct{})
		go func() {
			p.wg.Wait()
			close(done)
		}()

		select {
		case <-done:
			logx.Infof("åå°åŒæ­¥ goroutine å·²é€€å‡º [prefix=%s]", p.prefix)
		case <-time.After(waitTimeout):
			logx.Errorf("ç­‰å¾…åå° goroutine é€€å‡ºè¶…æ—¶ï¼ˆ%vï¼‰ï¼Œå¼ºåˆ¶å…³é—­ [prefix=%s]", waitTimeout, p.prefix)

			// âœ… æ£€æŸ¥æ˜¯å¦è¿˜åœ¨åŒæ­¥ä¸­
			p.syncMutex.Lock()
			if p.syncInProgress {
				logx.Errorf("æ£€æµ‹åˆ°æ­£åœ¨è¿›è¡Œçš„åŒæ­¥æ“ä½œï¼ˆç­‰å¾…æœ€å¤š %vï¼‰[prefix=%s]", syncTimeout, p.prefix)

				// ç­‰å¾…åŒæ­¥å®Œæˆæˆ–è¶…æ—¶
				syncDone := make(chan struct{})
				go func() {
					for p.syncInProgress {
						time.Sleep(100 * time.Millisecond)
					}
					close(syncDone)
				}()

				select {
				case <-syncDone:
					logx.Infof("åŒæ­¥æ“ä½œå·²å®Œæˆ [prefix=%s]", p.prefix)
				case <-time.After(syncTimeout):
					logx.Errorf("ç­‰å¾…åŒæ­¥å®Œæˆè¶…æ—¶ï¼ˆ%vï¼‰ï¼Œå¼ºåˆ¶é€€å‡º [prefix=%s]", syncTimeout, p.prefix)
				}
			}
			p.syncMutex.Unlock()
		}

		// ç§»é™¤ç®¡ç†å™¨å¼•ç”¨
		p.manager.RemoveRef(p.prefix)

		logx.Infof("å…±äº«BadgerDBå®ä¾‹å·²å…³é—­ [prefix=%s]", p.prefix)
	})

	return nil
}

// IsClosed æ£€æŸ¥å®ä¾‹æ˜¯å¦å·²å…³é—­
func (p *PrefixedBadgerDB[T]) IsClosed() bool {
	p.closeMu.RLock()
	defer p.closeMu.RUnlock()
	return p.closed
}

// ğŸ†• CheckAndEnforceLimit æ£€æŸ¥å¹¶æ‰§è¡Œæ•°é‡é™åˆ¶
func (p *PrefixedBadgerDB[T]) CheckAndEnforceLimit(model *T) error {
	// æ£€æŸ¥æ¨¡å‹æ˜¯å¦å®ç°äº† IAutoLimit æ¥å£

	limitConfig, ok := any(model).(IAutoLimit[T])
	if !ok {
		return nil // æœªå®ç°æ¥å£,ä¸æ‰§è¡Œé™åˆ¶
	}

	filterPrefix, maxCount, sortField, descending := limitConfig.GetLimitConfig()
	if maxCount <= 0 {
		return nil // æ— é™åˆ¶
	}

	// ç»Ÿè®¡å½“å‰æ•°é‡
	currentCount, err := p.CountByPrefix(filterPrefix)
	if err != nil {
		return fmt.Errorf("ç»Ÿè®¡æ•°é‡å¤±è´¥: %w", err)
	}

	if currentCount <= maxCount {
		return nil // æœªè¶…è¿‡é™åˆ¶
	}

	// éœ€è¦åˆ é™¤çš„æ•°é‡
	deleteCount := currentCount - maxCount
	logx.Infof("æ•°æ®è¶…é™ [prefix=%s, current=%d, max=%d, delete=%d]",
		p.prefix+filterPrefix, currentCount, maxCount, deleteCount)

	// è·å–éœ€è¦åˆ é™¤çš„æ—§æ•°æ®
	keysToDelete, err := p.getOldestKeys(filterPrefix, deleteCount, sortField, descending)
	if err != nil {
		return fmt.Errorf("è·å–æ—§æ•°æ®å¤±è´¥: %w", err)
	}

	if len(keysToDelete) == 0 {
		return nil
	}

	// æ‰¹é‡åˆ é™¤
	if err := p.batchDelete(keysToDelete); err != nil {
		return fmt.Errorf("æ‰¹é‡åˆ é™¤å¤±è´¥: %w", err)
	}

	logx.Infof("è‡ªåŠ¨æ¸…ç†å®Œæˆ [prefix=%s, deleted=%d]", p.prefix+filterPrefix, len(keysToDelete))
	return nil
}

// ğŸ†• è·å–æœ€æ—§çš„æ•°æ®keyåˆ—è¡¨
func (p *PrefixedBadgerDB[T]) getOldestKeys(filterPrefix string, count int, sortField string, descending bool) ([]string, error) {
	type itemWithKey struct {
		key       string
		sortValue interface{}
		timestamp time.Time
	}

	fullPrefix := p.prefix + filterPrefix
	items := make([]itemWithKey, 0)

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Seek([]byte(fullPrefix)); it.ValidForPrefix([]byte(fullPrefix)); it.Next() {
			item := it.Item()
			key := string(item.Key())

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// è·³è¿‡å·²åˆ é™¤çš„æ•°æ®
				if wrapper.IsDeleted {
					return nil
				}

				// æå–æ’åºå­—æ®µå€¼
				var sortValue interface{}
				var timestamp time.Time

				if wrapper.Item != nil {
					// ä½¿ç”¨åå°„è·å–æ’åºå­—æ®µ
					v := reflect.ValueOf(wrapper.Item)
					if v.Kind() == reflect.Ptr {
						v = v.Elem()
					}

					if v.Kind() == reflect.Struct {
						field := v.FieldByName(sortField)
						if field.IsValid() {
							sortValue = field.Interface()
							// å¦‚æœæ˜¯æ—¶é—´ç±»å‹
							if t, ok := sortValue.(time.Time); ok {
								timestamp = t
							}
						}
					}
				}

				// å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ’åºå­—æ®µ,ä½¿ç”¨åˆ›å»ºæ—¶é—´
				if timestamp.IsZero() {
					timestamp = wrapper.CreatedAt
				}

				items = append(items, itemWithKey{
					key:       key,
					sortValue: sortValue,
					timestamp: timestamp,
				})
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})

	if err != nil {
		return nil, err
	}

	// æ’åº(æ ¹æ®æ—¶é—´æˆ³)
	if descending {
		// é™åº: ä¿ç•™æœ€æ–°çš„,åˆ é™¤æœ€æ—§çš„
		for i := 0; i < len(items); i++ {
			for j := i + 1; j < len(items); j++ {
				if items[i].timestamp.Before(items[j].timestamp) {
					items[i], items[j] = items[j], items[i]
				}
			}
		}
	} else {
		// å‡åº: ä¿ç•™æœ€æ—§çš„,åˆ é™¤æœ€æ–°çš„
		for i := 0; i < len(items); i++ {
			for j := i + 1; j < len(items); j++ {
				if items[i].timestamp.After(items[j].timestamp) {
					items[i], items[j] = items[j], items[i]
				}
			}
		}
	}

	// å–éœ€è¦åˆ é™¤çš„key
	deleteCount := count
	if deleteCount > len(items) {
		deleteCount = len(items)
	}

	keys := make([]string, deleteCount)
	for i := 0; i < deleteCount; i++ {
		keys[i] = items[len(items)-deleteCount+i].key
	}

	return keys, nil
}

// ğŸ†• åœ¨ Set æ–¹æ³•åè‡ªåŠ¨æ£€æŸ¥é™åˆ¶
func (p *PrefixedBadgerDB[T]) SetWithAutoLimit(item *T, ttl time.Duration, fn ...func(wrapper *SyncQueueItem[T])) error {
	err := p.Set(item, ttl, fn...)
	if err != nil {
		return err
	}

	// å¼‚æ­¥æ£€æŸ¥å¹¶æ‰§è¡Œé™åˆ¶
	go func() {
		if err := p.CheckAndEnforceLimit(item); err != nil {
			logx.Errorf("è‡ªåŠ¨é™åˆ¶æ£€æŸ¥å¤±è´¥: %v", err)
		}
	}()

	return nil
}
