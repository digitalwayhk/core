package nosql

import (
	"fmt"
	"reflect"
	"strings"
	"sync"
	"time"

	"github.com/digitalwayhk/core/pkg/json"

	"github.com/dgraph-io/badger/v3"
	"github.com/digitalwayhk/core/pkg/persistence/entity"
	"github.com/digitalwayhk/core/pkg/persistence/types"
	"github.com/zeromicro/go-zero/core/logx"
)

// PrefixedBadgerDB å¸¦å‰ç¼€çš„å…±äº« BadgerDB
type PrefixedBadgerDB[T types.IModel] struct {
	manager *SharedBadgerManager
	prefix  string // "user:", "order:", "product:"

	syncDB         bool
	syncList       *entity.ModelList[T]
	syncLock       sync.RWMutex
	syncMutex      sync.Mutex
	syncInProgress bool
	closeCh        chan struct{}
	wg             sync.WaitGroup
	syncOnce       sync.Once
	isAutoClean    bool

	// å¾…åŒæ­¥è®¡æ•°ç¼“å­˜
	pendingCountCache int
	pendingCountMutex sync.RWMutex
	lastCountUpdate   time.Time
}

// NewSharedBadgerDB åˆ›å»ºå…±äº« BadgerDB å®ä¾‹
func NewSharedBadgerDB[T types.IModel](basePath string, config ...BadgerDBConfig) (*PrefixedBadgerDB[T], error) {
	prefix := reflect.TypeOf((*T)(nil)).Elem().Name() + ":"
	manager, err := GetSharedManager(basePath, config...)
	if err != nil {
		return nil, err
	}

	manager.AddRef(prefix)

	db := &PrefixedBadgerDB[T]{
		manager: manager,
		prefix:  prefix,
		closeCh: make(chan struct{}),
	}

	logx.Infof("å…±äº«BadgerDBå®ä¾‹å·²åˆ›å»º [prefix=%s]", prefix)
	return db, nil
}

// generateKey ç”Ÿæˆå¸¦å‰ç¼€çš„ key
func (p *PrefixedBadgerDB[T]) generateKey(item *T) string {
	if item == nil {
		return ""
	}
	if rowCode, ok := any(item).(types.IRowCode); ok {
		return p.prefix + rowCode.GetHash()
	}
	return ""
}

// SetSyncDB è®¾ç½®åŒæ­¥æ•°æ®åº“
func (p *PrefixedBadgerDB[T]) SetSyncDB(list *entity.ModelList[T]) {
	p.syncLock.Lock()
	defer p.syncLock.Unlock()

	if list != nil {
		if p.syncDB {
			return
		}
		p.syncDB = true
	} else {
		if !p.syncDB {
			return
		}
		p.syncDB = false
	}

	p.syncList = list

	if list != nil && p.syncDB {
		p.syncOnce.Do(func() {
			p.wg.Add(1)
			go p.syncToOtherDB()
			logx.Infof("å…±äº«DBè‡ªåŠ¨åŒæ­¥å·²å¯åŠ¨ [prefix=%s]", p.prefix)
		})
	}
}

// Set å†™å…¥æ•°æ®
func (p *PrefixedBadgerDB[T]) Set(item *T, ttl time.Duration, fn ...func(wrapper *SyncQueueItem[T])) error {
	key := p.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}

	p.syncLock.RLock()
	needSync := p.syncDB
	p.syncLock.RUnlock()

	data, err := p.setItem(key, needSync, item, fn...)
	if err != nil {
		return err
	}

	err = p.manager.db.Update(func(txn *badger.Txn) error {
		entry := badger.NewEntry([]byte(key), data)
		if ttl > 0 {
			entry = entry.WithTTL(ttl)
		}
		return txn.SetEntry(entry)
	})

	if err == nil && needSync {
		p.incrementPendingCount(1)
	}
	return err
}
func (p *PrefixedBadgerDB[T]) BatchInsert(items []*T) error {
	if len(items) == 0 {
		return nil
	}

	p.syncLock.RLock()
	needSync := p.syncDB
	p.syncLock.RUnlock()

	err := p.manager.db.Update(func(txn *badger.Txn) error {
		for _, item := range items {
			key := p.generateKey(item)
			if key == "" {
				return badger.ErrEmptyKey
			}

			data, err := p.setItem(key, needSync, item)
			if err != nil {
				return err
			}

			entry := badger.NewEntry([]byte(key), data)
			if err := txn.SetEntry(entry); err != nil {
				return err
			}
		}
		return nil
	})

	if err == nil && needSync {
		p.incrementPendingCount(len(items))
	}
	return err
}

// setItem å†…éƒ¨æ–¹æ³•ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
func (p *PrefixedBadgerDB[T]) setItem(key string, needSync bool, item *T, fn ...func(wrapper *SyncQueueItem[T])) ([]byte, error) {
	if item == nil {
		return nil, fmt.Errorf("item ä¸èƒ½ä¸ºç©º")
	}

	existingWrapper, err := p.getWrapper(key)
	var wrapper *SyncQueueItem[T]

	if err == nil && existingWrapper != nil {
		if existingWrapper.IsDeleted {
			return nil, fmt.Errorf("æ— æ³•æ›´æ–°å·²åˆ é™¤çš„é¡¹ï¼Œkey=%s", key)
		}
		wrapper = existingWrapper
		wrapper.Op = OpUpdate
		wrapper.Item = item
		wrapper.UpdatedAt = time.Now()
		wrapper.IsSynced = !needSync
	} else {
		now := time.Now()
		wrapper = &SyncQueueItem[T]{
			Key:       key,
			Item:      item,
			Op:        OpInsert,
			CreatedAt: now,
			UpdatedAt: now,
			IsSynced:  !needSync,
			IsDeleted: false,
		}
	}

	data, err := json.Marshal(wrapper)
	if err != nil {
		return nil, fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
	}

	if len(fn) > 0 {
		fn[0](wrapper)
	}
	return data, nil
}

// Get è·å–æ•°æ®
func (p *PrefixedBadgerDB[T]) Get(key string) (*T, error) {
	fullKey := p.prefix + key
	wrapper, err := p.getWrapper(fullKey)
	if err != nil {
		return nil, err
	}

	if wrapper.IsDeleted {
		return nil, badger.ErrKeyNotFound
	}

	return wrapper.Item, nil
}

// getWrapper å†…éƒ¨æ–¹æ³•
func (p *PrefixedBadgerDB[T]) getWrapper(key string) (*SyncQueueItem[T], error) {
	var wrapper = new(SyncQueueItem[T])

	err := p.manager.db.View(func(txn *badger.Txn) error {
		item, err := txn.Get([]byte(key))
		if err != nil {
			return err
		}

		return item.Value(func(val []byte) error {
			return json.Unmarshal(val, wrapper)
		})
	})

	if err != nil {
		return nil, err
	}

	if wrapper.Item != nil {
		if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
			hook.NewModel()
		}
	}

	return wrapper, nil
}

// Delete åˆ é™¤æ•°æ®
func (p *PrefixedBadgerDB[T]) Delete(key string) error {
	fullKey := p.prefix + key

	p.syncLock.RLock()
	needSync := p.syncDB
	p.syncLock.RUnlock()

	return p.delete(fullKey, needSync)
}
func (p *PrefixedBadgerDB[T]) DeleteByItem(item *T) error {
	key := p.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}

	p.syncLock.RLock()
	needSync := p.syncDB
	p.syncLock.RUnlock()

	return p.delete(key, needSync)
}
func (p *PrefixedBadgerDB[T]) DeleteByItemWithSync(item *T, needSync bool) error {
	key := p.generateKey(item)
	if key == "" {
		return badger.ErrEmptyKey
	}

	return p.delete(key, needSync)
}
func (p *PrefixedBadgerDB[T]) batchDelete(keys []string) error {
	if len(keys) == 0 {
		return nil
	}
	return p.manager.db.Update(func(txn *badger.Txn) error {
		for _, key := range keys {
			if err := txn.Delete([]byte(key)); err != nil {
				return err
			}
		}
		return nil
	})
}

// delete å†…éƒ¨æ–¹æ³•
func (p *PrefixedBadgerDB[T]) delete(key string, needSync bool) error {
	if !needSync {
		return p.manager.db.Update(func(txn *badger.Txn) error {
			return txn.Delete([]byte(key))
		})
	}

	if !p.syncDB {
		return fmt.Errorf("æœªå¯ç”¨åŒæ­¥æ•°æ®åº“åŠŸèƒ½ï¼Œæ— æ³•æ‰§è¡Œè½¯åˆ é™¤")
	}

	return p.manager.db.Update(func(txn *badger.Txn) error {
		item, err := txn.Get([]byte(key))
		if err != nil {
			if err == badger.ErrKeyNotFound {
				return nil
			}
			return err
		}

		var wrapper SyncQueueItem[T]
		err = item.Value(func(val []byte) error {
			return json.Unmarshal(val, &wrapper)
		})
		if err != nil {
			return err
		}

		if wrapper.IsDeleted {
			return nil
		}

		now := time.Now()
		wrapper.Op = OpDelete
		wrapper.IsDeleted = true
		wrapper.DeletedAt = now
		wrapper.UpdatedAt = now
		wrapper.IsSynced = false

		data, err := json.Marshal(&wrapper)
		if err != nil {
			return fmt.Errorf("åºåˆ—åŒ–å¤±è´¥: %w", err)
		}

		return txn.Set([]byte(key), data)
	})
}

// Scan æ‰«ææ•°æ®ï¼ˆä»…æ‰«æå½“å‰å‰ç¼€ï¼‰
func (p *PrefixedBadgerDB[T]) Scan(prefix string, limit int) ([]*T, error) {
	var results []*T
	prefix = p.prefix + prefix
	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		count := 0
		for it.Seek([]byte(prefix)); it.ValidForPrefix([]byte(prefix)); it.Next() {
			if limit > 0 && count >= limit {
				break
			}

			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				if wrapper.IsDeleted {
					return nil
				}

				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					results = append(results, wrapper.Item)
					count++
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})

	return results, err
}
func (p *PrefixedBadgerDB[T]) ScanAll() ([]*T, error) {
	var results []*T
	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()
		for it.Seek([]byte(p.prefix)); it.ValidForPrefix([]byte(p.prefix)); it.Next() {
			item := it.Item()
			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}
				if wrapper.IsDeleted {
					return nil
				}
				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					results = append(results, wrapper.Item)
				}
				return nil
			})
			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})
	return results, err
}
func (p *PrefixedBadgerDB[T]) ScanPage(prefix string, limit int, lastKey string) (*ScanResult[T], error) {
	prefix = p.prefix + prefix
	if limit <= 0 {
		limit = 1000 // é»˜è®¤æ¯é¡µ 1000 æ¡
	}

	result := &ScanResult[T]{
		Items: make([]*T, 0, limit),
	}

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = 100
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		// ç¡®å®šèµ·å§‹ä½ç½®
		var startKey []byte
		if lastKey != "" {
			startKey = []byte(lastKey)
		} else {
			startKey = []byte(prefix)
		}

		count := 0
		firstItem := true

		for it.Seek(startKey); it.ValidForPrefix([]byte(prefix)); it.Next() {
			// è·³è¿‡ä¸Šä¸€é¡µçš„æœ€åä¸€æ¡ï¼ˆé¿å…é‡å¤ï¼‰
			if lastKey != "" && firstItem {
				currentKey := string(it.Item().Key())
				if currentKey == lastKey {
					firstItem = false
					continue
				}
			}
			firstItem = false

			// è¾¾åˆ°é™åˆ¶åå†è¯»ä¸€æ¡ï¼Œåˆ¤æ–­æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
			if count >= limit {
				result.HasMore = true
				break
			}

			item := it.Item()
			currentKey := string(item.Key())

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// è¿‡æ»¤å·²åˆ é™¤çš„æ•°æ®
				if wrapper.IsDeleted {
					return nil
				}

				if wrapper.Item != nil {
					if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
						hook.NewModel()
					}
					result.Items = append(result.Items, wrapper.Item)
					result.LastKey = currentKey
					count++
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}

		return nil
	})

	return result, err
}

// GetPendingSyncCount è·å–å¾…åŒæ­¥æ•°é‡
func (p *PrefixedBadgerDB[T]) GetPendingSyncCount() (int, error) {
	count := 0

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Seek([]byte(p.prefix)); it.ValidForPrefix([]byte(p.prefix)); it.Next() {
			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				if !wrapper.IsSynced {
					count++
				}
				return nil
			})

			if err != nil {
				continue
			}
		}
		return nil
	})

	return count, err
}

// incrementPendingCount æ›´æ–°å¾…åŒæ­¥è®¡æ•°
func (p *PrefixedBadgerDB[T]) incrementPendingCount(delta int) {
	p.pendingCountMutex.Lock()
	p.pendingCountCache += delta
	p.pendingCountMutex.Unlock()
}

// getDataAction è·å–åŒæ­¥æ“ä½œ
func (p *PrefixedBadgerDB[T]) getDataAction(item *T) types.IDataAction {
	if p.syncList != nil {
		model := item
		if model == nil {
			model = new(T)
			if nm, ok := any(model).(types.IModelNewHook); ok {
				nm.NewModel()
			}
		}
		searchItem := p.syncList.GetSearchItem()
		searchItem.Model = model
		action := p.syncList.GetDBAdapter(searchItem)
		return action
	}
	return nil
}

// syncToOtherDB åŒæ­¥åˆ°å…¶ä»–æ•°æ®åº“ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
func (p *PrefixedBadgerDB[T]) syncToOtherDB() {
	defer p.wg.Done()

	config := p.manager.config
	interval := config.SyncInterval
	minInterval := config.SyncMinInterval
	maxInterval := config.SyncMaxInterval

	ticker := time.NewTicker(interval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			p.syncLock.RLock()
			hasDB := p.syncDB
			p.syncLock.RUnlock()

			if !hasDB {
				continue
			}

			pendingCount, err := p.GetPendingSyncCount()
			if err != nil {
				logx.Errorf("è·å–å¾…åŒæ­¥æ•°é‡å¤±è´¥ [prefix=%s]: %v", p.prefix, err)
				continue
			}

			if pendingCount == 0 {
				interval = min(interval*2, maxInterval)
				ticker.Reset(interval)
				continue
			}

			p.syncMutex.Lock()
			if p.syncInProgress {
				p.syncMutex.Unlock()
				interval = min(interval*2, maxInterval)
				ticker.Reset(interval)
				continue
			}
			p.syncInProgress = true
			p.syncMutex.Unlock()

			start := time.Now()

			if err := p.processSyncQueue(); err != nil {
				logx.Errorf("åŒæ­¥å¤±è´¥ [prefix=%s]: %v", p.prefix, err)
			}

			duration := time.Since(start)

			p.syncMutex.Lock()
			p.syncInProgress = false
			p.syncMutex.Unlock()

			if duration < interval/2 {
				interval = max(interval/2, minInterval)
			} else if duration > interval {
				interval = min(duration*2, maxInterval)
			}

			ticker.Reset(interval)
			logx.Infof("åŒæ­¥å®Œæˆ [prefix=%s, å¤„ç†: %d, è€—æ—¶: %v]", p.prefix, pendingCount, duration)

		case <-p.closeCh:
			logx.Infof("syncToOtherDB é€€å‡º [prefix=%s]", p.prefix)
			return
		}
	}
}

// processSyncQueue å¤„ç†åŒæ­¥é˜Ÿåˆ—ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼Œé™å®šå‰ç¼€ï¼‰
func (p *PrefixedBadgerDB[T]) processSyncQueue() error {
	unsyncedItems, err := p.getUnsyncedBatch(p.manager.config.SyncBatchSize)
	if err != nil {
		return fmt.Errorf("è·å–æœªåŒæ­¥æ•°æ®å¤±è´¥: %w", err)
	}

	if len(unsyncedItems) == 0 {
		return nil
	}

	_, err = p.syncBatch(unsyncedItems)
	return err
}

// getUnsyncedBatch è·å–æœªåŒæ­¥æ•°æ®ï¼ˆé™å®šå‰ç¼€ï¼‰
func (p *PrefixedBadgerDB[T]) getUnsyncedBatch(limit int) ([]*SyncQueueItem[T], error) {
	var items []*SyncQueueItem[T]

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true
		it := txn.NewIterator(opts)
		defer it.Close()

		count := 0
		for it.Seek([]byte(p.prefix)); it.ValidForPrefix([]byte(p.prefix)); it.Next() {
			if count >= limit {
				break
			}

			item := it.Item()

			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				if !wrapper.IsSynced {
					if wrapper.Item != nil {
						if hook, ok := any(wrapper.Item).(types.IModelNewHook); ok {
							hook.NewModel()
						}
					}
					items = append(items, &wrapper)
					count++
				}
				return nil
			})

			if err != nil {
				continue
			}
		}
		return nil
	})

	return items, err
}

// ğŸ”§ å®Œæ•´çš„æ‰¹é‡åŒæ­¥å®ç°ï¼ˆåŒ…å«é”™è¯¯å¤„ç†ï¼‰
func (p *PrefixedBadgerDB[T]) syncBatch(items []*SyncQueueItem[T]) ([]string, error) {
	if len(items) == 0 {
		return nil, nil
	}

	p.syncLock.RLock()
	if !p.syncDB {
		p.syncLock.RUnlock()
		return nil, fmt.Errorf("æœªå¼€å¯ syncDB")
	}
	p.syncLock.RUnlock()

	// æŒ‰æ“ä½œç±»å‹åˆ†ç»„
	var (
		insertItems []*SyncQueueItem[T]
		updateItems []*SyncQueueItem[T]
		deleteItems []*SyncQueueItem[T]
	)

	for _, wrapper := range items {
		setHashCode(wrapper.Item)

		switch wrapper.Op {
		case OpInsert:
			insertItems = append(insertItems, wrapper)
		case OpUpdate:
			updateItems = append(updateItems, wrapper)
		case OpDelete:
			deleteItems = append(deleteItems, wrapper)
		}
	}

	successKeys := make([]string, 0, len(items))

	// æ‰¹é‡æ’å…¥ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
	if len(insertItems) > 0 {
		keys := p.batchInsertWithErrorHandling(insertItems)
		successKeys = append(successKeys, keys...)
	}

	// æ‰¹é‡æ›´æ–°ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
	if len(updateItems) > 0 {
		keys := p.batchUpdateWithErrorHandling(updateItems)
		successKeys = append(successKeys, keys...)
	}

	// æ‰¹é‡åˆ é™¤ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
	if len(deleteItems) > 0 {
		keys := p.batchDeleteWithErrorHandling(deleteItems)
		successKeys = append(successKeys, keys...)
	}

	// æ‰¹é‡æ›´æ–°åŒæ­¥çŠ¶æ€
	if len(successKeys) > 0 {
		p.batchUpdateSyncedStatus(successKeys)
		p.incrementPendingCount(-len(successKeys))
	}

	return successKeys, nil
}

// ğŸ†• æ‰¹é‡æ’å…¥ï¼ˆä½¿ç”¨äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) batchInsertWithErrorHandling(items []*SyncQueueItem[T]) []string {
	if len(items) == 0 {
		return nil
	}

	syncAction := p.getDataAction(items[0].Item)
	if syncAction == nil {
		logx.Errorf("æœªæ‰¾åˆ°åŒæ­¥æ“ä½œå¯¹è±¡")
		return nil
	}

	successKeys := make([]string, 0, len(items))
	physicalDeleteKeys := make([]string, 0, len(items)) // ğŸ†• éœ€è¦ç‰©ç†åˆ é™¤çš„keys

	// ğŸ”§ å¼€å¯äº‹åŠ¡ï¼ˆæ‰¹é‡æ“ä½œï¼‰
	if err := syncAction.Transaction(); err != nil {
		logx.Errorf("å¼€å¯äº‹åŠ¡å¤±è´¥: %vï¼Œé™çº§ä¸ºé€æ¡æ’å…¥", err)
		return p.insertItemsOneByOne(items)
	}

	// åœ¨äº‹åŠ¡ä¸­é€æ¡æ’å…¥
	hasError := false
	for _, wrapper := range items {
		if wrapper.Item == nil {
			continue
		}

		err := syncAction.Insert(wrapper.Item)

		if err != nil {
			// ğŸ”§ å¤„ç†ä¸»é”®å†²çª - å°è¯•æ›´æ–°
			if strings.Contains(err.Error(), "duplicate key") ||
				strings.Contains(err.Error(), "UNIQUE constraint failed") {
				logx.Infof("æ’å…¥å†²çªï¼Œå°è¯•æ›´æ–° [%s]", wrapper.Key)

				err = syncAction.Update(wrapper.Item)
				if err == nil {
					successKeys = append(successKeys, wrapper.Key)
					continue
				}

				logx.Errorf("æ›´æ–°å¤±è´¥ [%s]: %v", wrapper.Key, err)
				hasError = true
				continue
			}

			// å…¶ä»–é”™è¯¯
			logx.Errorf("æ’å…¥å¤±è´¥ [%s]: %v", wrapper.Key, err)
			hasError = true
			continue
		}

		// ğŸ†• æ£€æŸ¥æ˜¯å¦å®ç° ISyncAfterDelete æ¥å£
		shouldPhysicalDelete := false
		if syncAfterDelete, ok := any(wrapper.Item).(ISyncAfterDelete[T]); ok {
			if needDelete := syncAfterDelete.IsSyncAfterDelete(); needDelete {
				logx.Infof("ISyncAfterDelete è¿”å› trueï¼Œå°†ç‰©ç†åˆ é™¤ [%s]", wrapper.Key)
				shouldPhysicalDelete = true
			}
		}

		if shouldPhysicalDelete {
			physicalDeleteKeys = append(physicalDeleteKeys, wrapper.Key)
		}
		if !shouldPhysicalDelete {
			// æ’å…¥æˆåŠŸ
			successKeys = append(successKeys, wrapper.Key)
		}
	}

	// ğŸ”§ æäº¤äº‹åŠ¡
	if err := syncAction.Commit(); err != nil {
		logx.Errorf("æäº¤äº‹åŠ¡å¤±è´¥: %vï¼Œå›æ»šå¹¶é™çº§ä¸ºé€æ¡å¤„ç†", err)

		// å®‰å…¨å›æ»š
		if rollbackErr := syncAction.Rollback(); rollbackErr != nil {
			logx.Errorf("å›æ»šå¤±è´¥: %v", rollbackErr)
		}
		return p.insertItemsOneByOne(items)
	}
	// ğŸ†• æ‰¹é‡ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜
	if len(physicalDeleteKeys) > 0 {
		if err := p.batchDelete(physicalDeleteKeys); err != nil {
			logx.Errorf("æ‰¹é‡ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥: %v", err)
		}
	}
	if hasError {
		logx.Errorf("æ‰¹é‡æ’å…¥éƒ¨åˆ†å¤±è´¥ï¼ŒæˆåŠŸ: %d/%d", len(successKeys), len(items))
	}

	return successKeys
}

// ğŸ†• æ‰¹é‡æ›´æ–°ï¼ˆä½¿ç”¨äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) batchUpdateWithErrorHandling(items []*SyncQueueItem[T]) []string {
	if len(items) == 0 {
		return nil
	}

	syncAction := p.getDataAction(items[0].Item)
	if syncAction == nil {
		logx.Errorf("æœªæ‰¾åˆ°åŒæ­¥æ“ä½œå¯¹è±¡")
		return nil
	}

	successKeys := make([]string, 0, len(items))
	physicalDeleteKeys := make([]string, 0, len(items)) // ğŸ†• éœ€è¦ç‰©ç†åˆ é™¤çš„keys

	// ğŸ”§ å¼€å¯äº‹åŠ¡ï¼ˆæ‰¹é‡æ“ä½œï¼‰
	if err := syncAction.Transaction(); err != nil {
		logx.Errorf("å¼€å¯äº‹åŠ¡å¤±è´¥: %vï¼Œé™çº§ä¸ºé€æ¡æ›´æ–°", err)
		return p.updateItemsOneByOne(items)
	}

	// åœ¨äº‹åŠ¡ä¸­é€æ¡æ›´æ–°
	hasError := false
	for _, wrapper := range items {
		if wrapper.Item == nil {
			continue
		}

		err := syncAction.Update(wrapper.Item)

		if err != nil {
			// ğŸ”§ å¤„ç†è®°å½•ä¸å­˜åœ¨ - å°è¯•æ’å…¥
			if strings.Contains(err.Error(), "record not found") ||
				strings.Contains(err.Error(), "no rows") {
				logx.Infof("è®°å½•ä¸å­˜åœ¨ï¼Œå°è¯•æ’å…¥ [%s]", wrapper.Key)

				err = syncAction.Insert(wrapper.Item)
				if err == nil {
					successKeys = append(successKeys, wrapper.Key)
					continue
				}

				// æ’å…¥ä¹Ÿå¤±è´¥ï¼ˆå¯èƒ½æ˜¯ä¸»é”®å†²çªï¼Œå†å°è¯•æ›´æ–°ï¼‰
				if strings.Contains(err.Error(), "duplicate key") ||
					strings.Contains(err.Error(), "UNIQUE constraint failed") {
					logx.Errorf("æ’å…¥å†²çªï¼Œé‡è¯•æ›´æ–° [%s]", wrapper.Key)
					err = syncAction.Update(wrapper.Item)
					if err == nil {
						successKeys = append(successKeys, wrapper.Key)
						continue
					}
				}

				logx.Errorf("æ’å…¥å¤±è´¥ [%s]: %v", wrapper.Key, err)
				hasError = true
				continue
			}

			logx.Errorf("æ›´æ–°å¤±è´¥ [%s]: %v", wrapper.Key, err)
			hasError = true
			continue
		}

		// ğŸ†• æ£€æŸ¥æ˜¯å¦å®ç° ISyncAfterDelete æ¥å£
		shouldPhysicalDelete := false
		if syncAfterDelete, ok := any(wrapper.Item).(ISyncAfterDelete[T]); ok {
			if needDelete := syncAfterDelete.IsSyncAfterDelete(); needDelete {
				logx.Infof("ISyncAfterDelete è¿”å› trueï¼Œå°†ç‰©ç†åˆ é™¤ [%s]", wrapper.Key)
				shouldPhysicalDelete = true
			}
		}
		if shouldPhysicalDelete {
			physicalDeleteKeys = append(physicalDeleteKeys, wrapper.Key)
		}
		if !shouldPhysicalDelete {
			// æ›´æ–°æˆåŠŸ
			successKeys = append(successKeys, wrapper.Key)
		}
	}

	// ğŸ”§ æäº¤äº‹åŠ¡
	if err := syncAction.Commit(); err != nil {
		logx.Errorf("æäº¤äº‹åŠ¡å¤±è´¥: %vï¼Œå›æ»šå¹¶é™çº§ä¸ºé€æ¡å¤„ç†", err)

		if rollbackErr := syncAction.Rollback(); rollbackErr != nil {
			logx.Errorf("å›æ»šå¤±è´¥: %v", rollbackErr)
		}
		return p.updateItemsOneByOne(items)
	}
	// ğŸ†• æ‰¹é‡ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜
	if len(physicalDeleteKeys) > 0 {
		if err := p.batchDelete(physicalDeleteKeys); err != nil {
			logx.Errorf("æ‰¹é‡ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥: %v", err)
		}
	}
	if hasError {
		logx.Errorf("æ‰¹é‡æ›´æ–°éƒ¨åˆ†å¤±è´¥ï¼ŒæˆåŠŸ: %d/%d", len(successKeys), len(items))
	}

	return successKeys
}

// ğŸ†• æ‰¹é‡åˆ é™¤ï¼ˆä½¿ç”¨äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) batchDeleteWithErrorHandling(items []*SyncQueueItem[T]) []string {
	if len(items) == 0 {
		return nil
	}

	syncAction := p.getDataAction(items[0].Item)
	if syncAction == nil {
		logx.Errorf("æœªæ‰¾åˆ°åŒæ­¥æ“ä½œå¯¹è±¡")
		return nil
	}

	successKeys := make([]string, 0, len(items))

	// ğŸ”§ å¼€å¯äº‹åŠ¡ï¼ˆæ‰¹é‡æ“ä½œï¼‰
	if err := syncAction.Transaction(); err != nil {
		logx.Errorf("å¼€å¯äº‹åŠ¡å¤±è´¥: %vï¼Œé™çº§ä¸ºé€æ¡åˆ é™¤", err)
		newSyncAction := p.getDataAction(items[0].Item)
		return p.deleteItemsOneByOne(items, newSyncAction)
	}

	// åœ¨äº‹åŠ¡ä¸­é€æ¡åˆ é™¤
	hasError := false
	for _, wrapper := range items {
		if wrapper.Item == nil {
			continue
		}

		err := syncAction.Delete(wrapper.Item)

		if err != nil {
			// ğŸ”§ å¤„ç†è®°å½•ä¸å­˜åœ¨ - è§†ä¸ºæˆåŠŸ
			if strings.Contains(err.Error(), "record not found") ||
				strings.Contains(err.Error(), "no rows") {
				logx.Infof("åˆ é™¤ç›®æ ‡ä¸å­˜åœ¨ï¼Œè·³è¿‡ [%s]", wrapper.Key)
				successKeys = append(successKeys, wrapper.Key)
				continue
			}

			// ğŸ”§ å¤„ç† WHERE æ¡ä»¶ç¼ºå¤± - è¿™æ˜¯ç¼–ç¨‹é”™è¯¯
			if strings.Contains(err.Error(), "WHERE conditions required") {
				logx.Errorf("åˆ é™¤æ¡ä»¶ç¼ºå¤± [%s]ï¼Œéœ€è¦æ£€æŸ¥ Delete å®ç°: %v", wrapper.Key, err)
				hasError = true

				// å›æ»šäº‹åŠ¡
				if rollbackErr := syncAction.Rollback(); rollbackErr != nil {
					logx.Errorf("å›æ»šå¤±è´¥: %v", rollbackErr)
				}

				// é‡æ–°è·å–æ–°çš„ syncAction
				newSyncAction := p.getDataAction(items[0].Item)
				if newSyncAction == nil {
					logx.Errorf("é‡æ–°è·å– syncAction å¤±è´¥")
					return nil
				}

				// é™çº§ä¸ºé€æ¡åˆ é™¤
				return p.deleteItemsOneByOne(items, newSyncAction)
			}

			logx.Errorf("åˆ é™¤å¤±è´¥ [%s]: %v", wrapper.Key, err)
			hasError = true
			continue
		}

		// åˆ é™¤æˆåŠŸ
		successKeys = append(successKeys, wrapper.Key)
	}

	// ğŸ”§ æäº¤äº‹åŠ¡
	if err := syncAction.Commit(); err != nil {
		logx.Errorf("æäº¤äº‹åŠ¡å¤±è´¥: %vï¼Œå›æ»šå¹¶é™çº§ä¸ºé€æ¡å¤„ç†", err)

		if rollbackErr := syncAction.Rollback(); rollbackErr != nil {
			logx.Errorf("å›æ»šå¤±è´¥: %v", rollbackErr)
		}

		// ğŸ†• é‡æ–°è·å–æ–°çš„ syncAction
		newSyncAction := p.getDataAction(items[0].Item)
		if newSyncAction == nil {
			logx.Errorf("é‡æ–°è·å– syncAction å¤±è´¥")
			return nil
		}

		return p.deleteItemsOneByOne(items, newSyncAction)
	}

	// ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜
	for _, key := range successKeys {
		if err := p.delete(key, false); err != nil {
			logx.Errorf("ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥ [%s]: %v", key, err)
		}
	}

	if hasError {
		logx.Errorf("æ‰¹é‡åˆ é™¤éƒ¨åˆ†å¤±è´¥ï¼ŒæˆåŠŸ: %d/%d", len(successKeys), len(items))
	}

	return successKeys
}

// ğŸ†• é€æ¡æ’å…¥ï¼ˆæ— äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) insertItemsOneByOne(items []*SyncQueueItem[T]) []string {
	successKeys := make([]string, 0, len(items))

	for _, wrapper := range items {
		if wrapper.Item == nil {
			continue
		}
		syncAction := p.getDataAction(wrapper.Item)
		err := syncAction.Insert(wrapper.Item)
		if err != nil {
			// ğŸ”§ å¤„ç†ä¸»é”®å†²çª - å°è¯•æ›´æ–°
			if strings.Contains(err.Error(), "duplicate key") ||
				strings.Contains(err.Error(), "UNIQUE constraint failed") {
				logx.Infof("æ’å…¥å†²çªï¼Œå°è¯•æ›´æ–° [%s]", wrapper.Key)

				err = syncAction.Update(wrapper.Item)
				if err == nil {
					successKeys = append(successKeys, wrapper.Key)
					continue
				}

				logx.Errorf("æ›´æ–°å¤±è´¥ [%s]: %v", wrapper.Key, err)
				continue
			}

			logx.Errorf("æ’å…¥å¤±è´¥ [%s]: %v", wrapper.Key, err)
			continue
		}
		if syncAfterDelete, ok := any(wrapper.Item).(ISyncAfterDelete[T]); ok {
			if needDelete := syncAfterDelete.IsSyncAfterDelete(); needDelete {
				err := p.delete(wrapper.Key, false)
				if err != nil {
					logx.Errorf("ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥ [%s]: %v", wrapper.Key, err)
				}
			}
		}
		successKeys = append(successKeys, wrapper.Key)
	}

	return successKeys
}

// ğŸ†• é€æ¡æ›´æ–°ï¼ˆæ— äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) updateItemsOneByOne(items []*SyncQueueItem[T]) []string {
	successKeys := make([]string, 0, len(items))

	for _, wrapper := range items {
		if wrapper.Item == nil {
			continue
		}

		syncAction := p.getDataAction(wrapper.Item)
		err := syncAction.Update(wrapper.Item)

		if err != nil {
			// ğŸ”§ å¤„ç†è®°å½•ä¸å­˜åœ¨ - å°è¯•æ’å…¥
			if strings.Contains(err.Error(), "record not found") ||
				strings.Contains(err.Error(), "no rows") {
				logx.Infof("è®°å½•ä¸å­˜åœ¨ï¼Œå°è¯•æ’å…¥ [%s]", wrapper.Key)

				err = syncAction.Insert(wrapper.Item)
				if err == nil {
					successKeys = append(successKeys, wrapper.Key)
					continue
				}

				// æ’å…¥ä¹Ÿå¤±è´¥ï¼ˆå¯èƒ½æ˜¯ä¸»é”®å†²çªï¼Œå†å°è¯•æ›´æ–°ï¼‰
				if strings.Contains(err.Error(), "duplicate key") ||
					strings.Contains(err.Error(), "UNIQUE constraint failed") {
					logx.Errorf("æ’å…¥å†²çªï¼Œé‡è¯•æ›´æ–° [%s]", wrapper.Key)
					err = syncAction.Update(wrapper.Item)
					if err == nil {
						successKeys = append(successKeys, wrapper.Key)
						continue
					}
				}

				logx.Errorf("æ’å…¥å¤±è´¥ [%s]: %v", wrapper.Key, err)
				continue
			}

			logx.Errorf("æ›´æ–°å¤±è´¥ [%s]: %v", wrapper.Key, err)
			continue
		}
		if syncAfterDelete, ok := any(wrapper.Item).(ISyncAfterDelete[T]); ok {
			if needDelete := syncAfterDelete.IsSyncAfterDelete(); needDelete {
				err := p.delete(wrapper.Key, false)
				if err != nil {
					logx.Errorf("ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥ [%s]: %v", wrapper.Key, err)
				}
			}
		}
		successKeys = append(successKeys, wrapper.Key)
	}

	return successKeys
}

// ğŸ†• é€æ¡åˆ é™¤ï¼ˆæ— äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) deleteItemsOneByOne(items []*SyncQueueItem[T], syncAction types.IDataAction) []string {
	successKeys := make([]string, 0, len(items))

	for _, wrapper := range items {
		if wrapper.Item == nil {
			continue
		}

		err := syncAction.Delete(wrapper.Item)

		if err != nil {
			// ğŸ”§ å¤„ç†è®°å½•ä¸å­˜åœ¨ - è§†ä¸ºæˆåŠŸ
			if strings.Contains(err.Error(), "record not found") ||
				strings.Contains(err.Error(), "no rows") {
				logx.Infof("åˆ é™¤ç›®æ ‡ä¸å­˜åœ¨ï¼Œè·³è¿‡ [%s]", wrapper.Key)
				successKeys = append(successKeys, wrapper.Key)

				// ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜
				if err1 := p.delete(wrapper.Key, false); err1 != nil {
					logx.Errorf("ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥ [%s]: %v", wrapper.Key, err1)
				}
				continue
			}

			// ğŸ”§ å¤„ç† WHERE æ¡ä»¶ç¼ºå¤± - è¿™æ˜¯ç¼–ç¨‹é”™è¯¯
			if strings.Contains(err.Error(), "WHERE conditions required") {
				logx.Errorf("åˆ é™¤æ¡ä»¶ç¼ºå¤± [%s]ï¼Œéœ€è¦æ£€æŸ¥ Delete å®ç°: %v", wrapper.Key, err)
				// ä¸åŠ å…¥æˆåŠŸåˆ—è¡¨ï¼Œç­‰å¾…é‡è¯•
				continue
			}

			logx.Errorf("åˆ é™¤å¤±è´¥ [%s]: %v", wrapper.Key, err)
			continue
		}

		// åˆ é™¤æˆåŠŸ
		successKeys = append(successKeys, wrapper.Key)

		// ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜
		if err1 := p.delete(wrapper.Key, false); err1 != nil {
			logx.Errorf("ç‰©ç†åˆ é™¤æœ¬åœ°ç¼“å­˜å¤±è´¥ [%s]: %v", wrapper.Key, err1)
		}
	}

	return successKeys
}

// ğŸ†• æ‰¹é‡æ›´æ–°åŒæ­¥çŠ¶æ€ï¼ˆä¸€æ¬¡ BadgerDB äº‹åŠ¡ï¼‰
func (p *PrefixedBadgerDB[T]) batchUpdateSyncedStatus(keys []string) error {
	if len(keys) == 0 {
		return nil
	}

	now := time.Now()

	return p.manager.db.Update(func(txn *badger.Txn) error {
		for _, key := range keys {
			item, err := txn.Get([]byte(key))
			if err != nil {
				if err == badger.ErrKeyNotFound {
					continue
				}
				logx.Errorf("è·å–keyå¤±è´¥ [%s]: %v", key, err)
				continue
			}

			var wrapper SyncQueueItem[T]
			err = item.Value(func(val []byte) error {
				return json.Unmarshal(val, &wrapper)
			})
			if err != nil {
				logx.Errorf("ååºåˆ—åŒ–å¤±è´¥ [%s]: %v", key, err)
				continue
			}

			// æ›´æ–°åŒæ­¥çŠ¶æ€
			wrapper.IsSynced = true
			wrapper.SyncedAt = now

			data, err := json.Marshal(&wrapper)
			if err != nil {
				logx.Errorf("åºåˆ—åŒ–å¤±è´¥ [%s]: %v", key, err)
				continue
			}

			if err := txn.Set([]byte(key), data); err != nil {
				logx.Errorf("æ›´æ–°åŒæ­¥çŠ¶æ€å¤±è´¥ [%s]: %v", key, err)
			}
		}
		return nil
	})
}

func (p *PrefixedBadgerDB[T]) Count() (int, error) {
	count := 0

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = false
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Seek([]byte(p.prefix)); it.ValidForPrefix([]byte(p.prefix)); it.Next() {
			count++
		}
		return nil
	})

	return count, err
}
func (p *PrefixedBadgerDB[T]) CountByPrefix(subPrefix string) (int, error) {
	count := 0
	fullPrefix := p.prefix + subPrefix

	err := p.manager.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = true // ğŸ”§ éœ€è¦è¯»å–å€¼æ¥åˆ¤æ–­æ˜¯å¦åˆ é™¤
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Seek([]byte(fullPrefix)); it.ValidForPrefix([]byte(fullPrefix)); it.Next() {
			item := it.Item()

			// ğŸ†• è§£ææ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦å·²åˆ é™¤
			err := item.Value(func(val []byte) error {
				var wrapper SyncQueueItem[T]
				if err := json.Unmarshal(val, &wrapper); err != nil {
					return err
				}

				// åªç»Ÿè®¡æœªåˆ é™¤çš„æ•°æ®
				if !wrapper.IsDeleted {
					count++
				}
				return nil
			})

			if err != nil {
				logx.Errorf("è§£ææ•°æ®å¤±è´¥: %v", err)
				continue
			}
		}
		return nil
	})

	return count, err
}

// Close å…³é—­å®ä¾‹
func (p *PrefixedBadgerDB[T]) Close() error {
	close(p.closeCh)
	p.wg.Wait()

	p.manager.RemoveRef(p.prefix)

	logx.Infof("å…±äº«BadgerDBå®ä¾‹å·²å…³é—­ [prefix=%s]", p.prefix)
	return nil
}
